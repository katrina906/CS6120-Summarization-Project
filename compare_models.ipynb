{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compare_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/compare_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31uUu2nsvi84"
      },
      "source": [
        "\n",
        "__Extractive__\n",
        "- Pros\n",
        "  - All information is guarenteed to be correct and in the article\n",
        "  - TextRank recall model has the best overall recall statistic out of any model, which means it puts the most information in front of the human evaluator to use (while still having a reasonable precision - does not generate significant garbage)\n",
        "  - Summary sentences are guarenteed to be gramatically correct\n",
        "  - Easier to explain to a non-technical audience how we are selecting sentences vs. trusting an algorithm to generate completely new data like in abstractive summarization\n",
        "- Cons\n",
        "  - Sentences can appear in illogical orders and be taken out of context: Ex one sentence refers to Bob and the next refers to 'he', but that 'he' was not originally Bob\n",
        "    - Can occur in abstractive too\n",
        "  - Extracted sentences can have an important point but also contain unnecessary supporting details.\n",
        "  - Needs to be long to get all of the information in a human generated summary, which will combine concepts from multiple article sentences into one summary sentence. \n",
        "\n",
        "__Abstractive__\n",
        "- Pros\n",
        "  - Human generated summaries are qualitatively better because combine multiple concepts from multiple text sentences into one summary sentence. Abstractive methods is better achieve this than extractive.\n",
        "    - Can take main points of sentences and drop the supporting details\n",
        "    - More engaging to read sentences\n",
        "  - Can use input text without having to make text cleaning decisions. Can consider more features like punctuation and capitalization \n",
        "  - Ordering of sentences is logical. \n",
        "- Cons\n",
        "  - Occasionally create information that is not in the original article (surprisingly rare!)\n",
        "  - Occasionally repetitive. Can tune ngram repeat hyperparameters to help prevent\n",
        "  - Not all capitalization in sentences is decoded correctly, especially proper names\n",
        "  - Sentences are often cut off part way through because we can only specify the max number of characters. Cuts off mid-sentence if did not generate an end of sequence character before reaching the specified max length. \n",
        "  - Decoding is more computationally intensive\n",
        "  - Encoding can only take the first 1,017 tokens of the text largely because of self-attention layer: requires n^2 calculations for n tokens because consider entire sequence for attention. If topics appear later in the article for the first time, they will be completely missed and not included in the summary\n",
        "    - Only 15% of articles have more than 1,017 tokens. And not much over - max is 1,819 tokens. Mean is 653 tokens. \n",
        "    - In general, news articles tend to include highlights of the most important information in the first few sentences followed by details, so this should not impact performance as drastically as in other contexts.\n",
        "    - BCG material follows the pyramid principle where you summarize the key points first, so similar structure to news articles. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0SzA4CNqJlf"
      },
      "source": [
        "# only using False versions of metrics because always choose the same models for both and thus all results are the same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaD-DFLdayq6"
      },
      "source": [
        "%%capture \n",
        "!pip install rouge-score\n",
        "!pip install import-ipynb\n",
        "!pip install fasttext\n",
        "!pip install compress-fasttext\n",
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWEhvsdtYUn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af4edf8-09b3-4d0b-81dd-fc6263bdbfbe"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from collections import Counter, OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "import networkx as nx\n",
        "from rouge_score import rouge_scorer\n",
        "import gensim\n",
        "import fasttext\n",
        "from gensim.models import FastText\n",
        "import compress_fasttext\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords  \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import import_ipynb\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))  \n",
        "import bokeh\n",
        "from bokeh.layouts import gridplot, column, row\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import Div\n",
        "from bokeh.models import Span\n",
        "from bokeh.models import ColumnDataSource, FactorRange\n",
        "from bokeh.transform import dodge\n",
        "from math import pi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFI_qA9o7lSx",
        "outputId": "1c04143a-dac9-4130-c0f9-50f4f75f7934"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSrAegHUapdB",
        "outputId": "f39e86f7-a066-4c05-d217-b6efb56b7e57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUwrmHsVNZ7K",
        "outputId": "f8676ba0-9270-4954-ecac-6b3da57d7857"
      },
      "source": [
        "# load in functions from extract_summarization notebook\n",
        "%cd \"drive/MyDrive/Colab Notebooks\"\n",
        "import extractive_summarization\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "importing Jupyter notebook from extractive_summarization.ipynb\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX1n-xlKw6nO"
      },
      "source": [
        "### Load best models (one per algorithm, per metric)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ork8eYSWw44f"
      },
      "source": [
        "model_dict = {}\n",
        "eval_dict = {}\n",
        "config_dict = {}\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  with open('/content/drive/MyDrive/data/trained_model_' + model + '.pkl', 'rb') as f:\n",
        "    load = pickle.load(f)\n",
        "    eval_dict[model] = load[1]\n",
        "    model_dict[model] = load[2]\n",
        "    config_dict[model] = load[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGKQQHKnEbJp",
        "outputId": "64af567a-65bc-4c2b-a615-1f6fd1714310"
      },
      "source": [
        "config_dict\n",
        "# lsa always no normalization\n",
        "# lsa and textrank both always bow binary\n",
        "# never embeddings used\n",
        "# all 3 have same extraction heuristic for fmeasure vs recall vs precision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'baseline': {('fmeasure', False): \"('baseline', 'num_words_gt')\",\n",
              "  ('fmeasure', True): \"('baseline', 'num_words_gt')\",\n",
              "  ('precision', False): \"('baseline', 'num_words_lt')\",\n",
              "  ('precision', True): \"('baseline', 'num_words_lt')\",\n",
              "  ('recall', False): \"('baseline', 'num_sentences')\",\n",
              "  ('recall', True): \"('baseline', 'num_sentences')\"},\n",
              " 'lsa': {('fmeasure',\n",
              "   False): \"('lsa', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\",\n",
              "  ('fmeasure',\n",
              "   True): \"('lsa', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\",\n",
              "  ('precision',\n",
              "   False): \"('lsa', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\",\n",
              "  ('precision',\n",
              "   True): \"('lsa', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\",\n",
              "  ('recall',\n",
              "   False): \"('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\",\n",
              "  ('recall',\n",
              "   True): \"('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\"},\n",
              " 'textrank': {('fmeasure',\n",
              "   False): \"('textrank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'jaccard', 'num_words_gt')\",\n",
              "  ('fmeasure',\n",
              "   True): \"('textrank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'jaccard', 'num_words_gt')\",\n",
              "  ('precision',\n",
              "   False): \"('textrank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'cosine', 'num_words_lt')\",\n",
              "  ('precision',\n",
              "   True): \"('textrank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'cosine', 'num_words_lt')\",\n",
              "  ('recall',\n",
              "   False): \"('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\",\n",
              "  ('recall',\n",
              "   True): \"('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\"}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zgVxt-kfsEG"
      },
      "source": [
        "## Calculate P-Value with Paired Bootstrap Test\n",
        "\n",
        "For best configuration for each evaluation metric, compare the 3 model types: which model is the best and what is the p-value?\n",
        "1. Calculate difference in stat performance (recall etc.)\n",
        "2. Generate N bootstrapped samples of data \n",
        "3. Train on bootstrapped data\n",
        "4. Calculate difference in performance on bootstraped data\n",
        "5. Count percent of replicate diffs that are >= 2 * original diff = p-value  \n",
        "  - Null hypothesis is that there is no difference and the true effect size is original diff (data happens to be biased towards one model)\n",
        "  - If see a lot of replicated diffs >= 2 * original diff, then null is true and there is no difference between the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga_P8wvHdKXC"
      },
      "source": [
        "def paired_bootstrap(evals, models, configs, pvalue_dict, model1, model2, metric, save_every_cnt = 10, filename = '', restart = True):\n",
        "  embeddings = extractive_summarization.load_embeddings()\n",
        "\n",
        "  # which model is better and by how much\n",
        "  if evals[model1][metric]['mean'] > evals[model2][metric]['mean']:\n",
        "    better_model = model1\n",
        "    other_model = model2\n",
        "  else:\n",
        "    better_model = model2\n",
        "    other_model = model1\n",
        "  diff =  evals[better_model][metric]['mean'] - evals[other_model][metric]['mean']\n",
        "  print(better_model, diff)\n",
        "\n",
        "  # allow start partway through 50 bootstrap samples \n",
        "  if not restart and os.path.exists('/content/drive/MyDrive/data/' + filename + '_' + model1 + '_' + model2 + '_' + str(metric) + '.pkl'):\n",
        "    with open('/content/drive/MyDrive/data/' + filename + '_' + model1 + '_' + model2 + '_' + str(metric) + '.pkl','rb') as f:\n",
        "      results_so_far = pickle.load(f) \n",
        "    gt_diff = results_so_far[0]\n",
        "    lt_diff = results_so_far[1]\n",
        "    start = results_so_far[2]\n",
        "  else:\n",
        "    gt_diff = 0\n",
        "    lt_diff = 0\n",
        "    start = 0\n",
        "\n",
        "  for i in range(start+1, 51):\n",
        "    print('BS', i)\n",
        "    # generate bootstrap samples \n",
        "    bs_sample = {}\n",
        "    bs_sample[model1] = models[model1][metric].sample(n = len(models[model1][metric]), replace = True)\n",
        "    bs_sample[model2] = models[model2][metric].sample(n = len(models[model2][metric]), replace = True)\n",
        "\n",
        "    # retrain both models on bootstrap samples with the current config\n",
        "    bs_results = {}\n",
        "    for m in [model1, model2]:\n",
        "      config = tuple(config_dict[m][metric].strip('(').strip(')').replace(\"'\", \"\").split(', '))\n",
        "      if 'baseline' in config:\n",
        "        tfidf, feature_array = extractive_summarization.corpus_tfidf(bs_sample[m])\n",
        "      else:\n",
        "        tfidf = ''\n",
        "        feature_array = ''\n",
        "      eval_results, _ = extractive_summarization.train_config_loop(bs_sample[m], tfidf, feature_array, embeddings, stop_words, \n",
        "                                                                    [config], eval_only = True)\n",
        "      bs_results[m] = eval_results[str(config)][metric]['mean']\n",
        "      \n",
        "    # find difference in relevant stat\n",
        "    diff_bs = bs_results[better_model] - bs_results[other_model]\n",
        "    print(diff_bs)\n",
        "    if diff_bs >= 2*diff:\n",
        "      gt_diff += 1\n",
        "    else:\n",
        "      lt_diff += 1\n",
        "\n",
        "    # save bootstrap samples every save_every_cnt in case of connection issue, timeout etc.\n",
        "    if ((i % save_every_cnt) == 0 or (i == 50)) and filename != '':\n",
        "      with open('/content/drive/MyDrive/data/' + filename + '_' + model1 + '_' + model2 + '_' + str(metric) + '.pkl', 'wb') as f:\n",
        "        pickle.dump([gt_diff, lt_diff, i], f)\n",
        "        print('saving!', i)\n",
        "       \n",
        "  # calculate p value\n",
        "  pvalue = gt_diff  / (gt_diff + lt_diff)\n",
        "  pvalue_dict[metric] = (better_model, pvalue)\n",
        "\n",
        "  return pvalue_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lztuzb95iKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1376eb4-fb98-4e8d-e66b-8dcfce50fa12"
      },
      "source": [
        "for models in [ ('lsa', 'textrank')]: \n",
        "  if os.path.exists('/content/drive/MyDrive/data/pvalue_' + models[0] + '_' + models[1] + '.pkl'):\n",
        "    # allow loading from pvalue dict with only some of the metrics\n",
        "    with open('/content/drive/MyDrive/data/pvalue_' + models[0] + '_' + models[1] + '.pkl', 'rb') as f:\n",
        "      pvalue_dict = pickle.load(f)\n",
        "  else:\n",
        "    pvalue_dict = {}\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    if str(metric) not in pvalue_dict.keys():\n",
        "      pvalue_dict = paired_bootstrap(eval_dict, model_dict, config_dict, pvalue_dict, models[0], models[1], metric,\n",
        "                                     filename = 'bootstrap_loop', restart = False)\n",
        "    with open('/content/drive/MyDrive/data/pvalue_' + models[0] + '_' + models[1] + '.pkl', 'wb') as f:\n",
        "      pickle.dump(pvalue_dict, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "textrank 0.04495256948552781\n",
            "textrank 0.06040568542246558\n",
            "textrank 0.10128939062273656\n",
            "BS 11\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10130036756464761\n",
            "BS 12\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10451321462295687\n",
            "BS 13\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10332683860968145\n",
            "BS 14\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.0970011743586251\n",
            "BS 15\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10422223256789548\n",
            "BS 16\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.09930432860927407\n",
            "BS 17\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10260851021136275\n",
            "BS 18\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10115815700961783\n",
            "BS 19\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.1005054399105827\n",
            "BS 20\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10295050287525581\n",
            "saving! 20\n",
            "BS 21\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10128942043008232\n",
            "BS 22\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10600342526699069\n",
            "BS 23\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.1036135685068984\n",
            "BS 24\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.0985173532655434\n",
            "BS 25\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.1013300906359903\n",
            "BS 26\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10390676829789702\n",
            "BS 27\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.1001661733491655\n",
            "BS 28\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10379218512356819\n",
            "BS 29\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10291056678455923\n",
            "BS 30\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10123391908919493\n",
            "saving! 30\n",
            "BS 31\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.1034852042312418\n",
            "BS 32\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.0996470657766676\n",
            "BS 33\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10256066006212022\n",
            "BS 34\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('textrank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'cosine', 'num_sentences')\n",
            "0.10332221988129126\n",
            "BS 35\n",
            "('lsa', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Q-_KYeQ8qY"
      },
      "source": [
        "# open completed pvalue dicts\n",
        "for models in [('baseline', 'lsa'), ('baseline', 'textrank'), ('lsa', 'textrank')]: \n",
        "  with open('/content/drive/MyDrive/data/pvalue_' + models[0] + '_' + models[1] + '.pkl', 'rb') as f:\n",
        "    pvalue_dict = pickle.load(f)\n",
        "    print(models, '\\n', pvalue_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfYNJMGMxCUr"
      },
      "source": [
        "### Compare algorithm performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJmsnwR-DQlv"
      },
      "source": [
        "__Best Configs for Respective Best Metric__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7HYlHQU21dZ"
      },
      "source": [
        "%cd figures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb24_eNwPYUX"
      },
      "source": [
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiOVJRQXSMzF"
      },
      "source": [
        "# calculate mean of best metric for each model \n",
        "mean_lst = {'lsa':[], 'textrank':[], 'baseline':[]}\n",
        "for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "  for model in ['lsa', 'textrank', 'baseline']:\n",
        "    mean_lst[model].append(eval_dict[model][metric]['mean'])\n",
        "\n",
        "yvalues = [[i for i in mean_lst[k]] for (k,v) in mean_lst.items()]\n",
        "yvalues = [item for sublist in yvalues for item in sublist]\n",
        "\n",
        "xvalues = [[k for i in range(len(mean_lst[k]))] for (k,v) in mean_lst.items()]\n",
        "xvalues = [item for sublist in xvalues for item in sublist]\n",
        "\n",
        "mean_lst['metrics'] = ['F-Measure', 'Precision', 'Recall']\n",
        "source = ColumnDataSource(data=mean_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_3Ea4Lcz7LT"
      },
      "source": [
        "# side by side metric values for each metric across models (bar plot)\n",
        "p = figure(x_range=['F-Measure', 'Precision', 'Recall'], y_range=(0, 0.5), plot_height=400, title=\"TextRank Uniformally Outperforms other Models\",\n",
        "           toolbar_location=None, tools=\"\",  background_fill_color=\"#fafafa\")\n",
        "\n",
        "p.vbar(x=dodge('metrics', -0.25, range=p.x_range), top='baseline', width=0.2, source=source, legend_label=\"TF-IDF\")\n",
        "p.vbar(x=dodge('metrics',  0.0,  range=p.x_range), top='lsa', width=0.2, source=source, color = 'darkorange', legend_label=\"LSA\")\n",
        "p.vbar(x=dodge('metrics',  0.25, range=p.x_range), top='textrank', width=0.2, source=source, color = 'forestgreen', legend_label=\"TextRank\")\n",
        "\n",
        "# formatting\n",
        "p.grid.grid_line_color=\"white\"\n",
        "p.xaxis.axis_label_text_font_size = '12pt'\n",
        "p.yaxis.axis_label_text_font_size = '12pt'\n",
        "p.title.text_font_size = '14pt'\n",
        "p.xaxis.major_label_text_font_size = '12pt'\n",
        "p.yaxis.major_label_text_font_size = '12pt'\n",
        "p.legend.location = \"top_left\"\n",
        "p.yaxis.minor_tick_line_color = None\n",
        "\n",
        "output_file(\"eval_best_compare.html\")\n",
        "show(p)\n",
        "\n",
        "# note these are the best models for each - not comparing precision vs recall for the same model\n",
        "# LSA always worst (except recall - TODO significant?), then tf-idf, then textrank is the best\n",
        "\n",
        "# precision for TF-IDF, textrank is the only difference that isn't significant. pvalue = 0.10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAp_PWoYDU-B"
      },
      "source": [
        "__Each of Best 9 models for all 3 metrics__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwGnZrTc_lOL"
      },
      "source": [
        "# calculate mean of all metrics for each model \n",
        "\n",
        "for model in ['textrank', 'lsa', 'baseline']:\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    model_dict[model][metric]['precision'] = model_dict[model][metric].rouge.map(lambda row: row['rouge1'].precision)\n",
        "    model_dict[model][metric]['recall'] = model_dict[model][metric].rouge.map(lambda row: row['rouge1'].recall)\n",
        "    model_dict[model][metric]['fmeasure'] = model_dict[model][metric].rouge.map(lambda row: row['rouge1'].fmeasure)\n",
        "\n",
        "mean_lst = {}\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  for stat in ['fmeasure', 'precision', 'recall']:\n",
        "    mean_lst[model + '-' + stat] = []\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    for stat in ['fmeasure', 'precision', 'recall']:\n",
        "      mean_lst[model + '-' + metric[0]].append(model_dict[model][metric][stat].mean())\n",
        "\n",
        "xvalues = [[k for i in range(len(mean_lst[k]))] for (k,v) in mean_lst.items()]\n",
        "xvalues = [item for sublist in xvalues for item in sublist]\n",
        "\n",
        "yvalues = [[i for i in mean_lst[k]] for (k,v) in mean_lst.items()]\n",
        "yvalues = [item for sublist in yvalues for item in sublist]\n",
        "\n",
        "mean_lst['metrics'] = ['F-Measure', 'Precision', 'Recall']\n",
        "source = ColumnDataSource(data=mean_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzT8YRtF0EnP"
      },
      "source": [
        "# side by side metric values for each metric across models (bar plot)\n",
        "p = figure(x_range=['F-Measure', 'Precision', 'Recall'], y_range=(0, 0.5), plot_height=400, plot_width = 800, title=\"TextRank Uniformally Outperforms other Models\",\n",
        "           toolbar_location=None, tools=\"\",  background_fill_color=\"#fafafa\")\n",
        "\n",
        "d = 0.1\n",
        "w = 0.09\n",
        "p.vbar(x=dodge('metrics', -4*d, range=p.x_range), top='baseline-fmeasure', width=w, source=source, legend_label=\"TF-IDF: F-Measure\", color = 'deepskyblue')\n",
        "p.vbar(x=dodge('metrics',  -3*d,  range=p.x_range), top='baseline-precision', width=w, source=source, legend_label=\"TF-IDF: Precision\")\n",
        "p.vbar(x=dodge('metrics',  -2*d, range=p.x_range), top='baseline-recall', width=w, source=source, legend_label=\"TF-IDF: Recall\", color = 'mediumblue')\n",
        "\n",
        "p.vbar(x=dodge('metrics', -1*d+0.015, range=p.x_range), top='lsa-fmeasure', width=w, source=source, legend_label=\"LSA: F-Measure\", color = 'sandybrown')\n",
        "p.vbar(x=dodge('metrics',  0,  range=p.x_range), top='lsa-precision', width=w, source=source, legend_label=\"LSA: Precision\", color = 'darkorange')\n",
        "p.vbar(x=dodge('metrics',  d, range=p.x_range), top='lsa-recall', width=w, source=source, legend_label=\"LSA: Recall\", color = 'orangered')\n",
        "\n",
        "p.vbar(x=dodge('metrics', 2*d+0.015, range=p.x_range), top='textrank-fmeasure', width=w, source=source, legend_label=\"TextRank: F-Measure\", color = 'limegreen')\n",
        "p.vbar(x=dodge('metrics',  3*d,  range=p.x_range), top='textrank-precision', width=w, source=source, legend_label=\"TextRank: Precision\", color = 'forestgreen')\n",
        "p.vbar(x=dodge('metrics',  4*d, range=p.x_range), top='textrank-recall', width=w, source=source, legend_label=\"TextRank: Recall\", color = 'green')\n",
        "\n",
        "# formatting\n",
        "p.grid.grid_line_color=\"white\"\n",
        "p.xaxis.axis_label_text_font_size = '12pt'\n",
        "p.yaxis.axis_label_text_font_size = '12pt'\n",
        "p.title.text_font_size = '14pt'\n",
        "p.xaxis.major_label_text_font_size = '12pt'\n",
        "p.yaxis.major_label_text_font_size = '12pt'\n",
        "p.add_layout(p.legend[0], 'right')\n",
        "p.yaxis.minor_tick_line_color = None\n",
        "\n",
        "output_file(\"eval_all_compare.html\")\n",
        "show(p)\n",
        "\n",
        "# Fmesaure similar to baseline for all textrank models. recall does significantly better than others and its equivalents do almost as well as other models. \n",
        "  # could do better in precision, but recall more important - human content curator will take all the info and revise the summary. Want more info to work with. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK6srcK-xHdT"
      },
      "source": [
        "## Compare Predicted Summaries across Algorithms and Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfyThW_pb8hy"
      },
      "source": [
        "__Length of Predicted Summaries__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adVrMOZPwybK"
      },
      "source": [
        "# length of summary by metric within model\n",
        "summary_len_metric = {'lsa':{}, 'textrank':{}, 'baseline':{}}\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    df = model_dict[model][metric]\n",
        "    df['summary_num_words'] = df.predicted_summary.map(lambda row: len(''.join(row).split(' ')))\n",
        "    df['summary_num_sentences'] = df.predicted_summary.map(lambda row: len(row))\n",
        "    summary_len_metric[model][metric] = [df.summary_num_sentences.mean(), df.summary_num_words.mean()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6vw5DY-YdZ9"
      },
      "source": [
        "def barplot(top, color, yrange, ylabel, title, noy = False):\n",
        "  metrics = ['F-Measure', 'Precision', 'Recall']\n",
        "  \n",
        "  p = figure(x_range=metrics, y_range = yrange, plot_height=350, plot_width = 300, title=title, toolbar_location=None, tools=\"\")\n",
        "  p.vbar(x=metrics, top=top, width=0.9, color = color)\n",
        "\n",
        "  # formatting\n",
        "  p.grid.grid_line_color=\"white\"\n",
        "  p.xaxis.axis_label_text_font_size = '12pt'\n",
        "  p.yaxis.axis_label_text_font_size = '12pt'\n",
        "  p.title.text_font_size = '13pt'\n",
        "  p.xaxis.major_label_text_font_size = '12pt'\n",
        "  p.yaxis.major_label_text_font_size = '12pt'\n",
        "  p.yaxis.axis_label = ylabel\n",
        "  p.yaxis.minor_tick_line_color = None\n",
        "  p.xaxis.major_label_orientation = pi/4\n",
        "\n",
        "  if noy:\n",
        "    p.yaxis.major_label_text_font_size = '0pt'\n",
        "\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dDdyP-TYmJf"
      },
      "source": [
        "p1 = barplot([i[0] for i in list(summary_len_metric['baseline'].values())], '#1F77B4', (0,5.2), ylabel = '# Summary Sentences', title = 'TF-IDF')\n",
        "p2 = barplot([i[0] for i in list(summary_len_metric['lsa'].values())], 'darkorange', (0,5.2), ylabel = '', title = 'LSA', noy = True)\n",
        "p3 = barplot([i[0] for i in list(summary_len_metric['textrank'].values())], 'forestgreen', (0,5.2), ylabel = '', title = 'TextRank', noy = True)\n",
        "\n",
        "suptitle = Div(text = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "h2\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h2>Precision Favors Short Summaries</h2>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "output_file(\"eval_numsentences.html\")\n",
        "show(column(suptitle,gridplot([[p1, p2, p3]])))\n",
        "\n",
        "# all 3 models get very similar sized summaries\n",
        "# precision favors shorter summaries. maximizing overlap with predicted summary, so extra non relevant info hurts \n",
        "# fmeasure balances between recall and precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jFRkoSdbkgW"
      },
      "source": [
        "p1 = barplot([i[1] for i in list(summary_len_metric['baseline'].values())], '#1F77B4', (0,125), ylabel = '# Summary Words', title = 'TF-IDF')\n",
        "p2 = barplot([i[1] for i in list(summary_len_metric['lsa'].values())], 'darkorange', (0,125), ylabel = '', title = 'LSA', noy = True)\n",
        "p3 = barplot([i[1] for i in list(summary_len_metric['textrank'].values())], 'forestgreen', (0,125), ylabel = '', title = 'TextRank', noy = True)\n",
        "\n",
        "suptitle = Div(text = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "h2\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h2>Precision Favors Short Summaries</h2>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "output_file(\"eval_numwords.html\")\n",
        "show(column(suptitle,gridplot([[p1, p2, p3]])))\n",
        "\n",
        "# textrank recall longest in terms of number of words - again good thing to provide more context to human content curators\n",
        "  # (as long as still reasonable precision - from above, 1 in every 5 words is in the true summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRxpuwFb_80"
      },
      "source": [
        "__Relative Length Predicted vs Label Summary__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roMarXqsKDD9"
      },
      "source": [
        "# relative length of actual vs predicted summary per model \n",
        "relative_len_metric = {'lsa':{}, 'textrank':{}, 'baseline':{}}\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    df = model_dict[model][metric]\n",
        "    df['article_num_words'] = df.summary.map(lambda row: len(row.split(' ')))\n",
        "    df['diff'] = df.summary_num_words - df.article_num_words \n",
        "    relative_len_metric[model][metric] = df['diff'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbw20O_VcES_"
      },
      "source": [
        "p1 = barplot([i for i in list(relative_len_metric['baseline'].values())], '#1F77B4', (-25,75), ylabel = '# Summary Words', title = 'TF-IDF')\n",
        "p2 = barplot([i for i in list(relative_len_metric['lsa'].values())], 'darkorange', (-25,75), ylabel = '', title = 'LSA', noy = True)\n",
        "p3 = barplot([i for i in list(relative_len_metric['textrank'].values())], 'forestgreen', (-25,75), ylabel = '', title = 'TextRank', noy = True)\n",
        "\n",
        "suptitle = Div(text = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "h3\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h3>Recall Summaries Longer; Precision Summaries Shorter than Gold Standard Summary</h3>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "output_file(\"eval_relativelength.html\")\n",
        "show(column(suptitle,gridplot([[p1, p2, p3]])))\n",
        "\n",
        "# getting longer than actual summary is ok because using article sentences, which we know are longer and will have some unnecessary information in them\n",
        "# human labeler will narrow down from too much text to correct summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnJXLTqmdZxA"
      },
      "source": [
        "__Length of Predicted Extracted Sentences__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjoiBqHcKGRl"
      },
      "source": [
        "# length of sentences: words per sentence for each model\n",
        "sentence_len_metric = {'lsa':{}, 'textrank':{}, 'baseline':{}}\n",
        "for model in ['lsa', 'textrank', 'baseline']:\n",
        "  for metric in [('fmeasure', False), ('precision', False), ('recall', False)]:\n",
        "    df = model_dict[model][metric]\n",
        "    df['article_num_words'] = df.predicted_summary.map(lambda row: np.mean([len(i.split(' ')) for i in row]))     \n",
        "    sentence_len_metric[model][metric] = df['article_num_words'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdfNnd0YdCbH"
      },
      "source": [
        "p1 = barplot([i for i in list(sentence_len_metric['baseline'].values())], '#1F77B4', (0,30), ylabel = '# Summary Words', title = 'TF-IDF')\n",
        "p2 = barplot([i for i in list(sentence_len_metric['lsa'].values())], 'darkorange', (0,30), ylabel = '', title = 'LSA', noy = True)\n",
        "p3 = barplot([i for i in list(sentence_len_metric['textrank'].values())], 'forestgreen', (0,30), ylabel = '', title = 'TextRank', noy = True)\n",
        "\n",
        "suptitle = Div(text = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "h3\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h3>Extracted Sentences of Uniform Length across Models</h3>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "output_file(\"eval_sentencelength.html\")\n",
        "show(column(suptitle,gridplot([[p1, p2, p3]])))\n",
        "\n",
        "# summary sentences pulled of a similar length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuq2zMbVfTQF"
      },
      "source": [
        "### Qualitative Comparison\n",
        "- Compare textrank recall vs precision. Get example where precision too vague, recall to rambly\n",
        "- Compare textrank vs lsa recall. Is LSA qualitatively worse? Why?\n",
        "- Evaluate abstractive model by itself: examples of short comings of abstractive models\n",
        "- Compare abstractive recall vs textrank recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjeYl-ugMHN"
      },
      "source": [
        "__Extractive: TextRank Recall vs Precision__\n",
        "  - Zimmerman case and parent's fear: precision is more too the point, recall model too rambly, repeats itself\n",
        "    - Recall: 'If, during this 16-month ordeal, that thought never crossed your mind, then you have no idea what it is like to be the parent of a young, black male in America.', \"Opinion: Zimmerman case echoes issues of race, guns  But this is what it's like to be the parent of a young, black male in this country.\", 'After it was determined I was not the black male he was looking for, he let me go.', 'To be the parent of a young black man in this country is to be torn between wanting your son to see the world with no boundaries and warning him of the boundaries that are out there.', \"That's when he pulled over, got out of his car, drew his weapon and yelled he was going to shoot me if I didn't stop running.\"\n",
        "    - Precision: Opinion: Zimmerman case echoes issues of race, guns  But this is what it's like to be the parent of a young, black male in this country.\n",
        "  - Carbon footprint of Football (soccer) and efforts to be more environmentally friendly: precision too brief, recall includes more details of the article\n",
        "    - Recall: \"The cistern stores rainwater that can be used to irrigate the pitch, but compared to most clubs' water consumption, it is just a drop in the ocean.\", '\"Ethical Consumer\" estimates that it takes an astonishing 20,000 liters of water per day to maintain a football pitch in the English Premier League, and at Camp Nou, home of Champions League winners Barcelona, up to 54,000 liters of water are needed to irrigate the pitch on a hot day.', 'The stadium is covered with a \"living roof\" of plants that provide a natural air filtration system, gray water is supplied from two huge ponds near the stadium and solar panels are used to heat water for the toilets.', 'It is not a huge investment, but it is enough to make a difference, and the club say the scheme will pay for itself by reducing energy expenditure.'\n",
        "    - Precision: The cistern stores rainwater that can be used to irrigate the pitch, but compared to most clubs' water consumption, it is just a drop in the ocean.\n",
        "\n",
        "__TextRank recall vs LSA recall__\n",
        "  - Release of Mac OS X Lion: LSA misses some of the main points and hinges on details instead\n",
        "    - LSA: 'Brian X. Chen of Wired wrote that \"some of Lion\\'s iOS-like features scale up very well, while others behave very poorly in a desktop environment.\"', \"He found the software's iPad-like scrolling feature distracting and dizzying (he eventually disabled it) but said he enjoyed the system's app-opening full-screen mode and praised new sharing and auto-save functions.\", 'The new system, however, does not run iPhone or iPad apps (at least, not yet) and it does run Adobe Flash -- something Steve Jobs and company have summarily banished from their mobile devices.'\n",
        "    - TextRank: \"Lion, the latest version of Apple's operating software for its Mac computers, was released to the public on Wednesday.\", 'Mac OS X Lion is available as a $29.99 upgrade for people with the latest version of the Snow Leopard operating system.', 'Lion is the latest in a long line of cat-named operating systems rolled out by Apple for its computers.'\n",
        "    - Gold standard: \"Apple's Mac OS X Lion released on Wednesday    The new operating system for Macs adopts features from mobile devices    New system has 250 new features, Apple says\"\n",
        "  - Dick Cheney heart attack\n",
        "    - LSA: Cheney had been on the cardiac transplant list for more than 20 months.\n",
        "    - TextRank: 'Former Vice President Dick Cheney has been released from a Virginia hospital 10 days after undergoing a heart transplant, his office said Tuesday.'\n",
        "    - Gold standard: Dick Cheney has been released from a Virginia hospital    He was recovering from a heart transplant    Cheney, 71, suffered from at least five heart attacks since 1978    The former Wyoming congressman served as a vice president under President George W. Bush\n",
        "\n",
        "\n",
        "__Abstractive__\n",
        "- Examples of abstractive creating information that isn't in the article\n",
        "  - \"john avlon: if Zimmerman was black, he wouldn't have been the parent of a young black man. he says he's a parent of young black males in the middle-class, predominantly white neighborhood..\" -- article often used the phrase \"If Zimmerman was black...\" and also discussed the fear of parents of Black children. But this phrase combines those two ideas in a non-sensical way.\n",
        "    \n",
        "- Repetitive (and incorrect information)\n",
        "  - Part of generated summary from story about children being kidnapped and brought to Cuba: \"he says the family is receiving \"exceptional cooperation\" from the united states and the united states.\"\n",
        "    - In reality, the article discussed recieving cooperation from the Cuban government\n",
        "- Not all capitalizaton is recovered correctly\n",
        "  - \"jeb Bush is the clear Republican presidential frontrunner\"\n",
        "- Sentences are cut off part way through because can only specify max characters\n",
        "  - \"the club says it has reduced landfill by 85 percent, moved to electric vehicles at the ground, and used eco-friendly paper for match-day programs. but some clubs are starting'\"\n",
        "\n",
        "In general, pastes together phrases from the articles in grammatically correct ways. Many chunks of words are directly from article.    \n",
        "    \n",
        "If wanted a system with no or very little human intervention, then abstractive summarization would be better. Summaries are more to the point (because can drop extraneous details in a sentence and only take the important part) and sentences are in a logical order. But if we have a human doing post-processing, then extractive is less likely to make mistakes. \n",
        "\n",
        "__Abstractive vs Extractive__\n",
        "- Zimmerman case: abstractive misses information\n",
        "  - Abstractive misses a lot of information, main point. Extractive if long, but hits all of the important points \n",
        "- Carbon footprint of Football: abstractive more to the point without extraneous details\n",
        "  - 'a 2008 survey by \"Ethical Consumer\" looked at the eco-credentials of clubs in the english premier league. the club says it has reduced landfill by 85 percent, moved to electric vehicles at the ground, and used eco-friendly paper for match-day programs. it's possible for small clubs to do their bit, but their use of solar'\n",
        "- Dick Cheney heart transplant: abstractive misses information\n",
        "  - \"former vice president was on the cardiac transplant list for more than 20 months. he had a left ventricular assis\" (cut off)\n",
        "  - Doesn't get main point about successful heart transplant, released from hospital. Extractive did get. \n",
        "- Storm in Sardinia: abstractive sentences in a better order\n",
        "  - Extractive: 'But Vargiu said a lot of ministry staff and emergency workers had come to aid the town, and there were many volunteers helping its inhabitants.', 'Vargiu said a lot of money would be needed for Sardinia to recover from the storm, with many families and businesses affected.', 'There are rivers of water in the town.', 'Olbia\\'s councilor for tourism, Marco Vargiu, told CNN that the storm had been \"a disaster\" for the town of about 70,000, with authorities saying 13 people had died in the broader area.', 'Vargiu said the houses were filled with a mixture of water, sand and rubbish.'\n",
        "    - Order of sentences doesn't make sense\n",
        "  - Abstractive: \"a storm has killed at least 16 people on the italian island of Sardinia. the government has allocated 20 million euros in immediate aid to the island. the money will be used to help save lives, assist the displaced and repair roads. the island has received six months' worth of rainfall in 12 hours.\"\n",
        "\n",
        "Extractive more consistently hits all of the important points in an article, even if the ordering of sentences is confusing and it can be too long. Given the humans are editing these summaries, getting all of the information is more important. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUpSWFN4d0Mu"
      },
      "source": [
        "model_dict['textrank'][('recall', False)][['summary', 'predicted_summary']].tail(15).values"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}