{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BeamSearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/BeamSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqlhDt_ctp1"
      },
      "source": [
        "# Text generation with pre-trained Transformers\n",
        "In this assignment we will work with Pre-trained Transformers such as GPT2 for generating text from a given sequence. Transformers aim to address the long term dependency issue in sequence-to-seuqence prediction by using concepts such as self-attention and positional encoding. GPT2 is a langauge model, pretrained on text generation, that can be used as a multi-task learner for tasks such as summarization, question-answering, and other generation tasks. This assignment's focus is on using GPT2 to generate text via greedy decoding and beam search. For more background on beam search, see [Jurafsky and Martin, chapter 11](https://web.stanford.edu/~jurafsky/slp3/11.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdzIfi61MGbz"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVs6K8YcLvXC"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4S1DJT8MFeh"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RiepaMyQfhl"
      },
      "source": [
        "sentences = ['I like walking and', \n",
        "             'Martha wanted to read a book that',\n",
        "             'Thomas is studying computer science to',\n",
        "             'Their friendship inspired',\n",
        "             'We should take the trash out since',\n",
        "             'I am not a fan of coffee because',\n",
        "             'I could not complete my homework by the deadline because',\n",
        "             'The last semester was much easier due to',\n",
        "             'I will be painting the walls white so that'\n",
        "            ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3sku1QrSIa"
      },
      "source": [
        "We apply greedy decoding to get predictions for each sentence here. This function returns the text output of greedy decoding. Modify it to return a tuple (ordered pair) of text and average log-likelihood per word for each sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKvC89c8Sjy2"
      },
      "source": [
        "__Interpretation:__ I interpreted the prompt as the average log-likelihood per _predicted_ word for each sentence. I did not include probabilities for the given words in the input sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IFNig2eRUtg"
      },
      "source": [
        "# Output: list of (predicted sentence, average log likelhiood of predicted words)\n",
        "\n",
        "def greedy_decode(sentences, max_length, tokenizer):\n",
        "  # Obtain loss from output and calculate\n",
        "  # log likelihood for each sentence\n",
        "  texts = []\n",
        "  for sentence in sentences:\n",
        "    word_probs = 0 # sum of log likelihood for each word\n",
        "    cnt = 0\n",
        "    predicted_sentence = copy.copy(sentence)\n",
        "    # Predict a word each itertation until the max length\n",
        "    for i in range(max_length):\n",
        "      indexed_tokens = tokenizer.encode(predicted_sentence)\n",
        "      token_tensors = torch.tensor([indexed_tokens])\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output = model(token_tensors, labels=token_tensors)\n",
        "        predictions = output[1]\n",
        "\n",
        "      # add to sum of log likelihood of next predicted word\n",
        "      word_probs += torch.max(predictions[0, -1, :])\n",
        "      cnt += 1\n",
        "      # get highest probability next predicted word\n",
        "      predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "      predicted_sentence = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "    # save predicted sentence and average log likelihood of predicted words for each sentence\n",
        "    texts.append((predicted_sentence, word_probs / cnt))\n",
        "\n",
        "  return texts"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWayRy7eQoKC",
        "outputId": "431f36cb-f1a1-481f-fab5-52e86e47182b"
      },
      "source": [
        "texts = greedy_decode(sentences, max_length=25, tokenizer=tokenizer)\n",
        "texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I like walking and biking, but I don't like being in a car. I like to be in a car. I like to be in\",\n",
              "  tensor(-126.9903)),\n",
              " ('Martha wanted to read a book that she had read in college. She was a little nervous about it, but she was excited about it. She was a little',\n",
              "  tensor(-121.1514)),\n",
              " ('Thomas is studying computer science to become a professor of computer science at the University of California, Berkeley. He is also a member of the Board of Trustees',\n",
              "  tensor(-87.4711)),\n",
              " (\"Their friendship inspired him to become a writer and a writer's assistant. He also wrote a book about his life and career.\\n\\n\\n\",\n",
              "  tensor(-109.5394)),\n",
              " ('We should take the trash out since it\\'s not going to be a problem,\" he said.\\n\\n\\n\"We\\'re going to have to do something about',\n",
              "  tensor(-102.8561)),\n",
              " ('I am not a fan of coffee because it is not good for you. I am a fan of coffee because it is not good for you.\\n\\n\\nI',\n",
              "  tensor(-91.6273)),\n",
              " ('I could not complete my homework by the deadline because I was too busy with my homework to finish it. I was so tired and tired of being bored. I was so tired',\n",
              "  tensor(-116.9200)),\n",
              " ('The last semester was much easier due to the fact that I had a lot of time to study and get my degree. I was able to get my degree in the',\n",
              "  tensor(-116.1225)),\n",
              " ('I will be painting the walls white so that I can see the faces of the people who are here. I will be painting the walls white so that I can see the',\n",
              "  tensor(-98.0968))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPYdTx9rMUO"
      },
      "source": [
        "You'll be implementing **beam search**, which returns a list of the $k$ most likely output sequences for each sentence. For this assignment, let $k = 8$. For the first token in the generated text, you will select the top $k$ output tokens. Then, for the next token, find the $k$-best continuations for each of those $k$ hypotheses and select the $k$-best overall. Return the $k$-best overall hypotheses according to average log likelihood per word. Note that if we don't average per word, the decoder will simply prefer shorter outputs. As above, return tuples of text output and average log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXS1ZBrZjYk"
      },
      "source": [
        "# TODO: only choosing among ones with max length? Then why does average per word matter? \n",
        "  # average per word at each step?\n",
        "# or choosing among all sequences regardless of if reached max length? \n",
        "\n",
        "# TODO: what about end of sequence token?\n",
        "\n",
        "# input: \n",
        "  # sentences_list = list of (predicted sentence, sum of log likelihood predicted words in sentence)\n",
        "  # length = current length\n",
        "  # max length = maximum length of generated sentence\n",
        "  # k_num = beam length k \n",
        "  # tokenizer \n",
        "\n",
        "# output: list of all (predicted sentences of length max length, average log likelihood of predicted words)\n",
        "\n",
        "def recursion(sentences_list, length, max_length, k_num, tokenizer):\n",
        "\n",
        "  # end recursion if reach max length\n",
        "  if length == max_length:\n",
        "    # calculate average log likelihood per word\n",
        "    sentences_list = [(i[0], i[1] / max_length) for i in sentences_list]  ## TODO does this need to be average?\n",
        "    # output final sequences and average log likelihoods\n",
        "    return sentences_list\n",
        "\n",
        "  else:\n",
        "    kseq = []\n",
        "    for input in sentences_list:\n",
        "      s = input[0] # sentence\n",
        "      p = input[1] # sum log likelihood probability\n",
        "      # encode sentence\n",
        "      indexed_tokens = tokenizer.encode(s)\n",
        "      token_tensors = torch.tensor([indexed_tokens])\n",
        "      # predict\n",
        "      with torch.no_grad():\n",
        "        output = model(token_tensors, labels=token_tensors)\n",
        "        predictions = output[1]\n",
        "      # generate k best following indexes and get log likelihoods\n",
        "      predicted_indexes = torch.argsort(predictions[0, -1, :], descending = True)[0:k_num]\n",
        "      predicted_probs = torch.sort(predictions[0, -1, :], descending = True)[0][0:k_num]\n",
        "      # decode each index and keep track of in list + add to probability sum\n",
        "      for i in range(len(predicted_indexes)):\n",
        "        predicted_sentence = tokenizer.decode(indexed_tokens + [predicted_indexes[i]])\n",
        "        kseq.append((predicted_sentence, p + predicted_probs[i]))\n",
        "\n",
        "    # choose k best at this length for the frontier\n",
        "    probs = [i[1].item() for i in kseq] ### TODO do probs need to be average?\n",
        "    best_indexes = np.argsort(probs)[::-1][0:k_num]\n",
        "    kseq = [kseq[i] for i in best_indexes]\n",
        "\n",
        "    return recursion(kseq, length + 1, max_length, k_num, tokenizer)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enI6j6iVS8Na"
      },
      "source": [
        "# output: list of lists. One outer list per input sentence. Inner list: k best (predicted sentence, average log likelhiood of predicted words)\n",
        "\n",
        "def beam_search(sentences, max_length, tokenizer, k_num=8):\n",
        "\n",
        "  texts = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "\n",
        "    # recursively generate k best sequences \n",
        "    predicted_sentence = copy.copy(sentence)\n",
        "    predicted_sequences = recursion([[predicted_sentence, 0]], 0, max_length, k_num, tokenizer)\n",
        "\n",
        "    texts.append(predicted_sequences)\n",
        "\n",
        "  return texts"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl4p-30eRuLu",
        "outputId": "bf76b1e6-fa04-4b98-ebeb-3cb2315b0305"
      },
      "source": [
        "beam_search(sentences, 25, tokenizer, 8)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA)',\n",
              "   tensor(-34.5525)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA),',\n",
              "   tensor(-34.6284)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA).',\n",
              "   tensor(-34.7347)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA,',\n",
              "   tensor(-34.7452)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA)—',\n",
              "   tensor(-34.7535)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA)-',\n",
              "   tensor(-34.7779)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA)(',\n",
              "   tensor(-34.7782)),\n",
              "  ('I like walking and talking,\" Mr. Johnson added.<|endoftext|>The U. S.\\'S.–Iran Free-Trade Agreement (USFATA-',\n",
              "   tensor(-34.7798))],\n",
              " [('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with the',\n",
              "   tensor(-67.5328)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with a',\n",
              "   tensor(-67.5493)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with an',\n",
              "   tensor(-67.6157)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with this',\n",
              "   tensor(-67.6318)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with me',\n",
              "   tensor(-67.6459)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with one',\n",
              "   tensor(-67.6574)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with her',\n",
              "   tensor(-67.6575)),\n",
              "  ('Martha wanted to read a book that said: The American way has failed us all,\" Mrs. Bowers says now.<|endoftext|>It may have just begun with some',\n",
              "   tensor(-67.6625))],\n",
              " [(\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixtieth\",\n",
              "   tensor(-55.6216)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixtys\",\n",
              "   tensor(-55.9391)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixt of\",\n",
              "   tensor(-55.9778)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixto\",\n",
              "   tensor(-55.9917)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixteen\",\n",
              "   tensor(-55.9975)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixteenth\",\n",
              "   tensor(-56.0177)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixt.\",\n",
              "   tensor(-56.0330)),\n",
              "  (\"Thomas is studying computer science to prepare him as one-third head of computer engineering at Columbia University's College Park School of Journalism, one-sixtth\",\n",
              "   tensor(-56.0370))],\n",
              " [('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. Dr',\n",
              "   tensor(-30.0660)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D.\\n',\n",
              "   tensor(-30.0984)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. \"',\n",
              "   tensor(-30.1263)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. is',\n",
              "   tensor(-30.1578)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. D',\n",
              "   tensor(-30.1603)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. was',\n",
              "   tensor(-30.1864)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. and',\n",
              "   tensor(-30.1897)),\n",
              "  ('Their friendship inspired him as well.\" - Dr. Jody Dyer of Dr Jody J D. Dr D. Dr D. The',\n",
              "   tensor(-30.1970))],\n",
              " [('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still still',\n",
              "   tensor(-50.8336)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still will',\n",
              "   tensor(-51.0280)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still Still',\n",
              "   tensor(-51.0552)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still.',\n",
              "   tensor(-51.0652)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still,',\n",
              "   tensor(-51.0704)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still\\n',\n",
              "   tensor(-51.0877)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still to',\n",
              "   tensor(-51.0898)),\n",
              "  ('We should take the trash out since there will still still still still still still will still still still will still still still still still still still still still still still.\"',\n",
              "   tensor(-51.0924))],\n",
              " [('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to be',\n",
              "   tensor(-26.5127)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have ever been',\n",
              "   tensor(-26.6258)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to come',\n",
              "   tensor(-26.7638)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to,',\n",
              "   tensor(-26.7716)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to have',\n",
              "   tensor(-26.7764)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to.',\n",
              "   tensor(-26.7769)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have to exist',\n",
              "   tensor(-26.7877)),\n",
              "  ('I am not a fan of coffee because of it,\" Mr. Smith said.<|endoftext|>It may have to do more harm than harm if there ever will have ever be',\n",
              "   tensor(-26.7948))],\n",
              " [('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject-',\n",
              "   tensor(-53.1141)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subjects',\n",
              "   tensor(-53.1267)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject.',\n",
              "   tensor(-53.1800)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject of',\n",
              "   tensor(-53.2133)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject,',\n",
              "   tensor(-53.2164)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subjective',\n",
              "   tensor(-53.2346)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject/',\n",
              "   tensor(-53.2424)),\n",
              "  ('I could not complete my homework by the deadline because there had to have already come to an agreed-for-theoretically agreed-for-the-actual-subject and',\n",
              "   tensor(-53.2562))],\n",
              " [('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr.',\n",
              "   tensor(-42.0880)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr.,',\n",
              "   tensor(-42.1197)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr.\\'',\n",
              "   tensor(-42.1650)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr as',\n",
              "   tensor(-42.1705)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr,',\n",
              "   tensor(-42.2071)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr and',\n",
              "   tensor(-42.2522)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr.:',\n",
              "   tensor(-42.2529)),\n",
              "  ('The last semester was much easier due to all of this.\"<|endoftext|>It may have just as important of implications on how much of America thinks of Donald Donald Jr Jr\\'s',\n",
              "   tensor(-42.2567))],\n",
              " [('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever be',\n",
              "   tensor(-67.9974)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will always be',\n",
              "   tensor(-68.1831)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever have',\n",
              "   tensor(-68.2438)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever will',\n",
              "   tensor(-68.2611)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever really',\n",
              "   tensor(-68.2831)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever once',\n",
              "   tensor(-68.2850)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever been',\n",
              "   tensor(-68.2893)),\n",
              "  ('I will be painting the walls white so that when the time comes for me on stage with all of us on board with it all, there can only ever will ever ever',\n",
              "   tensor(-68.2984))]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkZkCHSVubEh"
      },
      "source": [
        "# TODO check against model.generate() method - not the same\n",
        "# TODO observations "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFrvgUGFxTaA"
      },
      "source": [
        "**TODO:** Record your observations here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuN4xYMu1cq1"
      },
      "source": [
        "For further exploration, you can experiment with $k$ to see how the fluency of text changes."
      ]
    }
  ]
}