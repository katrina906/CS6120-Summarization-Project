{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_rank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2UBhzn5wJ7It/HQTpVBHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/text_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDMy4RQPzHg7"
      },
      "source": [
        "# load english wiki word vectors: https://fasttext.cc/docs/en/pretrained-vectors.html\r\n",
        "# load wiki fasttext bin and save only model object: smaller \r\n",
        "#ft = FastText.load_fasttext_format(\"/content/drive/MyDrive/data/wiki.en.bin\")\r\n",
        "#ft.wv.save('/content/drive/MyDrive/data/wiki.en.model')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "06eb1d8a-e82f-4e1c-8015-730822b19c32"
      },
      "source": [
        "!pip install rouge-score\r\n",
        "!pip install fasttext\r\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "#!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.10.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KEwm839obTgS",
        "outputId": "fc471807-26ff-4f13-e38b-d61843823539"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip glove*.zip\r\n",
        "# TODO: read into drive so don't have to wget every time? "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-10 00:05:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-03-10 00:05:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-03-10 00:05:07--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2021-03-10 00:07:47 (5.14 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.50d.txt        \n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.100d.txt       A\n",
            "\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: glove.6B.200d.txt       y\n",
            "y\n",
            "y\n",
            "\n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99-qm9GrHPX"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import itertools\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "from collections import Counter, OrderedDict\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "from rouge_score import rouge_scorer\r\n",
        "import fasttext\r\n",
        "import gensim\r\n",
        "from gensim.models import FastText\r\n",
        "import sys\r\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\r\n",
        "from nltk.corpus import stopwords  \r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))  \r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kSai_eCrMBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2f347f37-d760-4e51-da71-60bd3debd0ef"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_1p-6myxP_V"
      },
      "source": [
        "# load data\r\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")\r\n",
        "df = df.head(10000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\r\n",
        "# always do this function\r\n",
        "def text_cleaning(df):\r\n",
        "  # downcase everything\r\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\r\n",
        "  # remove punctuation \r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbTbtLtSs7yJ"
      },
      "source": [
        "# cleaning depending on configuration\r\n",
        "def text_cleaning_config(doc, config, stop_words):\r\n",
        "  words = [sentence.split() for sentence in doc]\r\n",
        "  if 'stopwords' in config:\r\n",
        "    words = [[w for w in sentence if not w in stop_words] for sentence in words]\r\n",
        "  if 'stem' in config:\r\n",
        "    stemmer = PorterStemmer()\r\n",
        "    words = [[stemmer.stem(w) for w in sentence] for sentence in words]\r\n",
        "  if 'lemma' in config:\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    words = [[lemmatizer.lemmatize(w) for w in sentence] for sentence in words]\r\n",
        "\r\n",
        "  doc = [' '.join(sentence) for sentence in words]\r\n",
        "\r\n",
        "  return doc"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHiiHeyCaEhW"
      },
      "source": [
        "### Train TFIDF in Corpus\r\n",
        "Used in baseline model to sum tfidf scores within each sentence in each document "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLrQnm0VYygp"
      },
      "source": [
        "def corpus_tfidf(df):\r\n",
        "  # list of words in each article\r\n",
        "  corpus = df.sentences_cleaned.to_list()\r\n",
        "  corpus = [''.join(article) for article in corpus]  \r\n",
        "  corpus = [article.split(' ') for article in corpus]\r\n",
        "\r\n",
        "  # tfidf trained on entire corpus: document = article\r\n",
        "  tfidf_vec = TfidfVectorizer(analyzer = 'word', \r\n",
        "                          tokenizer = lambda doc: doc, preprocessor = lambda doc: doc, token_pattern = None)\r\n",
        "                          # already did preprocessing, so using identity functions for tokenizer and preprocessor\r\n",
        "  tfidf = tfidf_vec.fit_transform(corpus) # sparse arrays of scores for each word in each article. articles x words\r\n",
        "  feature_array = list(tfidf_vec.get_feature_names())\r\n",
        "  \r\n",
        "  return tfidf, feature_array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \r\n",
        "Default: unigram bag of words with counts\r\n",
        "Options: \r\n",
        "1. Bow\r\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\r\n",
        "  - tf: term frequency normalization \r\n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\r\n",
        "  - idf: inverse document normalization \r\n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\r\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words\r\n",
        "2. Embeddings (pre-trained)\r\n",
        "  - GloVe\r\n",
        "  - Fasttext\r\n",
        "    - Advantage: generate embeddings for out of vocabulary words based on their parts\r\n",
        "    - But memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \r\n",
        "def vector_representation(doc, configuration, embeddings):\r\n",
        "\r\n",
        "  # list of words in each sentence \r\n",
        "  words = [sentence.split() for sentence in doc]\r\n",
        "\r\n",
        "  if 'bow' in configuration:\r\n",
        "\r\n",
        "    # include bigrams and/or trigrams (in addition to unigrams) in bow \r\n",
        "    grams = []\r\n",
        "    if 'bigram' in configuration or 'all' in configuration:\r\n",
        "      bigrams = [list(nltk.bigrams(sentence)) if len(sentence) >= 2 else '' for sentence in words ]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] for words in sentence] for sentence in bigrams]) # combine tuples of words into string\r\n",
        "    if 'trigram' in configuration or 'all' in configuration:\r\n",
        "      trigrams = [list(nltk.trigrams(sentence)) if len(sentence) >= 3 else '' for sentence in words ]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] + ' ' + words[2] for words in sentence] for sentence in trigrams]) # combine tuples of words into string\r\n",
        "    # concat with unigrams per sentence\r\n",
        "    for i in range(len(grams)):\r\n",
        "      words = [grams[i][j] + words[j] for j in range(len(words))] \r\n",
        "\r\n",
        "    # bag of words with binary indicators for words/n-grams rather than counts\r\n",
        "    if 'binary' in configuration: \r\n",
        "      words = [set(sentence) for sentence in words]\r\n",
        "\r\n",
        "    # bag of words: # sentences x # unique words\r\n",
        "    vec = DictVectorizer()\r\n",
        "    bow = vec.fit_transform(Counter(f) for f in words)\r\n",
        "\r\n",
        "    # term frequency normalization\r\n",
        "    if 'tf' in configuration: \r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = False)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "    # term frequency-inverse document frequency normalization\r\n",
        "    if 'tfidf' in configuration:\r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = True)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "\r\n",
        "    return bow\r\n",
        "\r\n",
        "  # possible extension: continued training on specific corpus. Probably unnecessary since wikipedia and news article words should be similar\r\n",
        "  if 'embedding' in configuration:\r\n",
        "\r\n",
        "    if 'glove' in configuration:\r\n",
        "      word_embeddings = embeddings['glove']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      # if unknown word, give embedding = 0 \r\n",
        "      sentence_vectors = []\r\n",
        "      for sentence in doc_processed:\r\n",
        "        sentence_vectors.append(sum([word_embeddings.get(word, np.zeros(100,)) for word in sentence.split()])/(len(sentence.split())))\r\n",
        "\r\n",
        "      return np.array(sentence_vectors)\r\n",
        "\r\n",
        "    # fasttext.\r\n",
        "    if 'fasttext' in configuration:\r\n",
        "      word_embeddings = embeddings['fasttext']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      sentence_vectors = []\r\n",
        "      for sentence in doc_processed:\r\n",
        "        sentence_vectors.append(sum([word_embeddings[word] for word in sentence.split()])/(len(sentence.split())))\r\n",
        "\r\n",
        "      return np.array(sentence_vectors)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZyF8DHeaVrY"
      },
      "source": [
        "### PageRank Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "# TODO: other similarity metrics?\r\n",
        "# TODO: other algorithms\r\n",
        "def pagerank(bow):\r\n",
        "  # similarity matrix between sentences\r\n",
        "  sim =  cosine_similarity(bow)\r\n",
        "  # graph where node = sentence, edge weight = simialarity score\r\n",
        "  G = nx.from_numpy_array(sim)\r\n",
        "  # page rank\r\n",
        "  pr = nx.pagerank(G)\r\n",
        "  # sort keys in order of page rank\r\n",
        "  bestkeys = sorted(pr, key=pr.get, reverse=True)\r\n",
        "\r\n",
        "  return bestkeys"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc6rDMo6aXL9"
      },
      "source": [
        "### Baseline Model\r\n",
        "- Train TF-IDF on entire corpus where document = article. Get a score for each word in each document\r\n",
        "- Sum scores for all words in each sentence \r\n",
        "- Produce sentences with highest total TF-IDF score \r\n",
        "\r\n",
        "Idea: Sentences that are indicative of the specifics of the article. High frequency in the article, but specific to the article\r\n",
        "\r\n",
        "Could also try straight term frequencies within the article. (or weighted like above so fractional of most frequent rather than diff. magnitudes). Would need to drop stop words first (https://stackabuse.com/text-summarization-with-nltk-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_UiVtJZGnZ"
      },
      "source": [
        "def tfidf_sum(doc, feature_array, tfidf):\r\n",
        "\r\n",
        "  # sum tfidf score within each sentence. \r\n",
        "  # Normalize by length of sentence. Otherwise recommend longest sentences \r\n",
        "  sentence_words = [sentence.split() for sentence in doc]\r\n",
        "  sentence_scores = [np.sum([tfidf[0,feature_array.index(word)] for word in sentence]) / len(sentence) for sentence in sentence_words]\r\n",
        "\r\n",
        "  # sort keys in order of summed tfidf score\r\n",
        "  bestkeys = np.argsort(sentence_scores)[::-1]\r\n",
        "\r\n",
        "  return bestkeys"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cJLGcOJuUo"
      },
      "source": [
        "### Extract Summary\r\n",
        "Grab best sentences based on ranking mechanism     \r\n",
        "Length of summary (Number of sentences)?\r\n",
        "- Number of sentences: generate 1 summary sentence per text sentence (average)\r\n",
        "  - Problem: text sentences are much longer than summary sentences, and since we are producing text sentences as our predicted summary, predicted summary is much longer than label summary\r\n",
        "- Number of words: generate 20 summary words per 1 text word\r\n",
        "  - Strict version: words in summary must be less than the threshold\r\n",
        "  - Less strict version: can go over limit by 1 sentence if reach threshold within the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(bestkeys, doc, config):\r\n",
        "\r\n",
        "  # summary based on number of sentences \r\n",
        "  if 'num_sentences' in config:\r\n",
        "    max_sentences = int(np.floor(len(doc) / 6)) # average 6 summary sentences per doc sentence\r\n",
        "    return [doc[i] for i in  bestkeys[0:max_sentences]]\r\n",
        "  # summary based on number of words\r\n",
        "  if 'num_words_gt' in config or 'num_words_lt' in config:\r\n",
        "    summary = []\r\n",
        "    num_words = 0\r\n",
        "    max_words = np.floor(len(''.join(doc).split(' ')) / 20) # average 20 summary words per text word\r\n",
        "    for i in bestkeys:\r\n",
        "      num_words += len(doc[i].split(' ')) \r\n",
        "      # strict version: words in summary must be less than threshold\r\n",
        "      if 'num_words_lt' in config:\r\n",
        "        if num_words >= max_words:\r\n",
        "          return summary\r\n",
        "        summary.append(doc[i])\r\n",
        "      # less strict version: can go over limit by 1 sentence \r\n",
        "      elif 'num_words_gt' in config:\r\n",
        "        summary.append(doc[i])\r\n",
        "        if num_words >= max_words:\r\n",
        "          return summary"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \r\n",
        "ROUGE metric:\r\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \r\n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \r\n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\r\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\r\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \r\n",
        "  - Did we get all the words in the actual summary?\r\n",
        "- F1 = harmonic mean\r\n",
        "- N-Gram vs. LCS. Do we care about order? Don't need it to measure fluency/proper syntax. But ordering of words can indicate phrases \r\n",
        "\r\n",
        "Cons: \r\n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\r\n",
        "- Doesn't consider meaning -- same words could have different meaning   \r\n",
        "  \r\n",
        "Also considered BLEU, but only gives precision.     \r\n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(predicted_summary, actual_summary):\r\n",
        "  # TODO: unigram, bigram etc. models for rouge? - do we care about the order of the words?\r\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\r\n",
        "  rouge = scorer.score(''.join(predicted_summary), ''.join(actual_summary))\r\n",
        "\r\n",
        "  return rouge"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izB1kImY9oEK"
      },
      "source": [
        "# TODO create matrix of configurations to iterate through\r\n",
        "  # report config with best precision, best recall, best fmeasure \r\n",
        "  # loop over all documents and average results "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2TBmrUoHLcZ"
      },
      "source": [
        "# TODO evaluation strategy. average of rouge1, rouge2, rouge3 (like bleu with weights?). then fmeasure? preicision and recall equally important? \r\n",
        "  # do some algorithms/configurations do better in precision and others do better in recall? \r\n",
        "# TODO compare evaluations between models with paired bootstrap test to test significance? "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOruEfeCJ9H"
      },
      "source": [
        "configurations_bow = [['pagerank'],\r\n",
        "                      ['nostop', 'stopwords'],\r\n",
        "                      ['no_stemlemma', 'lemma', 'stem'],\r\n",
        "                      ['bow'],\r\n",
        "                      ['counts', 'binary'],\r\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\r\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\r\n",
        "                      ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                      ]\r\n",
        "configurations_embeddings = [['pagerank'],\r\n",
        "                             ['nostop', 'stopwords'],\r\n",
        "                             ['no_stemlemma', 'lemma', 'stem'],\r\n",
        "                             ['embedding'],\r\n",
        "                             ['glove'], # 'fasttext'\r\n",
        "                             ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                             ]\r\n",
        "configurations_baseline = [['baseline'],\r\n",
        "                           ['nostop', 'stopwords'],\r\n",
        "                           ['no_stemlemma', 'lemma', 'stem'],\r\n",
        "                           ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                           ]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf8oHe9DuZmq"
      },
      "source": [
        "embeddings = {}\r\n",
        "\r\n",
        "# load glove embeddings - code from https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\r\n",
        "# 100 length vector for each word \r\n",
        "glove_wv = {}\r\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\r\n",
        "for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    glove_wv[word] = coefs\r\n",
        "f.close()\r\n",
        "embeddings['glove'] = glove_wv\r\n",
        "\r\n",
        "# load fasttext embeddings \r\n",
        "#embeddings['fasttext'] = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/data/wiki.en.model\")\r\n",
        "# TODO memory issues! more work to limit memory? (can load in by itself - compress? https://gist.github.com/generall/68fddb87ae1845d6f54c958ed3d0addb)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf84dU5GDRJ7"
      },
      "source": [
        "df = text_cleaning(df)\r\n",
        "doc_processed = df.iloc[0].sentences_cleaned # version for modeling\r\n",
        "doc_display = df.iloc[0].sentences # version for display (original punctuation, capitalization etc.)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Y4nNBsaOjV"
      },
      "source": [
        "tfidf, feature_array = corpus_tfidf(df)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npK5_hh9bhfD"
      },
      "source": [
        "# TODO think about memory - run in batches and save config results in an append fashion?\r\n",
        "# number of configurations quickly balooning -- currently 468 configurations...\r\n",
        "  # maybe evaluate on smaller subsample and then pick top x configurations. Then train on full sample. "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SbvnFi8EB3r_",
        "outputId": "1f9acd66-b797-4cbc-defe-29bb5422c33d"
      },
      "source": [
        "config_results = {}\r\n",
        "config_list = list(itertools.product(*configurations_bow)) + list(itertools.product(*configurations_embeddings)) + list(itertools.product(*configurations_baseline))\r\n",
        "for config in config_list:\r\n",
        "  print(config)\r\n",
        "  local_results = {}\r\n",
        "\r\n",
        "  doc_processed_config = text_cleaning_config(doc_processed, config, stop_words)\r\n",
        "\r\n",
        "  if 'baseline' in config:\r\n",
        "    bestKeys = tfidf_sum(doc_processed_config , feature_array, tfidf)\r\n",
        "  elif 'pagerank' in config:\r\n",
        "    vec = vector_representation(doc_processed_config, config, embeddings)\r\n",
        "    bestKeys = pagerank(vec)\r\n",
        "\r\n",
        "  local_results['predicted_summary'] = extract_summary(bestKeys, doc_display, config) \r\n",
        "  local_results['actual_summary'] = df.iloc[0].sentences_summary\r\n",
        "  local_results['rouge'] = evaluate(local_results['predicted_summary'], local_results['actual_summary'])\r\n",
        "\r\n",
        "  config_results[str(config)] = local_results"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_words_gt')\n",
            "('baseline', 'nostop', 'no_stemlemma', 'num_sentences')\n",
            "('baseline', 'nostop', 'no_stemlemma', 'num_words_lt')\n",
            "('baseline', 'nostop', 'no_stemlemma', 'num_words_gt')\n",
            "('baseline', 'nostop', 'lemma', 'num_sentences')\n",
            "('baseline', 'nostop', 'lemma', 'num_words_lt')\n",
            "('baseline', 'nostop', 'lemma', 'num_words_gt')\n",
            "('baseline', 'nostop', 'stem', 'num_sentences')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-3ae057821977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'baseline'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbestKeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_processed_config\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeature_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;34m'pagerank'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_processed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-56502c049f3a>\u001b[0m in \u001b[0;36mtfidf_sum\u001b[0;34m(doc, feature_array, tfidf)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Normalize by length of sentence. Otherwise recommend longest sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msentence_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msentence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# sort keys in order of summed tfidf score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-56502c049f3a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Normalize by length of sentence. Otherwise recommend longest sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msentence_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msentence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# sort keys in order of summed tfidf score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-56502c049f3a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Normalize by length of sentence. Otherwise recommend longest sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msentence_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0msentence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# sort keys in order of summed tfidf score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'offici' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rZkQzcEGoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72e8bf76-bbc4-4163-d419-8338eb85d611"
      },
      "source": [
        "max_rouge1_fmeasure = 0\r\n",
        "best_config = ''\r\n",
        "for k,v in config_results.items():\r\n",
        "  fmeasure = v['rouge']['rouge1'].fmeasure\r\n",
        "  if fmeasure > max_rouge1_fmeasure:\r\n",
        "    max_rouge1_fmeasure = fmeasure\r\n",
        "    best_config = k\r\n",
        "best_config"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"('pagerank', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "YFeOlia-xoED",
        "outputId": "33d4e7a2-6464-445a-ae31-a61e06fee36b"
      },
      "source": [
        "config_results[\"('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\"]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-9a68c76738d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: \"('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "deqHkWtUGuYQ",
        "outputId": "c09687bd-0663-40f0-ba43-5f637a2dc9bb"
      },
      "source": [
        "config_results[\"('pagerank', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\"]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actual_summary': ['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"',\n",
              "  'Obama sends a letter to the heads of the House and Senate',\n",
              "  'Obama to seek congressional approval on military action against Syria',\n",
              "  'Aim is to determine whether CW were used, not by whom, says U.N. spokesman'],\n",
              " 'predicted_summary': ['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.',\n",
              "  'He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria, and that \"a military solution is not an option.\"'],\n",
              " 'rouge': {'rouge1': Score(precision=0.4423076923076923, recall=0.3382352941176471, fmeasure=0.3833333333333333),\n",
              "  'rouge2': Score(precision=0.21568627450980393, recall=0.16417910447761194, fmeasure=0.18644067796610167),\n",
              "  'rougeL': Score(precision=0.3269230769230769, recall=0.25, fmeasure=0.2833333333333334)}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}