{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_rank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+3sAk3OF8yCMa0jDBtEjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/text_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLDzqav1L3WA"
      },
      "source": [
        "# https://stackabuse.com/text-summarization-with-nltk-in-python/ -- TODO baseline method, just summing weights in sentences \r\n",
        "# tfidf where train on entire corpus. sum tfidf score of words in each sentence.\r\n",
        "  # sentences that are indicative of the specifics of the article - high frequency in the article, but specific to the article\r\n",
        "# or just straight term frequencies within the article. (or weighted like above so fractional of most frequent rather than diff. magnitudes)\r\n",
        "  # would need to drop stop words first"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDMy4RQPzHg7"
      },
      "source": [
        "# load english wiki word vectors: https://fasttext.cc/docs/en/pretrained-vectors.html\r\n",
        "# load wiki fasttext bin and save only model object: smaller \r\n",
        "#ft = FastText.load_fasttext_format(\"/content/drive/MyDrive/data/wiki.en.bin\")\r\n",
        "#ft.wv.save('/content/drive/MyDrive/data/wiki.en.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da773834-b079-44fc-b100-2614381ce7d0"
      },
      "source": [
        "!pip install rouge-score\r\n",
        "!pip install fasttext\r\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "#!unzip glove*.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.10.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3094307 sha256=03c8ec347b8277323907b0d3115ade1341f6239e8eedc4c8331bcb08888d216f\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99-qm9GrHPX"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import itertools\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "from collections import Counter, OrderedDict\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "from rouge_score import rouge_scorer\r\n",
        "import fasttext\r\n",
        "import gensim\r\n",
        "from gensim.models import FastText\r\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kSai_eCrMBW"
      },
      "source": [
        "#from google.colab import drive\r\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_1p-6myxP_V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "b9d0d823-7f65-473d-ca85-b978a3bb9a10"
      },
      "source": [
        "# load data\r\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0b7babd20d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data/cleaned_df.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/cleaned_df.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\r\n",
        "# TODO: stemming or lemmatization? \r\n",
        "# TODO: stop word exclusion? \r\n",
        "def text_cleaning(doc):\r\n",
        "  # downcase everything\r\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\r\n",
        "  # remove punctuation \r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \r\n",
        "Default: unigram bag of words with counts\r\n",
        "Options: \r\n",
        "1. Bow\r\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\r\n",
        "  - tf: term frequency normalization \r\n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\r\n",
        "  - idf: inverse document normalization \r\n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\r\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words\r\n",
        "2. Embeddings (pre-trained)\r\n",
        "  - GloVe\r\n",
        "  - Fasttext\r\n",
        "    - Advantage: generate embeddings for out of vocabulary words based on their parts\r\n",
        "    - But memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \r\n",
        "def vector_representation(doc, configuration, embeddings):\r\n",
        "\r\n",
        "  # list of words in each sentence \r\n",
        "  words = [sentence.split() for sentence in doc]\r\n",
        "\r\n",
        "  if 'bow' in configuration:\r\n",
        "\r\n",
        "    # include bigrams and/or trigrams (in addition to unigrams) in bow \r\n",
        "    grams = []\r\n",
        "    if 'bigram' in configuration or 'all' in configuration:\r\n",
        "      bigrams = [list(nltk.bigrams(sentence)) for sentence in words]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] for words in sentence] for sentence in bigrams]) # combine tuples of words into string\r\n",
        "    if 'trigram' in configuration or 'all' in configuration:\r\n",
        "      trigrams = [list(nltk.trigrams(sentence)) for sentence in words]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] + ' ' + words[2] for words in sentence] for sentence in trigrams]) # combine tuples of words into string\r\n",
        "    # concat with unigrams per sentence\r\n",
        "    for i in range(len(grams)):\r\n",
        "      words = [grams[i][j] + words[j] for j in range(len(words))] \r\n",
        "\r\n",
        "    # bag of words with binary indicators for words/n-grams rather than counts\r\n",
        "    if 'binary' in configuration: \r\n",
        "      words = [set(sentence) for sentence in words]\r\n",
        "\r\n",
        "    # bag of words: # sentences x # unique words\r\n",
        "    vec = DictVectorizer()\r\n",
        "    bow = vec.fit_transform(Counter(f) for f in words)\r\n",
        "\r\n",
        "    # term frequency normalization\r\n",
        "    if 'tf' in configuration: \r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = False)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "    # term frequency-inverse document frequency normalization\r\n",
        "    if 'tfidf' in configuration:\r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = True)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "\r\n",
        "    return bow\r\n",
        "\r\n",
        "  # possible extension: continued training on specific corpus. Probably unnecessary since wikipedia and news article words should be similar\r\n",
        "  if 'embedding' in configuration:\r\n",
        "\r\n",
        "    if 'glove' in configuration:\r\n",
        "      word_embeddings = embeddings['glove']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      # if unknown word, give embedding = 0 \r\n",
        "      sentence_vectors = []\r\n",
        "      for sentence in doc_processed:\r\n",
        "        sentence_vectors.append(sum([word_embeddings.get(word, np.zeros(100,)) for word in sentence.split()])/(len(sentence.split())))\r\n",
        "\r\n",
        "      return np.array(sentence_vectors)\r\n",
        "\r\n",
        "    # fasttext.\r\n",
        "    if 'fasttext' in configuration:\r\n",
        "      word_embeddings = embeddings['fasttext']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      sentence_vectors = []\r\n",
        "      for sentence in doc_processed:\r\n",
        "        sentence_vectors.append(sum([word_embeddings[word] for word in sentence.split()])/(len(sentence.split())))\r\n",
        "\r\n",
        "      return np.array(sentence_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "# TODO: other similarity metrics?\r\n",
        "# TODO: other algorithms\r\n",
        "def pagerank(bow):\r\n",
        "  # similarity matrix between sentences\r\n",
        "  sim =  cosine_similarity(bow)\r\n",
        "  # graph where node = sentence, edge weight = simialarity score\r\n",
        "  G = nx.from_numpy_array(sim)\r\n",
        "  # page rank\r\n",
        "  pr = nx.pagerank(G)\r\n",
        "\r\n",
        "  return pr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cJLGcOJuUo"
      },
      "source": [
        "### Extract Summary\r\n",
        "Grab best sentences based on ranking mechanism     \r\n",
        "Length of summary (Number of sentences)?\r\n",
        "- Number of sentences: generate 1 summary sentence per text sentence (average)\r\n",
        "  - Problem: text sentences are much longer than summary sentences, and since we are producing text sentences as our predicted summary, predicted summary is much longer than label summary\r\n",
        "- Number of words: generate 20 summary words per 1 text word\r\n",
        "  - Strict version: words in summary must be less than the threshold\r\n",
        "  - Less strict version: can go over limit by 1 sentence if reach threshold within the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(pr, doc, config):\r\n",
        "  # sort keys in order of page rank\r\n",
        "  bestkeys = sorted(pr, key=pr.get, reverse=True)\r\n",
        "  # summary based on number of sentences \r\n",
        "  if 'num_sentences' in config:\r\n",
        "    max_sentences = int(np.floor(len(doc) / 6)) # average 6 summary sentences per doc sentence\r\n",
        "    return [doc[i] for i in  bestkeys[0:max_sentences]]\r\n",
        "  # summary based on number of words\r\n",
        "  if 'num_words_gt' in config or 'num_words_lt' in config:\r\n",
        "    summary = []\r\n",
        "    num_words = 0\r\n",
        "    max_words = np.floor(len(''.join(doc).split(' ')) / 20) # average 20 summary words per text word\r\n",
        "    for i in bestkeys:\r\n",
        "      num_words += len(doc[i].split(' ')) \r\n",
        "      # strict version: words in summary must be less than threshold\r\n",
        "      if 'num_words_lt' in config:\r\n",
        "        if num_words >= max_words:\r\n",
        "          return summary\r\n",
        "        summary.append(doc[i])\r\n",
        "      # less strict version: can go over limit by 1 sentence \r\n",
        "      elif 'num_words_gt' in config:\r\n",
        "        summary.append(doc[i])\r\n",
        "        if num_words >= max_words:\r\n",
        "          return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \r\n",
        "ROUGE metric:\r\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \r\n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \r\n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\r\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\r\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \r\n",
        "  - Did we get all the words in the actual summary?\r\n",
        "- F1 = harmonic mean\r\n",
        "- N-Gram vs. LCS. Do we care about order? Don't need it to measure fluency/proper syntax. But ordering of words can indicate phrases \r\n",
        "\r\n",
        "Cons: \r\n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\r\n",
        "- Doesn't consider meaning -- same words could have different meaning   \r\n",
        "  \r\n",
        "Also considered BLEU, but only gives precision.     \r\n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(predicted_summary, actual_summary):\r\n",
        "  # TODO: unigram, bigram etc. models for rouge? - do we care about the order of the words?\r\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\r\n",
        "  rouge = scorer.score(''.join(predicted_summary), ''.join(actual_summary))\r\n",
        "\r\n",
        "  return rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izB1kImY9oEK"
      },
      "source": [
        "# TODO create matrix of configurations to iterate through\r\n",
        "  # report config with best precision, best recall, best fmeasure \r\n",
        "  # loop over all documents and average results "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2TBmrUoHLcZ"
      },
      "source": [
        "# TODO evaluation strategy. average of rouge1, rouge2, rouge3 (like bleu with weights?). then fmeasure? preicision and recall equally important? \r\n",
        "  # do some algorithms/configurations do better in precision and others do better in recall? \r\n",
        "# TODO compare evaluations between models with paired bootstrap test to test significance? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOruEfeCJ9H"
      },
      "source": [
        "configurations_bow = [['bow'],\r\n",
        "                      ['counts', 'binary'],\r\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\r\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\r\n",
        "                      ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                      ]\r\n",
        "configurations_embeddings = [['embedding'],\r\n",
        "                             ['glove', 'fasttext'],\r\n",
        "                             ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                             ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf8oHe9DuZmq"
      },
      "source": [
        "embeddings = {}\r\n",
        "\r\n",
        "# load glove embeddings - code from https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\r\n",
        "# 100 length vector for each word \r\n",
        "glove_wv = {}\r\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\r\n",
        "for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    glove_wv[word] = coefs\r\n",
        "f.close()\r\n",
        "embeddings['glove'] = glove_wv\r\n",
        "\r\n",
        "# load fasttext embeddings \r\n",
        "embeddings['fasttext'] = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/data/wiki.en.model\")\r\n",
        "# TODO memory issues! more work to limit memory? (can load in by itself - compress? https://gist.github.com/generall/68fddb87ae1845d6f54c958ed3d0addb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf84dU5GDRJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "fb5e56ec-0c88-4681-8f5d-7ccf8693d9e9"
      },
      "source": [
        "df = text_cleaning(df)\r\n",
        "doc_processed = df.iloc[0].sentences_cleaned # version for modeling\r\n",
        "doc_display = df.iloc[0].sentences # version for display (original punctuation, capitalization etc.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-29115211552f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_cleaned\u001b[0m \u001b[0;31m# version for modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;31m# version for display (original punctuation, capitalization etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbvnFi8EB3r_",
        "outputId": "3373429b-fd91-4d2d-ce32-2c401e9c044f"
      },
      "source": [
        "config_results = {}\r\n",
        "config_list = list(itertools.product(*configurations_bow)) + list(itertools.product(*configurations_embeddings))\r\n",
        "for config in config_list:\r\n",
        "  \r\n",
        "  local_results = {}\r\n",
        "\r\n",
        "  vec = vector_representation(doc_processed, config, embeddings)\r\n",
        "  pr = pagerank(vec)  \r\n",
        "  local_results['predicted_summary'] = extract_summary(pr, doc_display, config) \r\n",
        "  local_results['actual_summary'] = df.iloc[0].sentences_summary\r\n",
        "  local_results['rouge'] = evaluate(local_results['predicted_summary'], local_results['actual_summary'])\r\n",
        "\r\n",
        "  config_results[str(config)] = local_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('embedding', 'glove', 'num_sentences')\n",
            "('embedding', 'glove', 'num_words_lt')\n",
            "('embedding', 'glove', 'num_words_gt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rZkQzcEGoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92941217-e556-409b-fe56-196a7082beca"
      },
      "source": [
        "max_rouge1_fmeasure = 0\r\n",
        "best_config = ''\r\n",
        "for k,v in config_results.items():\r\n",
        "  fmeasure = v['rouge']['rouge1'].fmeasure\r\n",
        "  if fmeasure > max_rouge1_fmeasure:\r\n",
        "    max_rouge1_fmeasure = fmeasure\r\n",
        "    best_config = k\r\n",
        "best_config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"('bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiTHd0c2hhPr",
        "outputId": "fc4b510b-1320-4c34-d498-0c83b9b6eed0"
      },
      "source": [
        "config_results[\"('embedding', 'glove')\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actual_summary': ['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"',\n",
              "  'Obama sends a letter to the heads of the House and Senate',\n",
              "  'Obama to seek congressional approval on military action against Syria',\n",
              "  'Aim is to determine whether CW were used, not by whom, says U.N. spokesman'],\n",
              " 'predicted_summary': ['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.',\n",
              "  \"Bergen:  Syria is a problem from hell for the U.S.  Obama: 'This menace must be confronted'  Obama's senior advisers have debated the next steps to take, and the president's comments Saturday came amid mounting political pressure over the situation in Syria.\",\n",
              "  'Why Russia, China, Iran stand by Assad  Syria\\'s government unfazed  After Obama\\'s speech, a military and political analyst on Syrian state TV said Obama is \"embarrassed\" that Russia opposes military action against Syria, is \"crying for help\" for someone to come to his rescue and is facing two defeats -- on the political and military levels.'],\n",
              " 'rouge': {'rouge1': Score(precision=0.5, recall=0.18571428571428572, fmeasure=0.2708333333333333),\n",
              "  'rouge2': Score(precision=0.19607843137254902, recall=0.07194244604316546, fmeasure=0.10526315789473684),\n",
              "  'rougeL': Score(precision=0.3269230769230769, recall=0.12142857142857143, fmeasure=0.17708333333333331)}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deqHkWtUGuYQ",
        "outputId": "2435b53b-d19f-45ea-f84c-bb62add995d4"
      },
      "source": [
        "config_results[\"('bow', 'binary', 'no_normalization', 'bigram')\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actual_summary': ['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"',\n",
              "  'Obama sends a letter to the heads of the House and Senate',\n",
              "  'Obama to seek congressional approval on military action against Syria',\n",
              "  'Aim is to determine whether CW were used, not by whom, says U.N. spokesman'],\n",
              " 'predicted_summary': ['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.',\n",
              "  'He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria, and that \"a military solution is not an option.\"',\n",
              "  \"5 key assertions: U.S. intelligence report on Syria  Syria: Who wants what after chemical weapons horror  Reactions mixed to Obama's speech  A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama's announcement.\"],\n",
              " 'rouge': {'rouge1': Score(precision=0.5384615384615384, recall=0.25688073394495414, fmeasure=0.3478260869565218),\n",
              "  'rouge2': Score(precision=0.21568627450980393, recall=0.10185185185185185, fmeasure=0.13836477987421383),\n",
              "  'rougeL': Score(precision=0.36538461538461536, recall=0.1743119266055046, fmeasure=0.23602484472049692)}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}