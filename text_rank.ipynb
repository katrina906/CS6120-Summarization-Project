{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_rank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD2t79TTBZi7JH93fva/M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/text_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "330664b6-6bf6-441c-c273-bd1c0b8d4b67"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99-qm9GrHPX"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\r\n",
        "from collections import Counter, OrderedDict\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "from rouge_score import rouge_scorer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8kSai_eCrMBW",
        "outputId": "ebcd3fa5-f9e4-4673-e5be-66351ffde472"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_1p-6myxP_V"
      },
      "source": [
        "# load data\r\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\r\n",
        "# TODO: stemming or lemmatization? \r\n",
        "# TODO: stop word exclusion? \r\n",
        "def text_cleaning(doc):\r\n",
        "  # downcase everything\r\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\r\n",
        "  # remove punctuation \r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \r\n",
        "Default: unigram bag of words with counts\r\n",
        "Options: \r\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\r\n",
        "  - tf: term frequency normalization \r\n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\r\n",
        "  - idf: inverse document normalization \r\n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\r\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \r\n",
        "\r\n",
        "# TODO: try embeddings\r\n",
        "def vector_representation(doc, binary = False, tf = False, idf = False,\r\n",
        "                          include_bigrams = False, include_trigrams = False):\r\n",
        "\r\n",
        "  # list of words in each sentence \r\n",
        "  words = [sentence.split() for sentence in doc]\r\n",
        "\r\n",
        "  # include bigrams and/or trigrams (in addition to unigrams) in bow \r\n",
        "  grams = []\r\n",
        "  if include_bigrams:\r\n",
        "    bigrams = [list(nltk.bigrams(sentence)) for sentence in words]\r\n",
        "    grams.append([[words[0] + ' ' + words[1] for words in sentence] for sentence in bigrams]) # combine tuples of words into string\r\n",
        "  if include_trigrams:\r\n",
        "    trigrams = [list(nltk.trigrams(sentence)) for sentence in words]\r\n",
        "    grams.append([[words[0] + ' ' + words[1] + ' ' + words[2] for words in sentence] for sentence in trigrams]) # combine tuples of words into string\r\n",
        "  # concat with unigrams per sentence\r\n",
        "  for i in range(len(grams)):\r\n",
        "    words = [grams[i][j] + words[j] for j in range(len(words))] \r\n",
        "\r\n",
        "  # bag of words with binary indicators for words/n-grams rather than counts\r\n",
        "  if binary: \r\n",
        "    words = [set(sentence) for sentence in words]\r\n",
        "\r\n",
        "  # bag of words: # sentences x # unique words\r\n",
        "  vec = DictVectorizer()\r\n",
        "  bow = vec.fit_transform(Counter(f) for f in words)\r\n",
        "\r\n",
        "  # term frequency normalization\r\n",
        "  # also inverse frequency normalization depending on idf value\r\n",
        "  if tf: \r\n",
        "    tfidf_transformer = TfidfTransformer(use_idf = idf)\r\n",
        "    tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "    return tfidf\r\n",
        "\r\n",
        "  return bow"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "# TODO: other similarity metrics?\r\n",
        "# TODO: other algorithms\r\n",
        "def pagerank(bow):\r\n",
        "  # similarity matrix between sentences\r\n",
        "  sim =  cosine_similarity(bow)\r\n",
        "  # graph where node = sentence, edge weight = simialarity score\r\n",
        "  G = nx.from_numpy_array(sim)\r\n",
        "  # page rank\r\n",
        "  pr = nx.pagerank(G)\r\n",
        "\r\n",
        "  return pr"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(pr, doc, topn):\r\n",
        "  # sort keys in order\r\n",
        "  bestkeys = sorted(pr, key=pr.get, reverse=True)[0:topn]\r\n",
        "  return [doc[i] for i in bestkeys]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \r\n",
        "ROUGE metric:\r\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \r\n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \r\n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\r\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\r\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \r\n",
        "  - Did we get all the words in the actual summary?\r\n",
        "- F1 = harmonic mean\r\n",
        "- N-Gram vs. LCS. Do we care about order? Don't need it to measure fluency/proper syntax. But ordering of words can indicate phrases \r\n",
        "\r\n",
        "Cons: \r\n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\r\n",
        "- Doesn't consider meaning -- same words could have different meaning   \r\n",
        "  \r\n",
        "Also considered BLEU, but only gives precision.     \r\n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(predicted_summary, actual_summary):\r\n",
        "  # TODO: unigram, bigram etc. models for rouge? - do we care about the order of the words?\r\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\r\n",
        "  rouge = scorer.score(''.join(predicted_summary), ''.join(actual_summary))\r\n",
        "\r\n",
        "  return rouge"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izB1kImY9oEK"
      },
      "source": [
        "# TODO create matrix of configurations to iterate through\r\n",
        "  # report config with best precision, best recall, best fmeasure "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbvnFi8EB3r_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPiSm1yLyGye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c834b975-83a0-4eb7-edac-a324d3d82757"
      },
      "source": [
        "df = text_cleaning(df)\r\n",
        "doc_processed = df.iloc[0].sentences_cleaned # version for modeling\r\n",
        "doc_display = df.iloc[0].sentences # version for display (original punctuation, capitalization etc.)\r\n",
        "\r\n",
        "doc_representations = {}\r\n",
        "doc_representations['bow'] = vector_representation(doc_processed)\r\n",
        "doc_representations['bow_binary'] = vector_representation(doc_processed, binary = True)\r\n",
        "doc_representations['tf'] = vector_representation(doc_processed, tf = True)\r\n",
        "doc_representations['tfidf'] = vector_representation(doc_processed, tf = True, idf = True)\r\n",
        "\r\n",
        "for k,v in doc_representations.items():\r\n",
        "  print(k)\r\n",
        "  pr = pagerank(v)\r\n",
        "  predicted_summary = extract_summary(pr, doc_display, 3) # TODO: choose best number of sentences (iterate with validation; rule of thumb based on EDA)\r\n",
        "  actual_summary = df.iloc[0].sentences_summary\r\n",
        "  print(predicted_summary)\r\n",
        "  print(actual_summary)\r\n",
        "  rouge = evaluate(predicted_summary, actual_summary)\r\n",
        "  print(rouge)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bow\n",
            "['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.', '\"The main reason Obama is turning to the Congress:  the military operation did not get enough support either in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of the international-affairs committee of the Russian State Duma, said in a Twitter post.', 'The Organization for the Prohibition of Chemical Weapons, which nine of the inspectors belong to, said Saturday that it could take up to three weeks to analyze the evidence they collected.']\n",
            "['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"', 'Obama sends a letter to the heads of the House and Senate', 'Obama to seek congressional approval on military action against Syria', 'Aim is to determine whether CW were used, not by whom, says U.N. spokesman']\n",
            "{'rouge1': Score(precision=0.5, recall=0.21666666666666667, fmeasure=0.3023255813953488), 'rouge2': Score(precision=0.23529411764705882, recall=0.10084033613445378, fmeasure=0.1411764705882353), 'rougeL': Score(precision=0.3269230769230769, recall=0.14166666666666666, fmeasure=0.19767441860465118)}\n",
            "bow_binary\n",
            "['He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria, and that \"a military solution is not an option.\"', 'Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.', 'Why Russia, China, Iran stand by Assad  Syria\\'s government unfazed  After Obama\\'s speech, a military and political analyst on Syrian state TV said Obama is \"embarrassed\" that Russia opposes military action against Syria, is \"crying for help\" for someone to come to his rescue and is facing two defeats -- on the political and military levels.']\n",
            "['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"', 'Obama sends a letter to the heads of the House and Senate', 'Obama to seek congressional approval on military action against Syria', 'Aim is to determine whether CW were used, not by whom, says U.N. spokesman']\n",
            "{'rouge1': Score(precision=0.5, recall=0.208, fmeasure=0.2937853107344633), 'rouge2': Score(precision=0.21568627450980393, recall=0.08870967741935484, fmeasure=0.1257142857142857), 'rougeL': Score(precision=0.36538461538461536, recall=0.152, fmeasure=0.21468926553672318)}\n",
            "tf\n",
            "['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.', '\"The main reason Obama is turning to the Congress:  the military operation did not get enough support either in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of the international-affairs committee of the Russian State Duma, said in a Twitter post.', 'The Organization for the Prohibition of Chemical Weapons, which nine of the inspectors belong to, said Saturday that it could take up to three weeks to analyze the evidence they collected.']\n",
            "['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"', 'Obama sends a letter to the heads of the House and Senate', 'Obama to seek congressional approval on military action against Syria', 'Aim is to determine whether CW were used, not by whom, says U.N. spokesman']\n",
            "{'rouge1': Score(precision=0.5, recall=0.21666666666666667, fmeasure=0.3023255813953488), 'rouge2': Score(precision=0.23529411764705882, recall=0.10084033613445378, fmeasure=0.1411764705882353), 'rougeL': Score(precision=0.3269230769230769, recall=0.14166666666666666, fmeasure=0.19767441860465118)}\n",
            "tfidf\n",
            "['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.', '\"The main reason Obama is turning to the Congress:  the military operation did not get enough support either in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of the international-affairs committee of the Russian State Duma, said in a Twitter post.', 'The Organization for the Prohibition of Chemical Weapons, which nine of the inspectors belong to, said Saturday that it could take up to three weeks to analyze the evidence they collected.']\n",
            "['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"', 'Obama sends a letter to the heads of the House and Senate', 'Obama to seek congressional approval on military action against Syria', 'Aim is to determine whether CW were used, not by whom, says U.N. spokesman']\n",
            "{'rouge1': Score(precision=0.5, recall=0.21666666666666667, fmeasure=0.3023255813953488), 'rouge2': Score(precision=0.23529411764705882, recall=0.10084033613445378, fmeasure=0.1411764705882353), 'rougeL': Score(precision=0.3269230769230769, recall=0.14166666666666666, fmeasure=0.19767441860465118)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgB4WvK0zc71"
      },
      "source": [
        "# TODO evaluation strategy. average of rouge1, rouge2, rouge3 (like bleu with weights?). then fmeasure? preicision and recall equally important? \r\n",
        "  # do some algorithms/configurations do better in precision and others do better in recall? \r\n",
        "# TODO compare evaluations between models/configurations with paired bootstrap test? "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}