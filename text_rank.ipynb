{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_rank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbTzK2ab4xaY61/adoYCCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/text_rank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLDzqav1L3WA"
      },
      "source": [
        "# https://stackabuse.com/text-summarization-with-nltk-in-python/ -- baseline method, just summing weights in sentences "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbf193b-b761-418c-f9b7-d9173982e0c7"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.10.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99-qm9GrHPX"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import itertools\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "from collections import Counter, OrderedDict\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "from rouge_score import rouge_scorer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kSai_eCrMBW",
        "outputId": "400b1834-6c9c-4a1b-9e27-1108c4fa7eaf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_1p-6myxP_V"
      },
      "source": [
        "# load data\r\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\r\n",
        "# TODO: stemming or lemmatization? \r\n",
        "# TODO: stop word exclusion? \r\n",
        "def text_cleaning(doc):\r\n",
        "  # downcase everything\r\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\r\n",
        "  # remove punctuation \r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \r\n",
        "Default: unigram bag of words with counts\r\n",
        "Options: \r\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\r\n",
        "  - tf: term frequency normalization \r\n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\r\n",
        "  - idf: inverse document normalization \r\n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\r\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rATG9pYPMN7p",
        "outputId": "d5b1d251-44cc-4c05-d645-8122c154eea5"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 14:56:56--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-03-07 14:56:56--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-03-07 14:56:56--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 40s  \n",
            "\n",
            "2021-03-07 14:59:37 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \r\n",
        "# TODO: try embeddings\r\n",
        "def vector_representation(doc, configuration):\r\n",
        "\r\n",
        "  # list of words in each sentence \r\n",
        "  words = [sentence.split() for sentence in doc]\r\n",
        "\r\n",
        "  if 'bow' in configuration:\r\n",
        "\r\n",
        "    # include bigrams and/or trigrams (in addition to unigrams) in bow \r\n",
        "    grams = []\r\n",
        "    if 'bigram' in configuration or 'all' in configuration:\r\n",
        "      bigrams = [list(nltk.bigrams(sentence)) for sentence in words]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] for words in sentence] for sentence in bigrams]) # combine tuples of words into string\r\n",
        "    if 'trigram' in configuration or 'all' in configuration:\r\n",
        "      trigrams = [list(nltk.trigrams(sentence)) for sentence in words]\r\n",
        "      grams.append([[words[0] + ' ' + words[1] + ' ' + words[2] for words in sentence] for sentence in trigrams]) # combine tuples of words into string\r\n",
        "    # concat with unigrams per sentence\r\n",
        "    for i in range(len(grams)):\r\n",
        "      words = [grams[i][j] + words[j] for j in range(len(words))] \r\n",
        "\r\n",
        "    # bag of words with binary indicators for words/n-grams rather than counts\r\n",
        "    if 'binary' in configuration: \r\n",
        "      words = [set(sentence) for sentence in words]\r\n",
        "\r\n",
        "    # bag of words: # sentences x # unique words\r\n",
        "    vec = DictVectorizer()\r\n",
        "    bow = vec.fit_transform(Counter(f) for f in words)\r\n",
        "\r\n",
        "    # term frequency normalization\r\n",
        "    if 'tf' in configuration: \r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = False)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "    # term frequency-inverse document frequency normalization\r\n",
        "    if 'tfidf' in configuration:\r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = True)\r\n",
        "      tfidf = tfidf_transformer.fit_transform(bow)\r\n",
        "      return tfidf\r\n",
        "\r\n",
        "    return bow\r\n",
        "\r\n",
        "  if 'embedding' in configuration:\r\n",
        "\r\n",
        "    if 'glove' in configuration:\r\n",
        "      # Extract word vectors as dictionary - code from https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\r\n",
        "      # 100 length vector for each word \r\n",
        "      word_embeddings = {}\r\n",
        "      f = open('glove.6B.100d.txt', encoding='utf-8')\r\n",
        "      for line in f:\r\n",
        "          values = line.split()\r\n",
        "          word = values[0]\r\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "          word_embeddings[word] = coefs\r\n",
        "      f.close()\r\n",
        "\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      sentence_vectors = []\r\n",
        "      for sentence in doc_processed:\r\n",
        "        sentence_vectors.append(sum([word_embeddings.get(word, np.zeros(100,)) for word in sentence.split()])/(len(sentence.split())))\r\n",
        "\r\n",
        "      return np.array(sentence_vectors)\r\n",
        "      \r\n",
        "    \r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "# TODO: other similarity metrics?\r\n",
        "# TODO: other algorithms\r\n",
        "def pagerank(bow):\r\n",
        "  # similarity matrix between sentences\r\n",
        "  sim =  cosine_similarity(bow)\r\n",
        "  # graph where node = sentence, edge weight = simialarity score\r\n",
        "  G = nx.from_numpy_array(sim)\r\n",
        "  # page rank\r\n",
        "  pr = nx.pagerank(G)\r\n",
        "\r\n",
        "  return pr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(pr, doc, topn):\r\n",
        "  # sort keys in order\r\n",
        "  bestkeys = sorted(pr, key=pr.get, reverse=True)[0:topn]\r\n",
        "  return [doc[i] for i in bestkeys]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \r\n",
        "ROUGE metric:\r\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \r\n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \r\n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\r\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\r\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \r\n",
        "  - Did we get all the words in the actual summary?\r\n",
        "- F1 = harmonic mean\r\n",
        "- N-Gram vs. LCS. Do we care about order? Don't need it to measure fluency/proper syntax. But ordering of words can indicate phrases \r\n",
        "\r\n",
        "Cons: \r\n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\r\n",
        "- Doesn't consider meaning -- same words could have different meaning   \r\n",
        "  \r\n",
        "Also considered BLEU, but only gives precision.     \r\n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(predicted_summary, actual_summary):\r\n",
        "  # TODO: unigram, bigram etc. models for rouge? - do we care about the order of the words?\r\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\r\n",
        "  rouge = scorer.score(''.join(predicted_summary), ''.join(actual_summary))\r\n",
        "\r\n",
        "  return rouge"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izB1kImY9oEK"
      },
      "source": [
        "# TODO create matrix of configurations to iterate through\r\n",
        "  # report config with best precision, best recall, best fmeasure \r\n",
        "  # loop over all documents and average results "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2TBmrUoHLcZ"
      },
      "source": [
        "# TODO evaluation strategy. average of rouge1, rouge2, rouge3 (like bleu with weights?). then fmeasure? preicision and recall equally important? \r\n",
        "  # do some algorithms/configurations do better in precision and others do better in recall? \r\n",
        "# TODO compare evaluations between models with paired bootstrap test to test significance? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOruEfeCJ9H"
      },
      "source": [
        "configurations_bow = [['bow'],\r\n",
        "                      ['counts', 'binary'],\r\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\r\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\r\n",
        "                      ]\r\n",
        "configurations_embeddings = [['embedding'],\r\n",
        "                             ['fasttext', 'glove']\r\n",
        "                             ]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf84dU5GDRJ7"
      },
      "source": [
        "df = text_cleaning(df)\r\n",
        "doc_processed = df.iloc[0].sentences_cleaned # version for modeling\r\n",
        "doc_display = df.iloc[0].sentences # version for display (original punctuation, capitalization etc.)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbvnFi8EB3r_",
        "outputId": "c5ad5799-3793-4d75-bf36-969411e66599"
      },
      "source": [
        "config_results = {}\r\n",
        "config_list = list(itertools.product(*configurations_bow)) + list(itertools.product(*configurations_embeddings))\r\n",
        "for config in config_list:\r\n",
        "  print(config)\r\n",
        "  local_results = {}\r\n",
        "\r\n",
        "  bow = vector_representation(doc_processed, config)\r\n",
        "  pr = pagerank(bow)\r\n",
        "  local_results['predicted_summary'] = extract_summary(pr, doc_display, 3) # TODO: choose best number of sentences (iterate with validation; rule of thumb based on EDA)\r\n",
        "  local_results['actual_summary'] = df.iloc[0].sentences_summary\r\n",
        "  local_results['rouge'] = evaluate(local_results['predicted_summary'], local_results['actual_summary'])\r\n",
        "\r\n",
        "  config_results[str(config)] = local_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('bow', 'None', 'unigram')\n",
            "('bow', 'None', 'bigram')\n",
            "('bow', 'None', 'trigram')\n",
            "('bow', 'None', 'all')\n",
            "('bow', 'tf', 'unigram')\n",
            "('bow', 'tf', 'bigram')\n",
            "('bow', 'tf', 'trigram')\n",
            "('bow', 'tf', 'all')\n",
            "('bow', 'tfidf', 'unigram')\n",
            "('bow', 'tfidf', 'bigram')\n",
            "('bow', 'tfidf', 'trigram')\n",
            "('bow', 'tfidf', 'all')\n",
            "('bow_binary', 'None', 'unigram')\n",
            "('bow_binary', 'None', 'bigram')\n",
            "('bow_binary', 'None', 'trigram')\n",
            "('bow_binary', 'None', 'all')\n",
            "('bow_binary', 'tf', 'unigram')\n",
            "('bow_binary', 'tf', 'bigram')\n",
            "('bow_binary', 'tf', 'trigram')\n",
            "('bow_binary', 'tf', 'all')\n",
            "('bow_binary', 'tfidf', 'unigram')\n",
            "('bow_binary', 'tfidf', 'bigram')\n",
            "('bow_binary', 'tfidf', 'trigram')\n",
            "('bow_binary', 'tfidf', 'all')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rZkQzcEGoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5266cabc-1d3c-49d3-b7f2-9b352898f648"
      },
      "source": [
        "max_rouge1_fmeasure = 0\r\n",
        "best_config = ''\r\n",
        "for k,v in config_results.items():\r\n",
        "  fmeasure = v['rouge']['rouge1'].fmeasure\r\n",
        "  if precision > max_rouge1_fmeasure:\r\n",
        "    max_rouge1_fmeasure = fmeasure\r\n",
        "    best_config = k\r\n",
        "best_config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"('bow_binary', 'tfidf', 'all')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deqHkWtUGuYQ",
        "outputId": "494e0edf-1364-4de8-89a0-ad54c2af4486"
      },
      "source": [
        "config_results[\"('bow_binary', 'tfidf', 'all')\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actual_summary': ['Syrian official: Obama climbed to the top of the tree, \"doesn\\'t know how to get down\"',\n",
              "  'Obama sends a letter to the heads of the House and Senate',\n",
              "  'Obama to seek congressional approval on military action against Syria',\n",
              "  'Aim is to determine whether CW were used, not by whom, says U.N. spokesman'],\n",
              " 'predicted_summary': ['Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons.',\n",
              "  'On Saturday, Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad.',\n",
              "  \"5 key assertions: U.S. intelligence report on Syria  Syria: Who wants what after chemical weapons horror  Reactions mixed to Obama's speech  A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama's announcement.\"],\n",
              " 'rouge': {'rouge1': Score(precision=0.4807692307692308, recall=0.25252525252525254, fmeasure=0.33112582781456956),\n",
              "  'rouge2': Score(precision=0.19607843137254902, recall=0.10204081632653061, fmeasure=0.13422818791946312),\n",
              "  'rougeL': Score(precision=0.34615384615384615, recall=0.18181818181818182, fmeasure=0.23841059602649012)}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}