{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstractive_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7yfsjO4lZkWopby57QHyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/abstractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAOZbr_ZKrFG",
        "outputId": "45a16e2f-04b1-44ef-ab8d-87400fc243de"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install import-ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8bU5xLtH6zu"
      },
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import torch\n",
        "import import_ipynb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI-pdbylWSEy"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ra--fQLrI0",
        "outputId": "8b007a23-f047-4ad6-abab-a755e4254f27"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bubbIbqiVTTJ",
        "outputId": "7ead0105-c7d4-46fb-8044-3d9ac948921d"
      },
      "source": [
        "# load in functions from extract_summarization notebook\n",
        "%cd \"drive/MyDrive/Colab Notebooks\"\n",
        "from attention import *\n",
        "%cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "importing Jupyter notebook from attention.ipynb\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTjrh_-RLggE"
      },
      "source": [
        "# load data\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")\n",
        "df = df.sample(frac = 1, random_state = 123).head(10000) # so comparable to extractive \n",
        "#article = df.iloc[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQJSGv_MXCT7"
      },
      "source": [
        "# TODO split into train, validate, and test. supervised learning. validate for training, test for statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_qH9aKbACj"
      },
      "source": [
        "df[['sentences', 'sentences_summary']] # TODO use cleaned text\n",
        "df['text_seq'] = df.sentences.map(lambda row: ' '.join(row))\n",
        "df['summary_seq'] = df.sentences_summary.map(lambda row: '_START_ ' + ' '.join(row) + ' _END_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzFnKA7Va-db"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "text_tr, text_test, summary_tr, summary_test = train_test_split(df['text_seq'].to_list(), df['summary_seq'].to_list(), test_size=0.1, random_state=0, shuffle=True) \n",
        "text_tr, text_val, summary_tr, summary_val = train_test_split(text_tr, summary_tr, test_size=0.1, random_state=0, shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tYX5bg3dap8"
      },
      "source": [
        "max_len_text = 50 # TODO set max len to actual max len "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E5WbM76XqB5"
      },
      "source": [
        "def generate_token_sequences(text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(list(text))\n",
        "  seq = tokenizer.texts_to_sequences(list(text))\n",
        "  seq = pad_sequences(seq,  maxlen=max_len_text, padding='post') \n",
        "  voc_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return seq, voc_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipuA5O7HX70y"
      },
      "source": [
        "text_tr, text_tr_voc_size = generate_token_sequences(text_tr)\n",
        "summary_tr, summary_tr_voc_size = generate_token_sequences(summary_tr)\n",
        "text_val, text_val_voc_size = generate_token_sequences(text_val)\n",
        "summary_val, summary_val_voc_size = generate_token_sequences(summary_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bJtqQ9dWhFZ"
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "latent_dim = 500 # TODO tune?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26NM1tDLXakK"
      },
      "source": [
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_text,)) \n",
        "enc_emb = Embedding(text_tr_voc_size, latent_dim,trainable=True)(encoder_inputs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLgBKFbyXbzr"
      },
      "source": [
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# TODO different numbers of layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHSIE2eaYvx5"
      },
      "source": [
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(summary_tr_voc_size, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0idt9memYiDK"
      },
      "source": [
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(summary_tr_voc_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1vCd0clYz5p"
      },
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLupllHPY97y"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy') # TODO try diff optimizers, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rzad3MAY-vp"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1) # TODO look at documentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4UepnhWZDIB"
      },
      "source": [
        "history=model.fit([text_tr,summary_tr[:,:-1]],\n",
        "                  summary_tr.reshape(summary_tr.shape[0],summary_tr.shape[1], 1)[:,1:],\n",
        "                  epochs=1, # 50\n",
        "                  callbacks=[es],\n",
        "                  batch_size=512, # 512\n",
        "                  validation_data=([text_val,summary_val[:,:-1]], summary_val.reshape(summary_val.shape[0],summary_val.shape[1], 1)[:,1:]))\n",
        "# https://keras.io/api/models/model_training_apis/#fit-method\n",
        "# why is it using x and y as the x data?\n",
        "# shifitng summary data in both cases?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbjEQsSbL78Y"
      },
      "source": [
        "#summarizer = pipeline(\"summarization\", framework = 'pt')\n",
        "#summarized = summarizer(article, min_length=0, max_length=100) # max length is number of words/tokens\n",
        "# max number of tokens can consider: 1024. Need to limit to first 1024\n",
        "  # ok because best info tends to be at front of articles (and bcg documents bc pyramid principle)\n",
        "  # potential extension is to fine tune with a higher max allowed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFdhMj2eU_xi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}