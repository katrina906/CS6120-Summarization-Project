{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstractive_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPfERyheHbggy/ElAJgRio",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/abstractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBTr49GUVZJ3"
      },
      "source": [
        "#https://www.thepythoncode.com/article/text-summarization-using-huggingface-transformers-python\n",
        "#https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "# use much smaller sample size. and then re-evaluate extractive models to match so have comparative stats? or just _best_ extractive model!\n",
        "  # discuss that results are less robust and list as extension to parallelize. "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOk3UcsdMtK"
      },
      "source": [
        "# Abstractive Summarization using Encoder-Decoder T5 Model \n",
        "TODO: describe model. Using pre-trained model. \n",
        "              \n",
        "__Pros__ over extractive summarization:\n",
        "- Generates new text, not just repeating what is in the article. Makes summary more engaging to read and may combine ideas better to make the summaries more to the point. \n",
        "- Can use input text without having to make text cleaning decisions. Can consider more features like punctuation and capitalization \n",
        "\n",
        "__Cons__ over extractive summarization:\n",
        "- Harder to evaluate: can generate sentences that mean the same thing as the given summary sentences, but using different words, which will not be understood by the ROUGE metrics. \n",
        "- Decoding is more computationally intensive and we were unable to generate summaries for as large a sample as we did for extractive summarization. Thus our results are less robust. \n",
        "- Sometimes generate <UNK> character\n",
        "- Cannot guarentee it will generate full sentences. Cuts off mid-sentence if did not generate an end of sequence character before reaching the specified max length.\n",
        "- Encoding can only take the first 1,017 characters of the text. If topics appear later in the article for the first time, they will be completely missed and not included in the summary\n",
        "  - TODO: include stat on average number of characters in our articles and how many go over threshold\n",
        "  - In general, news articles tend to include highlights of the most important information in the first few sentences followed by details, so this should not impact performance as drastically as in other contexts.\n",
        "  - BCG material follows the pyramid principle where you summarize the key points first, so similar structure to news articles. \n",
        "\n",
        "__Potential extensions__:\n",
        "- Parallelize decoding to allow for better run time\n",
        "- Grid search through parameters such as length penalty, number of beams, number of ngrams to not repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAOZbr_ZKrFG"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install import-ipynb\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8bU5xLtH6zu"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "import import_ipynb\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ra--fQLrI0",
        "outputId": "90187782-6b5a-46d8-b3e6-45754e41a8c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bubbIbqiVTTJ",
        "outputId": "1b0f00bc-636f-41b5-ccdc-ef7656786fe5"
      },
      "source": [
        "# load in functions from extract_summarization notebook\n",
        "%cd \"drive/MyDrive/Colab Notebooks\"\n",
        "from extractive_summarization import *\n",
        "%cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "importing Jupyter notebook from extractive_summarization.ipynb\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-uk4KYmC6E9"
      },
      "source": [
        "def load_model():\n",
        "  # initialize the model architecture and weights\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "  # initialize the model tokenizer\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "  return model, tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJHsnI1bGve"
      },
      "source": [
        "## Encode text for inference\n",
        "- Encode words into numerical vectors using model's tokenizer\n",
        "- Will automatically convert unknown words into <unk> \n",
        "- Use un-processed original sentences because model takes into account features like capitalization and punctuation. Also, features like stopwords are important for generating grammatically correct sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmoxHQCbCux"
      },
      "source": [
        "def encode_input(df, tokenizer):\n",
        "  df['encoded'] = df.sentences.map(lambda row: tokenizer.encode(\"summarize: \" + ' '.join(row), return_tensors=\"pt\", max_length=1017, truncation=True))\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6sorPtQZHty"
      },
      "source": [
        "## Decoding Methods\n",
        "- Greedy: select word with highest probability given all prior context: P(w | w<sub>1:t-1</sub>)\n",
        "  - Con: misses high probability words that occur after a lower probability word because never explore the path \n",
        "- Beam Search: considers probability of sequences num_beams long. \n",
        "  - Con: higher computation time \n",
        "- Sampling methods (_not using_): used to introduce randomness to the text and make it sound more human-like, especially in contexts like story generation. However, in this case, we do not want randomness but rather want the summaries to closely follow the content in the article. \n",
        "  - Ex: article about a man attacked by a tiger says that he was conscious and talking in the ambulance. Sampling decoding creates a sentence that claims he was \"conscious and talking\" with the animal\n",
        "\n",
        "## Length of Predicted Summary\n",
        "- Max length:\n",
        "  - Cannot set based on number of sentences; number of words only\n",
        "  - Heuristic: average 20 text words per summary word\n",
        "  - Two configs:\n",
        "    - If strict, cannot go over heuristic. \n",
        "    - If more lenient, can go over by 1 unit of the heuristic (20 words)\n",
        "- Min length: want to generate summary right around the heuristic; do not want to generate a shorter summary because want enough information for content curator to use. Thus allow to go under by 1 unit of the heuristic if the model predicts an end of sequence token. \n",
        "\n",
        "## Other Parameters\n",
        "- No repeat ngram = 4: these methods tend to generate repetitive sequences of words. This parameters disallows ngrams to repeat if they are of length 4. \n",
        "  - Bigrams and trigrams can repeat so entity names that are central to the article can appear multiple times. But do not allow entire phrases to repeat. \n",
        "  - Ex: without parameter get sequences like \"the heat index will make it feel like 113. the heat index will make it feel like 113\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbZ4NqiHYXjn"
      },
      "source": [
        "def decode(df, model, tokenizer, config):\n",
        "\n",
        "  df['max_words'] = df.sentences.map(lambda row: int(np.floor(len(''.join(row).split(' ')) / 20))) # average 20 text words per summary word\n",
        "  if 'max_words_plus' in config:\n",
        "    df.max_words = df.max_words + 20\n",
        "\n",
        "  if 'greedy' in config:\n",
        "    df['outputs'] = df.apply(lambda row: model.generate( \n",
        "                                         row.encoded, \n",
        "                                         max_length=row.max_words, \n",
        "                                         min_length=max(0, row.max_words - 20),\n",
        "                                         no_repeat_ngram_size = 4), \n",
        "                             axis = 1) \n",
        "  if 'beam' in config:\n",
        "    df['outputs'] = df.apply(lambda row: model.generate( \n",
        "                                         row.encoded, \n",
        "                                         max_length=row.max_words, \n",
        "                                         min_length=max(0, row.max_words - 20),\n",
        "                                         num_beams = 5,\n",
        "                                         early_stopping = True),\n",
        "                             axis = 1) \n",
        "    \n",
        "  # decode predicted summary of numbers back into text\n",
        "  df['predicted_summary'] = df.outputs.map(lambda row: tokenizer.decode(row[0], skip_special_tokens = True))\n",
        "\n",
        "  return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgxa8oVqij1G"
      },
      "source": [
        "def train_config_loop_extractive(df, model, tokenizer, config_list, eval_only = True, continue_iterations = False,\n",
        "                                 filename = ''):\n",
        "\n",
        "  if continue_iterations:\n",
        "    with open('/content/drive/MyDrive/data/' + filename + '.pkl', 'rb') as f:\n",
        "      results_so_far = pickle.load(f) \n",
        "      eval_results = results_so_far[1]\n",
        "      model_results = results_so_far[2]\n",
        "      config_list = config_list[config_list.index(results_so_far[0])+1:] # continue from last config\n",
        "  else:\n",
        "    eval_results = {}\n",
        "    model_results = {}\n",
        "\n",
        "  for config in config_list:\n",
        "    print(config)\n",
        "    df = decode(df, model, tokenizer, config)\n",
        "    eval_dict = evaluate(df)\n",
        "    eval_results[(str(config))] = metrics_distribution(df)\n",
        "    if not eval_only:\n",
        "      model_results[str(config)] = df[['sentences', 'summary', 'rouge', 'predicted_summary']]\n",
        "\n",
        "    # save every config after finishes (only 4 and each is very long)\n",
        "    if filename != '':\n",
        "      with open('/content/drive/MyDrive/data/' + filename + '.pkl', 'wb') as f:\n",
        "          pickle.dump([config, eval_results, model_results], f)\n",
        "          print('saving!')\n",
        "\n",
        "  return eval_results, model_results"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unziysyxmWlI"
      },
      "source": [
        "CONFIGURATIONS = [['greedy', 'beam'],\n",
        "                  ['max_words_strict', 'max_words_plus'],\n",
        "                  ]    \n",
        "# cross products of all possible combinations of configurations\n",
        "model_configurations = list(itertools.product(*CONFIGURATIONS)) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NexaeTk_i28X"
      },
      "source": [
        "def main():\n",
        "\n",
        "  # load T5 model and tokenizer\n",
        "  model, tokenizer = load_model()\n",
        "\n",
        "  # load data and encode input\n",
        "  df = data_setup(n = 20) # TODO increase  \n",
        "  df = encode_input(df, tokenizer)\n",
        "\n",
        "  # train each configuration on a subset of the data and get evaluation metrics \n",
        "  eval_results, _ = train_config_loop_extractive(df.head(5), model, tokenizer, model_configurations, eval_only = True,\n",
        "                                                 filename = 'train_config_loop_abstractive', continue_iterations = False)\n",
        "  # find best config for each evaluation metric\n",
        "  best_configs = find_best_configs(eval_results)\n",
        "\n",
        "  # train full model on best configurations for each metric\n",
        "  eval_results_dict = {} # for each eval metric, distribution of evaluation metrics \n",
        "  model_results_dict = {} # for each eval metric, data with predicted summaries\n",
        "  seen_configs = {}  # keep track of which configs we have trained so far\n",
        "  seen_metrics = []\n",
        "  for metric in best_configs.keys():\n",
        "    config = tuple(best_configs[metric].strip('(').strip(')').replace(\"'\", \"\").split(', '))\n",
        "    if config not in seen_configs.keys():\n",
        "      eval_results, model_results = train_config_loop_extractive(df, model, tokenizer, [config], eval_only = False)\n",
        "      eval_results_dict[metric] = eval_results[str(config)][metric]\n",
        "      model_results_dict[metric] = model_results[str(config)]\n",
        "      seen_configs[config] = metric\n",
        "    # prevent duplicative retraining: use existing results if best config for prior metric\n",
        "    else:\n",
        "      eval_results_dict[metric] = eval_results_dict[seen_configs[config]]\n",
        "      model_results_dict[metric] = model_results_dict[seen_configs[config]]\n",
        "    seen_metrics.append(metric)\n",
        "    # save best models\n",
        "    # save every iteration overwriting\n",
        "    # if need to restart, load in dictionaries, go through best_configs.keys() but not in seen_metrics, continue adding to dictionaries\n",
        "    with open('/content/drive/MyDrive/data/trained_model_abstractive.pkl', 'wb') as f: \n",
        "        pickle.dump([seen_metrics, eval_results_dict, model_results_dict, best_configs], f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwmzLfxmjxV"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN2xFZIGXjFu",
        "outputId": "f3df44b7-00af-4315-e1f3-0fc2d14d2aea"
      },
      "source": [
        "(10000*90)/60/60 # 250 hours for 10,000  \n",
        "# 1 min 30 seconds for 10 inputs\n",
        "# more than douple for 20 inputs = 3 min 30 seconds\n",
        "# greedy - half the time "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}