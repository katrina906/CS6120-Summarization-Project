{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extractive_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTfhbIJC6U3O1W5xH33Ve8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/extractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDMy4RQPzHg7"
      },
      "source": [
        "# TODO: other extractive methods\r\n",
        "# LSA: https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis\r\n",
        "  # choose most importance sentence from most important concepts, then most important sentence from second import concept (and other methods in article)\r\n",
        "    # ensures summary is capturing different aspects of the article \r\n",
        "# score each sentence according to some generated features: https://github.com/xiaoxu193/PyTeaser/blob/master/pyteaser.py\r\n",
        "  # other ideas for features: https://medium.com/@umerfarooq_26378/text-summarization-in-python-76c0a41f0dc4\r\n",
        "  # not sure want to go down this rabbit hole - lots of tuning, feature creatione etc.. could just mention as an alternative method. or baseline compare the package version.\r\n",
        "# try Jaccard similarity instead of cosine for text rank\r\n",
        "\r\n",
        "# https://medium.com/@umerfarooq_26378/text-summarization-in-python-76c0a41f0dc4 - also includes abstractive methods\r\n",
        "# https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/ - BART auto-regressive for summarization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0"
      },
      "source": [
        "%%capture\r\n",
        "!pip install rouge-score\r\n",
        "!pip install fasttext\r\n",
        "!pip install compress-fasttext\r\n",
        "!pip install gensim==3.8.3\r\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "#!unzip glove*.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwm839obTgS"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "#!unzip glove*.zip\r\n",
        "# TODO: read into drive so don't have to wget every time? - in downloads. need to cmd unzip and upload to drive!\r\n",
        "  # uploaded as zip in drive. see how fast !unzip is or if need to unzip in drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K99-qm9GrHPX",
        "outputId": "943a2bc4-2211-4719-b7c3-e11a2f29a72d"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import string\r\n",
        "import re\r\n",
        "import sys\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import itertools\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "from collections import Counter, OrderedDict\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "from rouge_score import rouge_scorer\r\n",
        "import gensim\r\n",
        "import fasttext\r\n",
        "from gensim.models import FastText\r\n",
        "import compress_fasttext\r\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\r\n",
        "from nltk.corpus import stopwords  \r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = set(stopwords.words('english'))  \r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kSai_eCrMBW",
        "outputId": "630e8a08-f4d9-4691-eaf8-7760d6648f5c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\r\n",
        "# always do this function\r\n",
        "# note that some sentence tokenization is messy. Ex: if numbered list, list becomes own sentence.\r\n",
        "  # but shouldn't show up as important as summary sentence anyways\r\n",
        "def text_cleaning(df):\r\n",
        "  # downcase everything\r\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\r\n",
        "  # remove punctuation \r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\r\n",
        "  # drop extra spaces\r\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"\\s+\", ' ', sentence) for sentence in text])\r\n",
        "  # drop sentences that only have one word (usually because numbered list etc.)\r\n",
        "    # also drop from sentences: need indexes to match for final summary extraction \r\n",
        "  df['drop_sentences'] = df.sentences_cleaned.apply(lambda text: [i for i in range(len(text)) if len(text[i].split()) <= 1])\r\n",
        "  df.sentences_cleaned = df.apply(lambda row: [row.sentences_cleaned[i] for i in range(len(row.sentences_cleaned)) if i not in row.drop_sentences], axis = 1)\r\n",
        "  df.sentences = df.apply(lambda row: [row.sentences[i] for i in range(len(row.sentences)) if i not in row.drop_sentences], axis = 1)\r\n",
        "  \r\n",
        "  return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbTbtLtSs7yJ"
      },
      "source": [
        "# cleaning depending on configuration\r\n",
        "def text_cleaning_config(df_config, config, stop_words):\r\n",
        "  df_config['words'] = df_config.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\r\n",
        "  if 'stopwords' in config:\r\n",
        "    df_config.words = df_config.words.apply(lambda row: [[w for w in sentence if not w in stop_words] for sentence in row])\r\n",
        "  if 'stem' in config:\r\n",
        "    stemmer = PorterStemmer()\r\n",
        "    df_config.words = df_config.words.apply(lambda row: [[stemmer.stem(w) for w in sentence] for sentence in row])\r\n",
        "  if 'lemma' in config:\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    df_config.words = df_config.words.apply(lambda row: [[lemmatizer.lemmatize(w) for w in sentence] for sentence in row])\r\n",
        "\r\n",
        "  # recombine words into sentences\r\n",
        "  df_config.sentences_cleaned = df_config.words.apply(lambda row: [' '.join(sentence) for sentence in row])\r\n",
        "\r\n",
        "  # drop sentences that only have one word after these exclusions\r\n",
        "    # also drop from sentences: need indexes to match for final summary extraction \r\n",
        "  df_config['drop_sentences'] = df_config.sentences_cleaned.apply(lambda text: [i for i in range(len(text)) if len(text[i].split()) <= 1])\r\n",
        "  df_config.sentences_cleaned = df_config.apply(lambda row: [row.sentences_cleaned[i] for i in range(len(row.sentences_cleaned)) if i not in row.drop_sentences], axis = 1)\r\n",
        "  df_config.sentences = df_config.apply(lambda row: [row.sentences[i] for i in range(len(row.sentences)) if i not in row.drop_sentences], axis = 1)\r\n",
        "\r\n",
        "  return df_config"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHiiHeyCaEhW"
      },
      "source": [
        "### Train TFIDF in Corpus\r\n",
        "Used in baseline model to sum tfidf scores within each sentence in each document "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLrQnm0VYygp"
      },
      "source": [
        "def corpus_tfidf(df):\r\n",
        "  \r\n",
        "  # list of words in each article\r\n",
        "  corpus = df.sentences_cleaned.to_list()\r\n",
        "  corpus = [' '.join(article) for article in corpus]  \r\n",
        "  corpus = [article.split(' ') for article in corpus]\r\n",
        "\r\n",
        "  # tfidf trained on entire corpus: document = article\r\n",
        "  tfidf_vec = TfidfVectorizer(analyzer = 'word', \r\n",
        "                              tokenizer = lambda doc: doc, preprocessor = lambda doc: doc, token_pattern = None)\r\n",
        "                              # already did preprocessing, so using identity functions for tokenizer and preprocessor\r\n",
        "  tfidf = tfidf_vec.fit_transform(corpus) # sparse arrays of scores for each word in each article. articles x words\r\n",
        "  feature_array = list(tfidf_vec.get_feature_names())\r\n",
        "  \r\n",
        "  return tfidf, feature_array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \r\n",
        "Default: unigram bag of words with counts\r\n",
        "Options: \r\n",
        "1. Bow\r\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\r\n",
        "  - tf: term frequency normalization \r\n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\r\n",
        "  - idf: inverse document normalization \r\n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\r\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words\r\n",
        "2. Embeddings (pre-trained)\r\n",
        "  - GloVe\r\n",
        "  - Fasttext\r\n",
        "    - Advantage: generate embeddings for out of vocabulary words based on their parts\r\n",
        "    - But memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \r\n",
        "def vector_representation(df, configuration, embeddings):\r\n",
        "\r\n",
        "  # list of words in each sentence \r\n",
        "  df['words'] = df.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\r\n",
        "\r\n",
        "  if 'bow' in configuration:\r\n",
        "\r\n",
        "    # include bigrams and/or trigrams (in addition to unigrams) in bow \r\n",
        "    if 'bigram' in configuration or 'all' in configuration:\r\n",
        "      df['bigrams'] = df.words.apply(lambda row: [list(nltk.bigrams(sentence)) if len(sentence) >= 2 else '' for sentence in row])\r\n",
        "      df.bigrams = df.bigrams.apply(lambda row: [[words[0] + ' ' + words[1] for words in sentence] for sentence in row])\r\n",
        "      df.words = df.apply(lambda row: [row.bigrams[j] + row.words[j] for j in range(len(row.words))], axis = 1)\r\n",
        "    if 'trigram' in configuration or 'all' in configuration:\r\n",
        "      df['trigrams'] = df.words.apply(lambda row: [list(nltk.trigrams(sentence)) if len(sentence) >= 3 else '' for sentence in row])\r\n",
        "      df.trigrams = df.bigrams.apply(lambda row: [[words[0] + ' ' + words[1] + words[2] for words in sentence] for sentence in row])\r\n",
        "      df.words = df.apply(lambda row: [row.trigrams[j] + row.words[j] for j in range(len(row.words))], axis = 1)\r\n",
        "\r\n",
        "    # bag of words with binary indicators for words/n-grams rather than counts\r\n",
        "    if 'binary' in configuration: \r\n",
        "      df.words = df.words.apply(lambda row: [set(sentence) for sentence in row])\r\n",
        "\r\n",
        "    # bag of words: # sentences x # unique words\r\n",
        "    vec = DictVectorizer()\r\n",
        "    df['vector_rep'] = df.words.apply(lambda row: vec.fit_transform(Counter(f) for f in row))\r\n",
        "\r\n",
        "    # term frequency normalization\r\n",
        "    if 'tf' in configuration: \r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = False)\r\n",
        "      df['vector_rep'] = df.vector_rep.apply(lambda row: tfidf_transformer.fit_transform(row))\r\n",
        "    # term frequency-inverse document frequency normalization\r\n",
        "    if 'tfidf' in configuration:\r\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = True)\r\n",
        "      df['vector_rep'] = df.vector_rep.apply(lambda row: tfidf_transformer.fit_transform(row))  \r\n",
        "\r\n",
        "  # possible extension: continued training on specific corpus. Probably unnecessary since wikipedia and news article words should be similar\r\n",
        "  if 'embedding' in configuration:\r\n",
        "\r\n",
        "    if 'glove' in configuration:\r\n",
        "      word_embeddings = embeddings['glove']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      # if unknown word, give embedding = 0 \r\n",
        "      df['vector_rep'] = df.sentences_cleaned.apply(lambda row: [sum([word_embeddings.get(word, np.zeros(100,)) for word in sentence.split()]) / len(sentence.split()) for sentence in row])\r\n",
        "\r\n",
        "    # fasttext.\r\n",
        "    if 'fasttext' in configuration:\r\n",
        "      word_embeddings = embeddings['fasttext']\r\n",
        "      # find average of word embeddings for each sentence \r\n",
        "      df['vector_rep'] = df.sentences_cleaned.apply(lambda row: [sum([word_embeddings[word] for word in sentence.split()]) / len(sentence.split()) for sentence in row])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZyF8DHeaVrY"
      },
      "source": [
        "### PageRank Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "# TODO: other similarity metrics?\r\n",
        "# TODO: other algorithms\r\n",
        "def pagerank(df):\r\n",
        "\r\n",
        "  # similarity matrix between sentences\r\n",
        "  df['sim'] = df.vector_rep.apply(lambda row: cosine_similarity(row))\r\n",
        "  # graph where node = sentence, edge weight = simialarity score\r\n",
        "  df['graph'] = df.sim.apply(lambda row: nx.from_numpy_array(row))  \r\n",
        "  # page rank\r\n",
        "  df['pr'] = df.graph.apply(lambda row: nx.pagerank_numpy(row))\r\n",
        "  # sort keys in order of page rank\r\n",
        "  df['bestkeys'] = df.pr.apply(lambda row: sorted(row, key = row.get, reverse = True))\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc6rDMo6aXL9"
      },
      "source": [
        "### Baseline Model\r\n",
        "- Train TF-IDF on entire corpus where document = article. Get a score for each word in each document\r\n",
        "- Sum scores for all words in each sentence \r\n",
        "- Produce sentences with highest total TF-IDF score \r\n",
        "\r\n",
        "Idea: Sentences that are indicative of the specifics of the article. High frequency in the article, but specific to the article\r\n",
        "\r\n",
        "Could also try straight term frequencies within the article. (or weighted like above so fractional of most frequent rather than diff. magnitudes). Would need to drop stop words first (https://stackabuse.com/text-summarization-with-nltk-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_UiVtJZGnZ"
      },
      "source": [
        "def tfidf_sum(df, feature_array, tfidf):\r\n",
        "\r\n",
        "  # sum tfidf score within each sentence. \r\n",
        "  # Normalize by length of sentence. Otherwise recommend longest sentences \r\n",
        "  df['doc_num'] = np.arange(len(df))\r\n",
        "  df['sentence_words'] = df.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\r\n",
        "  df['sentence_scores'] = df.apply(lambda row: [np.sum([tfidf[row.doc_num,feature_array.index(word)] for word in sentence]) / len(sentence) for sentence in row.sentence_words], axis = 1)\r\n",
        "\r\n",
        "  # sort keys in order of summed tfidf score\r\n",
        "  df['bestkeys'] = df.sentence_scores.apply(lambda row: np.argsort(row)[::-1])\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cJLGcOJuUo"
      },
      "source": [
        "### Extract Summary\r\n",
        "Grab best sentences based on ranking mechanism     \r\n",
        "Length of summary (Number of sentences)?\r\n",
        "- Number of sentences: generate 1 summary sentence per text sentence (average)\r\n",
        "  - Problem: text sentences are much longer than summary sentences, and since we are producing text sentences as our predicted summary, predicted summary is much longer than label summary\r\n",
        "- Number of words: generate 20 summary words per 1 text word\r\n",
        "  - Strict version: words in summary must be less than the threshold\r\n",
        "  - Less strict version: can go over limit by 1 sentence if reach threshold within the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnqnqVlcuY1W"
      },
      "source": [
        "def extract_summary_num_words(row, config):\r\n",
        "  num_words = 0\r\n",
        "  summary = []\r\n",
        "  cnt = 0 # ensure give at least one sentence in summary\r\n",
        "  for i in row.bestkeys:\r\n",
        "    num_words += len(row.sentences[i].split())\r\n",
        "    if 'num_words_lt' in config:\r\n",
        "      if (num_words >= row.max_words) and (cnt != 0):\r\n",
        "        return summary\r\n",
        "      summary.append(row.sentences[i])\r\n",
        "    if 'num_words_gt' in config:\r\n",
        "      summary.append(row.sentences[i])\r\n",
        "      if num_words >= row.max_words:\r\n",
        "        return summary\r\n",
        "    cnt += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(df, config):\r\n",
        "\r\n",
        "  # summary based on number of sentences \r\n",
        "  if 'num_sentences' in config:\r\n",
        "    df['max_sentences'] = df.sentences.apply(lambda row: int(np.floor(len(row) / 6))) # average 6 summary sentences per doc sentence\r\n",
        "    df['predicted_summary'] = df.apply(lambda row: [row.sentences[i] for i in row.bestkeys[0:row.max_sentences]], axis = 1)\r\n",
        "  # summary based on number of words\r\n",
        "  if 'num_words_gt' in config or 'num_words_lt' in config:\r\n",
        "    df['max_words'] = df.sentences.apply(lambda row: np.floor(len(''.join(row).split(' ')) / 20)) # average 20 summary words per text word\r\n",
        "    df['predicted_summary'] = df.apply(lambda row: extract_summary_num_words(row, config), axis = 1)\r\n",
        "  \r\n",
        "  return df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \r\n",
        "ROUGE metric:\r\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \r\n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \r\n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\r\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\r\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \r\n",
        "  - Did we get all the words in the actual summary?\r\n",
        "- F1 = harmonic mean\r\n",
        "- Look at both purely unigram mesaure and an average of unigram and bigram measure\r\n",
        "  - Don't care about order of words (captured by bigram) as much as in other settings where worried about fluency, syntax of text. Here we know the produced sentences are real English. But still bigrams can capture phrases. \r\n",
        "\r\n",
        "Cons: \r\n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\r\n",
        "- Doesn't consider meaning -- same words could have different meaning   \r\n",
        "  \r\n",
        "Also considered BLEU, but only gives precision.     \r\n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(df):\r\n",
        "  \r\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\r\n",
        "  df['rouge'] = df.apply(lambda row: scorer.score(''.join(row.predicted_summary), ''.join(row.summary)), axis = 1)\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2TBmrUoHLcZ"
      },
      "source": [
        "# TODO compare evaluations between models with paired bootstrap test to test significance? separate config loop for each model type (baseline, page rank etc.)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOruEfeCJ9H"
      },
      "source": [
        "CONFIGURATIONS_BOW = [['pagerank'],\r\n",
        "                      ['nostop', 'stopwords'],\r\n",
        "                      ['no_stemlemma', 'lemma', 'stem'],\r\n",
        "                      ['bow'],\r\n",
        "                      ['counts', 'binary'],\r\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\r\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\r\n",
        "                      ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                      ]\r\n",
        "CONFIGURATIONS_EMBEDDINGS = [['pagerank'],\r\n",
        "                             ['nostop', 'stopwords'],\r\n",
        "                             ['no_stemlemma', 'lemma', 'stem'],\r\n",
        "                             ['embedding'],\r\n",
        "                             ['glove', 'fasttext'],\r\n",
        "                             ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                             ]\r\n",
        "# no custom text cleaning options: true baseline. Stop word removal unnecessary with tfidf.\r\n",
        "CONFIGURATIONS_BASELINE = [['baseline'],\r\n",
        "                           ['num_sentences', 'num_words_lt', 'num_words_gt']\r\n",
        "                           ]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48rmkjQRUM3m"
      },
      "source": [
        "def data_setup():\r\n",
        "\r\n",
        "  # load data\r\n",
        "  df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")\r\n",
        "  df = df.head(100)\r\n",
        "\r\n",
        "  # text cleaning \r\n",
        "  df = text_cleaning(df)\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf84dU5GDRJ7"
      },
      "source": [
        "def feature_setup(df):\r\n",
        "\r\n",
        "  # 1. train TF-IDF on entire corpus for baseline model\r\n",
        "  tfidf, feature_array = corpus_tfidf(df) \r\n",
        "\r\n",
        "  # 2. load embeddings \r\n",
        "  embeddings = {}\r\n",
        "\r\n",
        "  # load glove embeddings - code from https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\r\n",
        "  # 100 length vector for each word \r\n",
        "  glove_wv = {}\r\n",
        "  f = open('glove.6B.100d.txt', encoding='utf-8')\r\n",
        "  for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "      glove_wv[word] = coefs\r\n",
        "  f.close()\r\n",
        "  embeddings['glove'] = glove_wv\r\n",
        "\r\n",
        "  # load fasttext embeddings \r\n",
        "  embeddings['fasttext'] = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/data/shrunk_fasttext_svd.model\")\r\n",
        "\r\n",
        "  return tfidf, feature_array, embeddings"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npK5_hh9bhfD"
      },
      "source": [
        "# TODO think about memory - run in batches and save config results in an append fashion?\r\n",
        "# number of configurations quickly ballooning -- currently 468 configurations...\r\n",
        "  # maybe evaluate on smaller subsample and then pick top x configurations. Then train on full sample. "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbvnFi8EB3r_"
      },
      "source": [
        "def train_loop(df, tfidf, feature_array, embeddings, config_list):\r\n",
        "  config_results = {}\r\n",
        "\r\n",
        "  # loop through configurations\r\n",
        "  for config in config_list:\r\n",
        "    print(config)\r\n",
        "    df_config = df.copy()\r\n",
        "\r\n",
        "    if 'stopwords' in config or 'stem' in config or 'lemma' in config: \r\n",
        "      df_config = text_cleaning_config(df_config, config, stop_words)\r\n",
        "\r\n",
        "    if 'baseline' in config:\r\n",
        "      df_config = tfidf_sum(df_config, feature_array, tfidf)\r\n",
        "    elif 'pagerank' in config:\r\n",
        "      df_config = vector_representation(df_config, config, embeddings)\r\n",
        "      df_config = pagerank(df_config)\r\n",
        "\r\n",
        "    df = extract_summary(df_config, config) \r\n",
        "    df = evaluate(df)\r\n",
        "\r\n",
        "    # evaluate along each metric and average across documents for config stats\r\n",
        "    eval_dict = {}\r\n",
        "    for metric_type in ['fmeasure', 'precision', 'recall']:\r\n",
        "      for avg in [True, False]:\r\n",
        "        if metric_type == 'fmeasure':\r\n",
        "          df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].fmeasure)\r\n",
        "          df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].fmeasure)\r\n",
        "\r\n",
        "        elif metric_type == 'precision':\r\n",
        "          df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].precision)\r\n",
        "          df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].precision)\r\n",
        "\r\n",
        "        elif metric_type == 'recall':\r\n",
        "          df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].recall)\r\n",
        "          df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].recall)\r\n",
        "\r\n",
        "        if avg:\r\n",
        "          df['metric'] = (df.metric1 + df.metric2) / 2\r\n",
        "        else: \r\n",
        "          df['metric'] = df.metric1 \r\n",
        "\r\n",
        "        # average across all documents\r\n",
        "        eval_dict[(metric_type, avg)] = df.metric.mean()\r\n",
        "\r\n",
        "    config_results[str(config)] = eval_dict\r\n",
        "\r\n",
        "  return config_results"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULK7I0KYWnHs"
      },
      "source": [
        "### Find Best Configurations\r\n",
        "- Find configuration with best ROUGE (1) Precision (2) Recall and (3) F-Measure.   \r\n",
        "- Unigram vs average of unigram and bigram metrics\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9y_eRZ8WdmP"
      },
      "source": [
        "def best_configs(config_results):\r\n",
        "  max_metric = 0\r\n",
        "  best_config = ''\r\n",
        "\r\n",
        "  for metric in [('fmeasure', False), ('fmeasure', True), ('precision', False), ('precision', True), ('recall', False), ('recall', True)]:\r\n",
        "    max_metric = 0\r\n",
        "    best_config = ''\r\n",
        "    for k,v in config_results.items():\r\n",
        "      if v[metric] > max_metric:\r\n",
        "        max_metric = v[metric]\r\n",
        "        best_config = k\r\n",
        "\r\n",
        "    if metric[1] == True:\r\n",
        "      avg_text = 'Gram-Average'\r\n",
        "    else:\r\n",
        "      avg_text = ''\r\n",
        "    print(metric, avg_text, ':', best_config, max_metric)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOtDkahyUqN-"
      },
      "source": [
        "def main():\r\n",
        "\r\n",
        "  df = data_setup()\r\n",
        "  tfidf, feature_array, embeddings = feature_setup(df)\r\n",
        "\r\n",
        "  config_list_pr = list(itertools.product(*CONFIGURATIONS_BOW)) + list(itertools.product(*CONFIGURATIONS_EMBEDDINGS)) \r\n",
        "  config_list_baseline = list(itertools.product(*CONFIGURATIONS_BASELINE))\r\n",
        "\r\n",
        "  config_results_baseline = train_loop(df, tfidf, feature_array, embeddings, config_list_baseline)\r\n",
        "  config_results_pr = train_loop(df, tfidf, feature_array, embeddings, config_list_pr) \r\n",
        "  \r\n",
        "  for model in ['baseline', 'pr']:\r\n",
        "    if model == 'baseline':\r\n",
        "      print('------- Baseline -------')\r\n",
        "      config_results = config_results_baseline\r\n",
        "    elif model == 'pr':\r\n",
        "      print('------- Text Rank -------')\r\n",
        "      config_results = config_results_pr\r\n",
        "    best_configs(config_results)\r\n",
        "\r\n",
        "  return config_results_baseline, config_results_pr"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuBzDfGKV8El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dae6f8-dc4a-4a05-d774-3be81b94e73f"
      },
      "source": [
        "config_results_baseline, config_results_pr = main()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('baseline', 'num_sentences')\n",
            "('baseline', 'num_words_lt')\n",
            "('baseline', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'counts', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'no_normalization', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tf', 'all', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'unigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'bow', 'binary', 'tfidf', 'all', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'no_stemlemma', 'embedding', 'fasttext', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'lemma', 'embedding', 'fasttext', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'nostop', 'stem', 'embedding', 'fasttext', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'no_stemlemma', 'embedding', 'fasttext', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'lemma', 'embedding', 'fasttext', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'glove', 'num_words_gt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'fasttext', 'num_sentences')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'fasttext', 'num_words_lt')\n",
            "('pagerank', 'stopwords', 'stem', 'embedding', 'fasttext', 'num_words_gt')\n",
            "------- Baseline -------\n",
            "('fmeasure', False)  : ('baseline', 'num_words_gt') 0.2576556335761917\n",
            "('fmeasure', True) Gram-Average : ('baseline', 'num_words_gt') 0.16834545772761758\n",
            "('precision', False)  : ('baseline', 'num_sentences') 0.3800165150186769\n",
            "('precision', True) Gram-Average : ('baseline', 'num_sentences') 0.25281425579414024\n",
            "('recall', False)  : ('baseline', 'num_words_lt') 0.32612571711141647\n",
            "('recall', True) Gram-Average : ('baseline', 'num_words_lt') 0.21086882348522962\n",
            "------- Text Rank -------\n",
            "('fmeasure', False)  : ('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt') 0.28448580264390644\n",
            "('fmeasure', True) Gram-Average : ('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'bigram', 'num_words_gt') 0.1885361928293757\n",
            "('precision', False)  : ('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'trigram', 'num_sentences') 0.4750228309666687\n",
            "('precision', True) Gram-Average : ('pagerank', 'nostop', 'stem', 'bow', 'binary', 'tfidf', 'bigram', 'num_sentences') 0.3183516090003104\n",
            "('recall', False)  : ('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tf', 'all', 'num_words_lt') 0.3412046033100429\n",
            "('recall', True) Gram-Average : ('pagerank', 'stopwords', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'all', 'num_words_lt') 0.22428739592759983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rZkQzcEGoF"
      },
      "source": [
        "# TODO: save distribution of scores; not just averages for each config. "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnBlkEugTSXE"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGhYAOJZ5Hth"
      },
      "source": [
        "#config_results[\"('pagerank', 'nostop', 'no_stemlemma', 'bow', 'counts', 'tfidf', 'unigram', 'num_words_lt')\"]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deqHkWtUGuYQ"
      },
      "source": [
        "#config_results[\"('baseline', 'num_words_lt')\"]"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}