{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extractive_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN895LScKnkbzC32k3QEqGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/extractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDMy4RQPzHg7"
      },
      "source": [
        "# this project is focusing on unsupervised approaches with less human assumptions about what makes a good summary. Allow content owners to ultimately decide \n",
        "# but there are some other interesting, more supervised-ish (kinda?) approaches that exist: \n",
        "  # Possible extension/further research: score each sentence according to some generated features: https://github.com/xiaoxu193/PyTeaser/blob/master/pyteaser.py\n",
        "    # other ideas for features: https://medium.com/@umerfarooq_26378/text-summarization-in-python-76c0a41f0dc4\n",
        "    # not sure want to go down this rabbit hole - lots of tuning, feature creation etc\n",
        "  # also some people do binary classification for if each sentence should be in the summary. \n",
        "\n",
        "# https://medium.com/@umerfarooq_26378/text-summarization-in-python-76c0a41f0dc4 - also includes abstractive methods\n",
        "# https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/ - BART auto-regressive for summarization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeuS-7H7Yy0"
      },
      "source": [
        "%%capture\n",
        "!pip install rouge-score\n",
        "!pip install fasttext\n",
        "!pip install compress-fasttext\n",
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K99-qm9GrHPX",
        "outputId": "6a8c1d0e-ce88-43a9-8631-f4da844dc44d"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from collections import Counter, OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "import networkx as nx\n",
        "from rouge_score import rouge_scorer\n",
        "import gensim\n",
        "import fasttext\n",
        "from gensim.models import FastText\n",
        "import compress_fasttext\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8tlcZihfAr"
      },
      "source": [
        "# clean sentences for similarity comparisons; not for final display\n",
        "# always do this function\n",
        "# note that some sentence tokenization is messy. Ex: if numbered list, list becomes own sentence.\n",
        "  # but shouldn't show up as important as summary sentence anyways\n",
        "def text_cleaning(df):\n",
        "  # downcase everything\n",
        "  df['sentences_cleaned'] = df.sentences.apply(lambda text: [sentence.lower() for sentence in text])\n",
        "  # remove punctuation \n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"[^\\w\\s]\", '', sentence) for sentence in text])\n",
        "  # drop extra spaces\n",
        "  df.sentences_cleaned = df.sentences_cleaned.apply(lambda text: [re.sub(\"\\s+\", ' ', sentence) for sentence in text])\n",
        "  # drop sentences that only have one word (usually because numbered list etc.)\n",
        "    # also drop from sentences: need indexes to match for final summary extraction \n",
        "  df['drop_sentences'] = df.sentences_cleaned.apply(lambda text: [i for i in range(len(text)) if len(text[i].split()) <= 1])\n",
        "  df.sentences_cleaned = df.apply(lambda row: [row.sentences_cleaned[i] for i in range(len(row.sentences_cleaned)) if i not in row.drop_sentences], axis = 1)\n",
        "  df.sentences = df.apply(lambda row: [row.sentences[i] for i in range(len(row.sentences)) if i not in row.drop_sentences], axis = 1)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbTbtLtSs7yJ"
      },
      "source": [
        "# cleaning depending on configuration\n",
        "def text_cleaning_config(df_config, config, stop_words):\n",
        "  df_config['words'] = df_config.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\n",
        "  if 'stopwords' in config:\n",
        "    df_config.words = df_config.words.apply(lambda row: [[w for w in sentence if not w in stop_words] for sentence in row])\n",
        "  if 'stem' in config:\n",
        "    stemmer = PorterStemmer()\n",
        "    df_config.words = df_config.words.apply(lambda row: [[stemmer.stem(w) for w in sentence] for sentence in row])\n",
        "  if 'lemma' in config:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df_config.words = df_config.words.apply(lambda row: [[lemmatizer.lemmatize(w) for w in sentence] for sentence in row])\n",
        "\n",
        "  # recombine words into sentences\n",
        "  df_config.sentences_cleaned = df_config.words.apply(lambda row: [' '.join(sentence) for sentence in row])\n",
        "\n",
        "  # drop sentences that only have one word after these exclusions\n",
        "    # also drop from sentences: need indexes to match for final summary extraction \n",
        "  df_config['drop_sentences'] = df_config.sentences_cleaned.apply(lambda text: [i for i in range(len(text)) if len(text[i].split()) <= 1])\n",
        "  df_config.sentences_cleaned = df_config.apply(lambda row: [row.sentences_cleaned[i] for i in range(len(row.sentences_cleaned)) if i not in row.drop_sentences], axis = 1)\n",
        "  df_config.sentences = df_config.apply(lambda row: [row.sentences[i] for i in range(len(row.sentences)) if i not in row.drop_sentences], axis = 1)\n",
        "\n",
        "  return df_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHiiHeyCaEhW"
      },
      "source": [
        "### Train TFIDF in Corpus\n",
        "Used in baseline model to sum tfidf scores within each sentence in each document "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLrQnm0VYygp"
      },
      "source": [
        "def corpus_tfidf(df):\n",
        "  \n",
        "  # list of words in each article\n",
        "  corpus = df.sentences_cleaned.to_list()\n",
        "  corpus = [' '.join(article) for article in corpus]  \n",
        "  corpus = [article.split(' ') for article in corpus]\n",
        "\n",
        "  # tfidf trained on entire corpus: document = article\n",
        "  tfidf_vec = TfidfVectorizer(analyzer = 'word', \n",
        "                              tokenizer = lambda doc: doc, preprocessor = lambda doc: doc, token_pattern = None)\n",
        "                              # already did preprocessing, so using identity functions for tokenizer and preprocessor\n",
        "  tfidf = tfidf_vec.fit_transform(corpus) # sparse arrays of scores for each word in each article. articles x words\n",
        "  feature_array = list(tfidf_vec.get_feature_names())\n",
        "  \n",
        "  return tfidf, feature_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzEExkib6-l5"
      },
      "source": [
        "### Vector Representation \n",
        "Default: unigram bag of words with counts\n",
        "Options: \n",
        "1. Bow\n",
        "  - binary: bag of words with binary indicators rather than counts (don't use with tfidf)\n",
        "  - tf: term frequency normalization \n",
        "    - Same as default if cosine similarity. Cosine similarity does the normalization (double check this!!)\n",
        "  - idf: inverse document normalization \n",
        "  - include_bigrams/include_trigrams: include bigrams and/or trigrams of words in addition to unigrams as distinct tokens in bag of words\n",
        "    - Gives sense of order in sentence, capture _concepts_ rather than just individual words\n",
        "2. Embeddings (pre-trained)\n",
        "  - GloVe\n",
        "  - Fasttext\n",
        "    - Advantage: generate embeddings for out of vocabulary words based on their parts\n",
        "    - But memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDh393Ow2Ze"
      },
      "source": [
        "# vector representation of words in each sentence in document \n",
        "def vector_representation(df, configuration, embeddings):\n",
        "\n",
        "  # list of words in each sentence \n",
        "  df['words'] = df.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\n",
        "\n",
        "  if 'bow' in configuration:\n",
        "\n",
        "    # include bigrams and/or trigrams (in addition to unigrams) in bow \n",
        "    if 'bigram' in configuration or 'all' in configuration:\n",
        "      df['bigrams'] = df.words.apply(lambda row: [list(nltk.bigrams(sentence)) if len(sentence) >= 2 else '' for sentence in row])\n",
        "      df.bigrams = df.bigrams.apply(lambda row: [[words[0] + ' ' + words[1] for words in sentence] for sentence in row])\n",
        "      df.words = df.apply(lambda row: [row.bigrams[j] + row.words[j] for j in range(len(row.words))], axis = 1)\n",
        "    if 'trigram' in configuration or 'all' in configuration:\n",
        "      df['trigrams'] = df.words.apply(lambda row: [list(nltk.trigrams(sentence)) if len(sentence) >= 3 else '' for sentence in row])\n",
        "      df.trigrams = df.trigrams.apply(lambda row: [[words[0] + ' ' + words[1] + words[2] for words in sentence] for sentence in row])\n",
        "      df.words = df.apply(lambda row: [row.trigrams[j] + row.words[j] for j in range(len(row.words))], axis = 1)\n",
        "\n",
        "    # bag of words with binary indicators for words/n-grams rather than counts\n",
        "    if 'binary' in configuration: \n",
        "      df.words = df.words.apply(lambda row: [set(sentence) for sentence in row])\n",
        "\n",
        "    # bag of words: # sentences x # unique words\n",
        "    vec = DictVectorizer()\n",
        "    df['vector_rep'] = df.words.apply(lambda row: vec.fit_transform(Counter(f) for f in row))\n",
        "\n",
        "    # term frequency normalization\n",
        "    if 'tf' in configuration: \n",
        "      tfidf_transformer = TfidfTransformer(use_idf = False)\n",
        "      df['vector_rep'] = df.vector_rep.apply(lambda row: tfidf_transformer.fit_transform(row))\n",
        "    # term frequency-inverse document frequency normalization\n",
        "    if 'tfidf' in configuration:\n",
        "      tfidf_transformer = TfidfTransformer(use_idf = True)\n",
        "      df['vector_rep'] = df.vector_rep.apply(lambda row: tfidf_transformer.fit_transform(row))  \n",
        "\n",
        "  # possible extension: continued training on specific corpus. Probably unnecessary since wikipedia and news article words should be similar\n",
        "  if 'embedding' in configuration:\n",
        "\n",
        "    if 'glove' in configuration:\n",
        "      word_embeddings = embeddings['glove']\n",
        "      # find average of word embeddings for each sentence \n",
        "      # if unknown word, give embedding = 0 \n",
        "      df['vector_rep'] = df.sentences_cleaned.apply(lambda row: [sum([word_embeddings.get(word, np.zeros(100,)) for word in sentence.split()]) / len(sentence.split()) for sentence in row])\n",
        "\n",
        "    # fasttext.\n",
        "    if 'fasttext' in configuration:\n",
        "      word_embeddings = embeddings['fasttext']\n",
        "      # find average of word embeddings for each sentence \n",
        "      df['vector_rep'] = df.sentences_cleaned.apply(lambda row: [sum([word_embeddings[word] for word in sentence.split()]) / len(sentence.split()) for sentence in row])\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA9LokhRPWTF"
      },
      "source": [
        "### Baseline Model\n",
        "- Train TF-IDF on entire corpus where document = article. Get a score for each word in each document\n",
        "- Sum scores for all words in each sentence \n",
        "- Produce sentences with highest total TF-IDF score \n",
        "\n",
        "Idea: Sentences that are indicative of the specifics of the article. High frequency in the article, but specific to the article\n",
        "\n",
        "Could also try straight term frequencies within the article. (or weighted like above so fractional of most frequent rather than diff. magnitudes). Would need to drop stop words first (https://stackabuse.com/text-summarization-with-nltk-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5IguMtPWTF"
      },
      "source": [
        "def tfidf_sum(df, feature_array, tfidf):\n",
        "\n",
        "  # sum tfidf score within each sentence. \n",
        "  # Normalize by length of sentence. Otherwise recommend longest sentences \n",
        "  df['doc_num'] = np.arange(len(df))\n",
        "  df['sentence_words'] = df.sentences_cleaned.apply(lambda row: [sentence.split() for sentence in row])\n",
        "  df['sentence_scores'] = df.apply(lambda row: [np.sum([tfidf[row.doc_num,feature_array.index(word)] for word in sentence]) / len(sentence) for sentence in row.sentence_words], axis = 1)\n",
        "\n",
        "  # sort keys in order of summed tfidf score\n",
        "  df['bestkeys'] = df.sentence_scores.apply(lambda row: np.argsort(row)[::-1])\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZyF8DHeaVrY"
      },
      "source": [
        "### TextRank Model\n",
        "Similarity Metrics:\n",
        "- Cosine\n",
        "- Jaccard: proportion of elements that are the same where at least one is non-zero\n",
        "  - AKA, what percent of words in the two sentences are the same?\n",
        "- Hamming: proportion of elements that are not same (including zeroes)\n",
        "  - AKA, out of all words in the vocabulary, what percent of words are both in or both not in two sentences\n",
        "\n",
        "Both Jaccard and Hamming only make sense on binary representations of the data. Only applied to binary vector representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fl60wuOr-C2"
      },
      "source": [
        "%%capture\n",
        "def textrank(df, config):\n",
        "\n",
        "  # similarity matrix between sentences\n",
        "  if 'cosine' in config:\n",
        "    df['sim'] = df.vector_rep.apply(lambda row: cosine_similarity(row))\n",
        "  elif 'jaccard' in config:\n",
        "    df['sim'] = df.vector_rep.apply(lambda row: 1 - pairwise_distances(row.A, metric = 'jaccard'))\n",
        "  elif 'hamming' in config:\n",
        "    df['sim'] = df.vector_rep.apply(lambda row: 1 - pairwise_distances(row.A, metric = 'hamming'))\n",
        "\n",
        "  # graph where node = sentence, edge weight = simialarity score\n",
        "  df['graph'] = df.sim.apply(lambda row: nx.from_numpy_array(row))  \n",
        "  # page rank\n",
        "  df['textrank'] = df.graph.apply(lambda row: nx.pagerank_numpy(row))\n",
        "  # sort keys in order of page rank\n",
        "  df['bestkeys'] = df.pr.apply(lambda row: sorted(row, key = row.get, reverse = True))\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fphax_87PQNr"
      },
      "source": [
        "### Latent Semantic Allocation (Singular Value Decomposition)\n",
        "1. SVD to get matrix of sentences x latent concepts, ordered in importance of concept to article\n",
        "2. For each concept, get the highest scored sentence for that concept. Get best sentence for each concept in decreasing order of concept importance. \n",
        "  - Ensures summary captures the most important aspects of the article and covers the span of concepts.\n",
        "\n",
        "Other sentence extraction techniques have been explored, but they are mostly deal with choosing multiple sentences from the same concept. In our application, the summaries are very short (a few sentences each) and it is more important to capture the breadth of topics discussed so that it is retrievable in search and so that the content creator can edit and specify based on an overall outline of their content. \n",
        "\n",
        "https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis\n",
        "\n",
        "\n",
        "Also considered using LDA, but there's no sense of dominant topic for the overall corpus (article). Rather, you can only get the probability each sentence belongs to each topic. So I could get best sentence per topic, but don't know in which order to place those sentences in the overall ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dogL5-PPObm"
      },
      "source": [
        "def lsa(df):\n",
        "\n",
        "  # SVD: get Vt sentences x concepts matrix\n",
        "  df['svd'] = df.vector_rep.apply(lambda row: np.linalg.svd(row.T.todense(), full_matrices = False)[2])\n",
        "  # sentence with highest score for each concept in order of most important concept to document\n",
        "  df['bestkeys'] = df.svd.apply(lambda row: [np.argmax(row[:, i]) for i in range(len(row))])\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cJLGcOJuUo"
      },
      "source": [
        "### Extract Summary\n",
        "Grab best sentences based on ranking mechanism     \n",
        "Length of summary (Number of sentences)?\n",
        "- Number of sentences: generate 1 summary sentence per text sentence (average)\n",
        "  - Problem: text sentences are much longer than summary sentences, and since we are producing text sentences as our predicted summary, predicted summary is much longer than label summary\n",
        "- Number of words: generate 20 summary words per 1 text word\n",
        "  - Strict version: words in summary must be less than the threshold\n",
        "  - Less strict version: can go over limit by 1 sentence if reach threshold within the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnqnqVlcuY1W"
      },
      "source": [
        "def extract_summary_num_words(row, config):\n",
        "  num_words = 0\n",
        "  summary = []\n",
        "  cnt = 0 # ensure give at least one sentence in summary\n",
        "  for i in row.bestkeys:\n",
        "    num_words += len(row.sentences[i].split())\n",
        "    if 'num_words_lt' in config:\n",
        "      if (num_words >= row.max_words) and (cnt != 0):\n",
        "        return summary\n",
        "      summary.append(row.sentences[i])\n",
        "    if 'num_words_gt' in config:\n",
        "      summary.append(row.sentences[i])\n",
        "      if num_words >= row.max_words:\n",
        "        return summary\n",
        "    cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6uxY2t9y4x3"
      },
      "source": [
        "def extract_summary(df, config):\n",
        "\n",
        "  # summary based on number of sentences \n",
        "  if 'num_sentences' in config:\n",
        "    df['max_sentences'] = df.sentences.apply(lambda row: int(np.floor(len(row) / 6))) # average 6 summary sentences per doc sentence\n",
        "    df['predicted_summary'] = df.apply(lambda row: [row.sentences[i] for i in row.bestkeys[0:row.max_sentences]], axis = 1)\n",
        "  # summary based on number of words\n",
        "  if 'num_words_gt' in config or 'num_words_lt' in config:\n",
        "    df['max_words'] = df.sentences.apply(lambda row: np.floor(len(''.join(row).split(' ')) / 20)) # average 20 summary words per text word\n",
        "    df['predicted_summary'] = df.apply(lambda row: extract_summary_num_words(row, config), axis = 1)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVFZ2LEeEX4"
      },
      "source": [
        "### Evaluation \n",
        "ROUGE metric:\n",
        "https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.YEKJyI5KiUl   \n",
        "- Precision = # overlapping ngrams / # total ngrams in produced summary \n",
        "  - Measure of junk. Did we produce a lot in the generated summary that is not in the actual summary?\n",
        "  - Important if we don't manually set the length. The generated summary could be very long which causes good recall\n",
        "- Recall = # overlapping ngrams / # total ngrams in label summary  \n",
        "  - Did we get all the words in the actual summary?\n",
        "- F1 = harmonic mean\n",
        "- Look at both purely unigram mesaure and an average of unigram and bigram measure\n",
        "  - Don't care about order of words (captured by bigram) as much as in other settings where worried about fluency, syntax of text. Here we know the produced sentences are real English. But still bigrams can capture phrases. \n",
        "\n",
        "Cons: \n",
        "- Doesn't look at sentence structure --> doesn't apply here because using correct sentences\n",
        "- Doesn't consider meaning -- same words could have different meaning   \n",
        "  \n",
        "Also considered BLEU, but only gives precision.     \n",
        "https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9IsZ0MU7e6B"
      },
      "source": [
        "def evaluate(df):\n",
        "  \n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = False)\n",
        "  df['rouge'] = df.apply(lambda row: scorer.score(''.join(row.predicted_summary), ''.join(row.summary)), axis = 1)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuTc_IzefUe0"
      },
      "source": [
        "# get distribution of each metric across documents\n",
        "def metrics_distribution(df):\n",
        "    \n",
        "  eval_dict = {}\n",
        "  for metric_type in ['fmeasure', 'precision', 'recall']:\n",
        "    for avg in [True, False]:\n",
        "      if metric_type == 'fmeasure':\n",
        "        df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].fmeasure)\n",
        "        df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].fmeasure)\n",
        "\n",
        "      elif metric_type == 'precision':\n",
        "        df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].precision)\n",
        "        df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].precision)\n",
        "\n",
        "      elif metric_type == 'recall':\n",
        "        df['metric1'] = df.rouge.apply(lambda row: row['rouge1'].recall)\n",
        "        df['metric2'] = df.rouge.apply(lambda row: row['rouge2'].recall)\n",
        "\n",
        "      if avg:\n",
        "        df['metric'] = (df.metric1 + df.metric2) / 2\n",
        "      else: \n",
        "        df['metric'] = df.metric1 \n",
        "\n",
        "      # record distribution of metric across all documents\n",
        "      eval_dict[(metric_type, avg)] = df.metric.describe()\n",
        "\n",
        "  return eval_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TfX9h6HjY2v"
      },
      "source": [
        "### Find Best Configurations\n",
        "- Find configuration with best ROUGE (1) Precision (2) Recall and (3) F-Measure.   \n",
        "- For each configuration, mean metric for all documents \n",
        "- Unigram vs average of unigram and bigram metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjKTpAt3jW9n"
      },
      "source": [
        "def find_best_configs(config_results):\n",
        "  max_metric = 0\n",
        "  best_config = ''\n",
        "  best_config_dict = {}\n",
        "\n",
        "  for metric in [('fmeasure', False), ('fmeasure', True), ('precision', False), ('precision', True), ('recall', False), ('recall', True)]:\n",
        "    max_metric = 0\n",
        "    best_config = ''\n",
        "    for k,v in config_results.items():\n",
        "      if v[metric]['mean'] > max_metric:\n",
        "        max_metric = v[metric]['mean']\n",
        "        best_config = k\n",
        "\n",
        "    best_config_dict[metric] = best_config\n",
        "  return best_config_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PPBhb_UbP5_"
      },
      "source": [
        "### Main Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOruEfeCJ9H"
      },
      "source": [
        "# All possible configurations for each model\n",
        "CONFIGURATIONS_BOW = [['textrank'],\n",
        "                      ['nostop', 'stopwords'],\n",
        "                      ['no_stemlemma', 'lemma', 'stem'],\n",
        "                      ['bow'],\n",
        "                      ['counts', 'binary'],\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\n",
        "                      ['cosine', 'hamming', 'jaccard'],\n",
        "                      ['num_sentences', 'num_words_lt', 'num_words_gt']\n",
        "                      ]          \n",
        "CONFIGURATIONS_EMBEDDINGS = [['textrank'],\n",
        "                             ['nostop', 'stopwords'],\n",
        "                             ['no_stemlemma', 'lemma', 'stem'],\n",
        "                             ['embedding'],\n",
        "                             ['glove', 'fasttext'],\n",
        "                             ['cosine'],\n",
        "                             ['num_sentences', 'num_words_lt', 'num_words_gt']\n",
        "                             ]\n",
        "# no custom text cleaning options: true baseline. Stop word removal unnecessary with tfidf.\n",
        "CONFIGURATIONS_BASELINE = [['baseline'],\n",
        "                           ['num_sentences', 'num_words_lt', 'num_words_gt']\n",
        "                           ]\n",
        "CONFIGURATIONS_LSA = [['lsa'],\n",
        "                      ['nostop', 'stopwords'],\n",
        "                      ['no_stemlemma', 'lemma', 'stem'],\n",
        "                      ['bow'],\n",
        "                      ['counts', 'binary'],\n",
        "                      ['no_normalization', 'tf', 'tfidf'],\n",
        "                      ['unigram', 'bigram', 'trigram', 'all'],\n",
        "                      ['num_sentences', 'num_words_lt', 'num_words_gt']]\n",
        "\n",
        "# cross products of all possible combinations of configurations\n",
        "model_configurations = {'textrank':list(itertools.product(*CONFIGURATIONS_BOW)) + list(itertools.product(*CONFIGURATIONS_EMBEDDINGS)),\n",
        "                        'baseline':list(itertools.product(*CONFIGURATIONS_BASELINE)),\n",
        "                        'lsa':list(itertools.product(*CONFIGURATIONS_LSA))}\n",
        "                    \n",
        "# textrank: remove configurations with jaccard or hamming distance that don't use a binary vector representation\n",
        "model_configurations['textrank'] = [i for i in model_configurations['textrank'] if (('jaccard' not in i) and ('hamming') not in i) or (('binary' in i) and ('no_normalization' in i))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48rmkjQRUM3m"
      },
      "source": [
        "def data_setup():\n",
        "\n",
        "  # load data\n",
        "  df = pd.read_pickle(\"/content/drive/MyDrive/data/cleaned_df.pkl\")\n",
        "  df = df.head(110) # TODO 10000 is this a good size to limit to? _randomize_\n",
        "\n",
        "  # text cleaning \n",
        "  df = text_cleaning(df)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFz0Rs3LUZ1A"
      },
      "source": [
        "def load_embeddings():\n",
        "  embeddings = {}\n",
        "\n",
        "  # load glove embeddings - code from https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
        "  # 100 length vector for each word \n",
        "  glove_wv = {}\n",
        "  f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      glove_wv[word] = coefs\n",
        "  f.close()\n",
        "  embeddings['glove'] = glove_wv\n",
        "\n",
        "  # load fasttext embeddings \n",
        "  embeddings['fasttext'] = gensim.models.KeyedVectors.load(\"/content/drive/MyDrive/data/shrunk_fasttext_svd.model\")\n",
        "\n",
        "  return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbvnFi8EB3r_"
      },
      "source": [
        "def train_config_loop(df, tfidf, feature_array, embeddings, config_list):\n",
        "  eval_results = {}\n",
        "  model_results = {}\n",
        "\n",
        "  # loop through configurations\n",
        "  for config in config_list:\n",
        "    print(config)\n",
        "    df_config = df.copy()\n",
        "\n",
        "    if 'stopwords' in config or 'stem' in config or 'lemma' in config: \n",
        "      df_config = text_cleaning_config(df_config, config, stop_words)\n",
        "\n",
        "    if 'baseline' in config:\n",
        "      df_config = tfidf_sum(df_config, feature_array, tfidf)\n",
        "    elif 'textrank' in config:\n",
        "      df_config = vector_representation(df_config, config, embeddings)\n",
        "      df_config = textrank(df_config, config)\n",
        "    elif 'lsa' in config:\n",
        "      df_config = vector_representation(df_config, config, embeddings)\n",
        "      df_config = lsa(df_config)\n",
        "\n",
        "    df_config = extract_summary(df_config, config) \n",
        "    eval_dict = evaluate(df_config)\n",
        "\n",
        "    # distribution of metrics across all documents\n",
        "    eval_results[str(config)] = metrics_distribution(df_config)\n",
        "    # keep track of model results (with predicted_summaries) \n",
        "    model_results[str(config)] = df_config\n",
        "\n",
        "  return eval_results, model_results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}