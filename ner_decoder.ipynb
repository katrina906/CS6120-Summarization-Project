{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katrina906/CS6120-Summarization-Project/blob/main/ner_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY0LJOIT_cG"
      },
      "source": [
        "# Implementing a Viterbi Decoder and Evaluation for Sequence Labeling\n",
        "\n",
        "In this assignment, you will build a Viterbi decoder for an LSTM named-entity recognition model. As we mentioned in class, recurrent and bidirectional recurrent neural networks, of which LSTMs are the most common examples, can be used to perform sequence labeling. Although these models encode information from the surrounding words in order to make predictions, there are no \"hard\" constraints on what tags can appear where.\n",
        "\n",
        "There hard constraints are particularly important for tasks that label spans of more than one token. The most common example of a span-labeling task is named-entity recognition (NER). As described in Eisenstein, Jurafksy & Martin, and other texts, the goal of NER is to label spans of one or more words as _mentions_ of an _entity_, such as a person, location, organization, etc.\n",
        "\n",
        "The most common approach to NER is to reduce it to a sequence-labeling task, where each token in the input is labeled either with an `O`, if it is \"outside\" any named-entity span, or with `B-TYPE`, if it is the first token in an entity of type `TYPE`, or with `I-TYPE`, if it is the second or later token in an entity of type `TYPE`. Distinguishing between the first and later tokens of an entity allow us to identify distinct entity spans even when they are adjacent.\n",
        "\n",
        "Common values of `TYPE` include `PER` for person, `LOC` for location, `DATE` for date, and so on. In the dataset we load below, there are 17 distinct types.\n",
        "\n",
        "The span-labeling scheme just described implies that the labels on tokens must obey certain constraints: the tag `I-PER` must follow either `B-PER` or another `I-PER`. It cannot follow `O`, `B-LOC`, or `I-LOC`, i.e., a tag for a different entity type. By themselves, LSTMs or bidirectional LSTMs cannot directly enforce these constraints. This is one reason why conditional random fields (CRFs), which _can_ enforce these constraints, are often layered on top of these recurrent models.\n",
        "\n",
        "In this assignment, you will implement the simplest possible CRF: a CRF so simple that it does not require any training. Rather, it will assign weight 1 to any sequence of tags that obeys the constraints and weight 0 to any sequence of tags that violates them. The inputs to the CRF, which are analogous to the emission probabilities in an HMM, will come from an LSTM.\n",
        "\n",
        "But first, in order to test your decoder, you will also implement some functions to evaluate the output of an NER system according to two metrics:\n",
        "1. You will count the number of _violations_ of the NER label constraints, i.e., how many times `I-TYPE` follows `O` or a tag of a different type or occurs at the beginning of a sentence. This number will be greater than 0 in the raw LSTM output, but should be 0 for your CRF output.\n",
        "1. You will compute the _span-level_ precision, recall, and F1 of NER output. Although the baseline LSTM was trained to achieve high _token-level_ accuracy, this metric can be misleadingly high, since so many tokens are correctly labeled `O`. In other words, what proportion of spans predicted by the model line up exactly with spans in the gold standard, and what proportion of spans in the gold standard were predicted by the model? Define _span_ as a sequence of tags that starts with a `B-TYPE` followed by zero or more `I-TYPE` tags. Sequences solely of `I-TYPE` tags don't count as spans.For more, see the original task definition: https://www.aclweb.org/anthology/W03-0419/.\n",
        "\n",
        "We start with loading some code and data and the describe your tasks in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnn49QEU_Ik"
      },
      "source": [
        "## Set Up Dependencies and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJINX1MwOLBT"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade spacy==2.1.0 allennlp==0.9.0\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zJfaIlJ2bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855f74db-c22f-477d-b33b-a92e9dae47da"
      },
      "source": [
        "from typing import Iterator, List, Dict\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.dataset_readers import conll2003\n",
        "import pandas as pd\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f905f8367d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo16Ko0Gchxk"
      },
      "source": [
        "class LstmTagger(Model):\n",
        "  def __init__(self,\n",
        "               word_embeddings: TextFieldEmbedder,\n",
        "               encoder: Seq2SeqEncoder,\n",
        "               vocab: Vocabulary) -> None:\n",
        "    super().__init__(vocab)\n",
        "    self.word_embeddings = word_embeddings\n",
        "    self.encoder = encoder\n",
        "    self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(self,\n",
        "              tokens: Dict[str, torch.Tensor],\n",
        "              metadata,\n",
        "              tags: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "    mask = get_text_field_mask(tokens)\n",
        "    embeddings = self.word_embeddings(tokens)\n",
        "    encoder_out = self.encoder(embeddings, mask)\n",
        "    tag_logits = self.hidden2tag(encoder_out)\n",
        "    output = {\"tag_logits\": tag_logits}\n",
        "    if tags is not None:\n",
        "      self.accuracy(tag_logits, tags, mask)\n",
        "      output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, tags, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {\"accuracy\": self.accuracy.get_metric(reset)}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdKvPftVVLt"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOVVZslKm3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bdd61d-9a17-4943-f965-723c3e392473"
      },
      "source": [
        "reader = conll2003.Conll2003DatasetReader()\n",
        "train_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.train.ner.sample'))\n",
        "validation_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.development.ner.sample'))\n",
        "\n",
        "from itertools import chain\n",
        "vocab = Vocabulary.from_instances(chain(train_dataset, validation_dataset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159377B [00:00, 2605116.85B/s]\n",
            "562it [00:00, 18547.33it/s]\n",
            "8366B [00:00, 5210802.98B/s]\n",
            "23it [00:00, 3081.68it/s]\n",
            "585it [00:00, 57725.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9bWKPmGMUbW"
      },
      "source": [
        "#list(train_dataset[100]['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpg2Udr-Vnwm"
      },
      "source": [
        "## Define and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kDQQBMywdKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21faae48-5a81-4678-f088-69f3c83dce33"
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=False, batch_first=True))\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=100,\n",
        "                  cuda_device=cuda_device)\n",
        "trainer.train()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8442, loss: 0.9027 ||: 100%|██████████| 281/281 [00:01<00:00, 188.78it/s]\n",
            "accuracy: 0.7878, loss: 1.2009 ||: 100%|██████████| 12/12 [00:00<00:00, 423.15it/s]\n",
            "accuracy: 0.8442, loss: 0.7302 ||: 100%|██████████| 281/281 [00:01<00:00, 255.52it/s]\n",
            "accuracy: 0.7878, loss: 1.1918 ||: 100%|██████████| 12/12 [00:00<00:00, 411.32it/s]\n",
            "accuracy: 0.8442, loss: 0.7157 ||: 100%|██████████| 281/281 [00:01<00:00, 259.25it/s]\n",
            "accuracy: 0.7878, loss: 1.1967 ||: 100%|██████████| 12/12 [00:00<00:00, 458.44it/s]\n",
            "accuracy: 0.8442, loss: 0.7069 ||: 100%|██████████| 281/281 [00:01<00:00, 254.95it/s]\n",
            "accuracy: 0.7878, loss: 1.1707 ||: 100%|██████████| 12/12 [00:00<00:00, 419.80it/s]\n",
            "accuracy: 0.8442, loss: 0.6985 ||: 100%|██████████| 281/281 [00:01<00:00, 255.25it/s]\n",
            "accuracy: 0.7878, loss: 1.1755 ||: 100%|██████████| 12/12 [00:00<00:00, 395.90it/s]\n",
            "accuracy: 0.8442, loss: 0.6904 ||: 100%|██████████| 281/281 [00:01<00:00, 257.33it/s]\n",
            "accuracy: 0.7878, loss: 1.1467 ||: 100%|██████████| 12/12 [00:00<00:00, 382.56it/s]\n",
            "accuracy: 0.8442, loss: 0.6812 ||: 100%|██████████| 281/281 [00:01<00:00, 258.54it/s]\n",
            "accuracy: 0.7878, loss: 1.1563 ||: 100%|██████████| 12/12 [00:00<00:00, 421.81it/s]\n",
            "accuracy: 0.8442, loss: 0.6691 ||: 100%|██████████| 281/281 [00:01<00:00, 257.47it/s]\n",
            "accuracy: 0.7878, loss: 1.1478 ||: 100%|██████████| 12/12 [00:00<00:00, 489.10it/s]\n",
            "accuracy: 0.8442, loss: 0.6544 ||: 100%|██████████| 281/281 [00:01<00:00, 260.98it/s]\n",
            "accuracy: 0.7878, loss: 1.0997 ||: 100%|██████████| 12/12 [00:00<00:00, 410.61it/s]\n",
            "accuracy: 0.8442, loss: 0.6383 ||: 100%|██████████| 281/281 [00:01<00:00, 259.45it/s]\n",
            "accuracy: 0.7878, loss: 1.0883 ||: 100%|██████████| 12/12 [00:00<00:00, 417.39it/s]\n",
            "accuracy: 0.8442, loss: 0.6136 ||: 100%|██████████| 281/281 [00:01<00:00, 262.50it/s]\n",
            "accuracy: 0.7878, loss: 1.0329 ||: 100%|██████████| 12/12 [00:00<00:00, 424.92it/s]\n",
            "accuracy: 0.8442, loss: 0.5776 ||: 100%|██████████| 281/281 [00:01<00:00, 261.10it/s]\n",
            "accuracy: 0.7878, loss: 0.9972 ||: 100%|██████████| 12/12 [00:00<00:00, 465.29it/s]\n",
            "accuracy: 0.8463, loss: 0.5388 ||: 100%|██████████| 281/281 [00:01<00:00, 255.27it/s]\n",
            "accuracy: 0.7878, loss: 0.9105 ||: 100%|██████████| 12/12 [00:00<00:00, 452.42it/s]\n",
            "accuracy: 0.8557, loss: 0.5025 ||: 100%|██████████| 281/281 [00:01<00:00, 266.32it/s]\n",
            "accuracy: 0.7878, loss: 0.8672 ||: 100%|██████████| 12/12 [00:00<00:00, 502.15it/s]\n",
            "accuracy: 0.8589, loss: 0.4710 ||: 100%|██████████| 281/281 [00:01<00:00, 263.54it/s]\n",
            "accuracy: 0.7878, loss: 0.8246 ||: 100%|██████████| 12/12 [00:00<00:00, 458.92it/s]\n",
            "accuracy: 0.8604, loss: 0.4483 ||: 100%|██████████| 281/281 [00:01<00:00, 266.31it/s]\n",
            "accuracy: 0.7878, loss: 0.8034 ||: 100%|██████████| 12/12 [00:00<00:00, 480.04it/s]\n",
            "accuracy: 0.8610, loss: 0.4307 ||: 100%|██████████| 281/281 [00:01<00:00, 262.51it/s]\n",
            "accuracy: 0.7939, loss: 0.7764 ||: 100%|██████████| 12/12 [00:00<00:00, 393.61it/s]\n",
            "accuracy: 0.8618, loss: 0.4177 ||: 100%|██████████| 281/281 [00:01<00:00, 264.78it/s]\n",
            "accuracy: 0.7918, loss: 0.7886 ||: 100%|██████████| 12/12 [00:00<00:00, 417.45it/s]\n",
            "accuracy: 0.8624, loss: 0.4086 ||: 100%|██████████| 281/281 [00:01<00:00, 267.35it/s]\n",
            "accuracy: 0.7939, loss: 0.7775 ||: 100%|██████████| 12/12 [00:00<00:00, 436.93it/s]\n",
            "accuracy: 0.8635, loss: 0.3991 ||: 100%|██████████| 281/281 [00:01<00:00, 261.24it/s]\n",
            "accuracy: 0.7939, loss: 0.7548 ||: 100%|██████████| 12/12 [00:00<00:00, 462.45it/s]\n",
            "accuracy: 0.8637, loss: 0.3915 ||: 100%|██████████| 281/281 [00:01<00:00, 264.10it/s]\n",
            "accuracy: 0.7939, loss: 0.7258 ||: 100%|██████████| 12/12 [00:00<00:00, 458.06it/s]\n",
            "accuracy: 0.8639, loss: 0.3846 ||: 100%|██████████| 281/281 [00:01<00:00, 266.82it/s]\n",
            "accuracy: 0.7939, loss: 0.7546 ||: 100%|██████████| 12/12 [00:00<00:00, 499.42it/s]\n",
            "accuracy: 0.8644, loss: 0.3825 ||: 100%|██████████| 281/281 [00:01<00:00, 263.26it/s]\n",
            "accuracy: 0.7939, loss: 0.7421 ||: 100%|██████████| 12/12 [00:00<00:00, 463.11it/s]\n",
            "accuracy: 0.8648, loss: 0.3772 ||: 100%|██████████| 281/281 [00:01<00:00, 265.10it/s]\n",
            "accuracy: 0.7939, loss: 0.7249 ||: 100%|██████████| 12/12 [00:00<00:00, 483.50it/s]\n",
            "accuracy: 0.8654, loss: 0.3732 ||: 100%|██████████| 281/281 [00:01<00:00, 262.39it/s]\n",
            "accuracy: 0.7939, loss: 0.7204 ||: 100%|██████████| 12/12 [00:00<00:00, 452.98it/s]\n",
            "accuracy: 0.8656, loss: 0.3697 ||: 100%|██████████| 281/281 [00:01<00:00, 258.26it/s]\n",
            "accuracy: 0.7959, loss: 0.7053 ||: 100%|██████████| 12/12 [00:00<00:00, 481.87it/s]\n",
            "accuracy: 0.8674, loss: 0.3649 ||: 100%|██████████| 281/281 [00:01<00:00, 261.06it/s]\n",
            "accuracy: 0.7939, loss: 0.6900 ||: 100%|██████████| 12/12 [00:00<00:00, 442.45it/s]\n",
            "accuracy: 0.8704, loss: 0.3604 ||: 100%|██████████| 281/281 [00:01<00:00, 259.42it/s]\n",
            "accuracy: 0.8000, loss: 0.6886 ||: 100%|██████████| 12/12 [00:00<00:00, 471.91it/s]\n",
            "accuracy: 0.8692, loss: 0.3578 ||: 100%|██████████| 281/281 [00:01<00:00, 262.10it/s]\n",
            "accuracy: 0.8041, loss: 0.6803 ||: 100%|██████████| 12/12 [00:00<00:00, 496.74it/s]\n",
            "accuracy: 0.8720, loss: 0.3530 ||: 100%|██████████| 281/281 [00:01<00:00, 261.29it/s]\n",
            "accuracy: 0.8020, loss: 0.6898 ||: 100%|██████████| 12/12 [00:00<00:00, 486.14it/s]\n",
            "accuracy: 0.8730, loss: 0.3512 ||: 100%|██████████| 281/281 [00:01<00:00, 262.82it/s]\n",
            "accuracy: 0.8122, loss: 0.6684 ||: 100%|██████████| 12/12 [00:00<00:00, 499.22it/s]\n",
            "accuracy: 0.8734, loss: 0.3469 ||: 100%|██████████| 281/281 [00:01<00:00, 258.68it/s]\n",
            "accuracy: 0.8102, loss: 0.6622 ||: 100%|██████████| 12/12 [00:00<00:00, 455.48it/s]\n",
            "accuracy: 0.8752, loss: 0.3448 ||: 100%|██████████| 281/281 [00:01<00:00, 267.74it/s]\n",
            "accuracy: 0.8082, loss: 0.6600 ||: 100%|██████████| 12/12 [00:00<00:00, 479.74it/s]\n",
            "accuracy: 0.8770, loss: 0.3410 ||: 100%|██████████| 281/281 [00:01<00:00, 258.94it/s]\n",
            "accuracy: 0.8163, loss: 0.6475 ||: 100%|██████████| 12/12 [00:00<00:00, 492.26it/s]\n",
            "accuracy: 0.8771, loss: 0.3385 ||: 100%|██████████| 281/281 [00:01<00:00, 261.72it/s]\n",
            "accuracy: 0.8143, loss: 0.6657 ||: 100%|██████████| 12/12 [00:00<00:00, 453.47it/s]\n",
            "accuracy: 0.8778, loss: 0.3366 ||: 100%|██████████| 281/281 [00:01<00:00, 259.25it/s]\n",
            "accuracy: 0.8102, loss: 0.6780 ||: 100%|██████████| 12/12 [00:00<00:00, 486.86it/s]\n",
            "accuracy: 0.8772, loss: 0.3337 ||: 100%|██████████| 281/281 [00:01<00:00, 258.22it/s]\n",
            "accuracy: 0.8224, loss: 0.6388 ||: 100%|██████████| 12/12 [00:00<00:00, 439.82it/s]\n",
            "accuracy: 0.8772, loss: 0.3322 ||: 100%|██████████| 281/281 [00:01<00:00, 261.71it/s]\n",
            "accuracy: 0.8265, loss: 0.6314 ||: 100%|██████████| 12/12 [00:00<00:00, 445.93it/s]\n",
            "accuracy: 0.8784, loss: 0.3289 ||: 100%|██████████| 281/281 [00:01<00:00, 258.61it/s]\n",
            "accuracy: 0.8224, loss: 0.6294 ||: 100%|██████████| 12/12 [00:00<00:00, 362.76it/s]\n",
            "accuracy: 0.8806, loss: 0.3267 ||: 100%|██████████| 281/281 [00:01<00:00, 260.83it/s]\n",
            "accuracy: 0.8245, loss: 0.6163 ||: 100%|██████████| 12/12 [00:00<00:00, 448.03it/s]\n",
            "accuracy: 0.8823, loss: 0.3219 ||: 100%|██████████| 281/281 [00:01<00:00, 256.66it/s]\n",
            "accuracy: 0.8286, loss: 0.6260 ||: 100%|██████████| 12/12 [00:00<00:00, 409.48it/s]\n",
            "accuracy: 0.8837, loss: 0.3200 ||: 100%|██████████| 281/281 [00:01<00:00, 267.17it/s]\n",
            "accuracy: 0.8265, loss: 0.6083 ||: 100%|██████████| 12/12 [00:00<00:00, 436.65it/s]\n",
            "accuracy: 0.8835, loss: 0.3185 ||: 100%|██████████| 281/281 [00:01<00:00, 262.58it/s]\n",
            "accuracy: 0.8347, loss: 0.6129 ||: 100%|██████████| 12/12 [00:00<00:00, 486.94it/s]\n",
            "accuracy: 0.8845, loss: 0.3170 ||: 100%|██████████| 281/281 [00:01<00:00, 256.68it/s]\n",
            "accuracy: 0.8286, loss: 0.6286 ||: 100%|██████████| 12/12 [00:00<00:00, 469.49it/s]\n",
            "accuracy: 0.8869, loss: 0.3135 ||: 100%|██████████| 281/281 [00:01<00:00, 253.32it/s]\n",
            "accuracy: 0.8286, loss: 0.6085 ||: 100%|██████████| 12/12 [00:00<00:00, 451.21it/s]\n",
            "accuracy: 0.8869, loss: 0.3113 ||: 100%|██████████| 281/281 [00:01<00:00, 260.89it/s]\n",
            "accuracy: 0.8449, loss: 0.5925 ||: 100%|██████████| 12/12 [00:00<00:00, 429.11it/s]\n",
            "accuracy: 0.8891, loss: 0.3077 ||: 100%|██████████| 281/281 [00:01<00:00, 257.43it/s]\n",
            "accuracy: 0.8347, loss: 0.6124 ||: 100%|██████████| 12/12 [00:00<00:00, 491.46it/s]\n",
            "accuracy: 0.8865, loss: 0.3054 ||: 100%|██████████| 281/281 [00:01<00:00, 261.01it/s]\n",
            "accuracy: 0.8388, loss: 0.6078 ||: 100%|██████████| 12/12 [00:00<00:00, 461.27it/s]\n",
            "accuracy: 0.8899, loss: 0.3024 ||: 100%|██████████| 281/281 [00:01<00:00, 259.84it/s]\n",
            "accuracy: 0.8449, loss: 0.5989 ||: 100%|██████████| 12/12 [00:00<00:00, 444.28it/s]\n",
            "accuracy: 0.8929, loss: 0.2975 ||: 100%|██████████| 281/281 [00:01<00:00, 261.16it/s]\n",
            "accuracy: 0.8408, loss: 0.5790 ||: 100%|██████████| 12/12 [00:00<00:00, 507.06it/s]\n",
            "accuracy: 0.8921, loss: 0.2976 ||: 100%|██████████| 281/281 [00:01<00:00, 258.51it/s]\n",
            "accuracy: 0.8429, loss: 0.5710 ||: 100%|██████████| 12/12 [00:00<00:00, 484.07it/s]\n",
            "accuracy: 0.8911, loss: 0.2924 ||: 100%|██████████| 281/281 [00:01<00:00, 259.08it/s]\n",
            "accuracy: 0.8449, loss: 0.5734 ||: 100%|██████████| 12/12 [00:00<00:00, 427.63it/s]\n",
            "accuracy: 0.8945, loss: 0.2904 ||: 100%|██████████| 281/281 [00:01<00:00, 257.79it/s]\n",
            "accuracy: 0.8469, loss: 0.5710 ||: 100%|██████████| 12/12 [00:00<00:00, 423.03it/s]\n",
            "accuracy: 0.8946, loss: 0.2881 ||: 100%|██████████| 281/281 [00:01<00:00, 259.79it/s]\n",
            "accuracy: 0.8429, loss: 0.5551 ||: 100%|██████████| 12/12 [00:00<00:00, 468.80it/s]\n",
            "accuracy: 0.8974, loss: 0.2850 ||: 100%|██████████| 281/281 [00:01<00:00, 257.89it/s]\n",
            "accuracy: 0.8449, loss: 0.5647 ||: 100%|██████████| 12/12 [00:00<00:00, 426.66it/s]\n",
            "accuracy: 0.8943, loss: 0.2843 ||: 100%|██████████| 281/281 [00:01<00:00, 256.87it/s]\n",
            "accuracy: 0.8490, loss: 0.5565 ||: 100%|██████████| 12/12 [00:00<00:00, 449.98it/s]\n",
            "accuracy: 0.8965, loss: 0.2795 ||: 100%|██████████| 281/281 [00:01<00:00, 259.73it/s]\n",
            "accuracy: 0.8551, loss: 0.5458 ||: 100%|██████████| 12/12 [00:00<00:00, 522.44it/s]\n",
            "accuracy: 0.8978, loss: 0.2767 ||: 100%|██████████| 281/281 [00:01<00:00, 260.59it/s]\n",
            "accuracy: 0.8490, loss: 0.5474 ||: 100%|██████████| 12/12 [00:00<00:00, 483.56it/s]\n",
            "accuracy: 0.9008, loss: 0.2740 ||: 100%|██████████| 281/281 [00:01<00:00, 261.72it/s]\n",
            "accuracy: 0.8469, loss: 0.5319 ||: 100%|██████████| 12/12 [00:00<00:00, 462.38it/s]\n",
            "accuracy: 0.9001, loss: 0.2708 ||: 100%|██████████| 281/281 [00:01<00:00, 253.82it/s]\n",
            "accuracy: 0.8510, loss: 0.5462 ||: 100%|██████████| 12/12 [00:00<00:00, 432.80it/s]\n",
            "accuracy: 0.9015, loss: 0.2704 ||: 100%|██████████| 281/281 [00:01<00:00, 259.76it/s]\n",
            "accuracy: 0.8469, loss: 0.5289 ||: 100%|██████████| 12/12 [00:00<00:00, 510.84it/s]\n",
            "accuracy: 0.9003, loss: 0.2650 ||: 100%|██████████| 281/281 [00:01<00:00, 256.84it/s]\n",
            "accuracy: 0.8490, loss: 0.5173 ||: 100%|██████████| 12/12 [00:00<00:00, 499.05it/s]\n",
            "accuracy: 0.9027, loss: 0.2636 ||: 100%|██████████| 281/281 [00:01<00:00, 264.28it/s]\n",
            "accuracy: 0.8510, loss: 0.5093 ||: 100%|██████████| 12/12 [00:00<00:00, 493.96it/s]\n",
            "accuracy: 0.9027, loss: 0.2594 ||: 100%|██████████| 281/281 [00:01<00:00, 256.30it/s]\n",
            "accuracy: 0.8571, loss: 0.5226 ||: 100%|██████████| 12/12 [00:00<00:00, 406.14it/s]\n",
            "accuracy: 0.9020, loss: 0.2589 ||: 100%|██████████| 281/281 [00:01<00:00, 260.47it/s]\n",
            "accuracy: 0.8592, loss: 0.5105 ||: 100%|██████████| 12/12 [00:00<00:00, 453.15it/s]\n",
            "accuracy: 0.9054, loss: 0.2556 ||: 100%|██████████| 281/281 [00:01<00:00, 258.76it/s]\n",
            "accuracy: 0.8592, loss: 0.5129 ||: 100%|██████████| 12/12 [00:00<00:00, 481.01it/s]\n",
            "accuracy: 0.9063, loss: 0.2526 ||: 100%|██████████| 281/281 [00:01<00:00, 263.77it/s]\n",
            "accuracy: 0.8531, loss: 0.5120 ||: 100%|██████████| 12/12 [00:00<00:00, 513.00it/s]\n",
            "accuracy: 0.9073, loss: 0.2483 ||: 100%|██████████| 281/281 [00:01<00:00, 260.46it/s]\n",
            "accuracy: 0.8592, loss: 0.4974 ||: 100%|██████████| 12/12 [00:00<00:00, 403.96it/s]\n",
            "accuracy: 0.9076, loss: 0.2505 ||: 100%|██████████| 281/281 [00:01<00:00, 260.30it/s]\n",
            "accuracy: 0.8612, loss: 0.4953 ||: 100%|██████████| 12/12 [00:00<00:00, 374.84it/s]\n",
            "accuracy: 0.9103, loss: 0.2467 ||: 100%|██████████| 281/281 [00:01<00:00, 258.47it/s]\n",
            "accuracy: 0.8571, loss: 0.4889 ||: 100%|██████████| 12/12 [00:00<00:00, 401.95it/s]\n",
            "accuracy: 0.9089, loss: 0.2427 ||: 100%|██████████| 281/281 [00:01<00:00, 262.25it/s]\n",
            "accuracy: 0.8592, loss: 0.4796 ||: 100%|██████████| 12/12 [00:00<00:00, 452.46it/s]\n",
            "accuracy: 0.9110, loss: 0.2402 ||: 100%|██████████| 281/281 [00:01<00:00, 257.01it/s]\n",
            "accuracy: 0.8592, loss: 0.4681 ||: 100%|██████████| 12/12 [00:00<00:00, 492.32it/s]\n",
            "accuracy: 0.9118, loss: 0.2375 ||: 100%|██████████| 281/281 [00:01<00:00, 261.09it/s]\n",
            "accuracy: 0.8592, loss: 0.4630 ||: 100%|██████████| 12/12 [00:00<00:00, 378.27it/s]\n",
            "accuracy: 0.9124, loss: 0.2358 ||: 100%|██████████| 281/281 [00:01<00:00, 252.34it/s]\n",
            "accuracy: 0.8551, loss: 0.4582 ||: 100%|██████████| 12/12 [00:00<00:00, 432.85it/s]\n",
            "accuracy: 0.9111, loss: 0.2340 ||: 100%|██████████| 281/281 [00:01<00:00, 254.96it/s]\n",
            "accuracy: 0.8592, loss: 0.4719 ||: 100%|██████████| 12/12 [00:00<00:00, 473.74it/s]\n",
            "accuracy: 0.9136, loss: 0.2332 ||: 100%|██████████| 281/281 [00:01<00:00, 251.34it/s]\n",
            "accuracy: 0.8612, loss: 0.4673 ||: 100%|██████████| 12/12 [00:00<00:00, 366.86it/s]\n",
            "accuracy: 0.9107, loss: 0.2340 ||: 100%|██████████| 281/281 [00:01<00:00, 247.84it/s]\n",
            "accuracy: 0.8633, loss: 0.4506 ||: 100%|██████████| 12/12 [00:00<00:00, 498.22it/s]\n",
            "accuracy: 0.9145, loss: 0.2275 ||: 100%|██████████| 281/281 [00:01<00:00, 261.16it/s]\n",
            "accuracy: 0.8653, loss: 0.4513 ||: 100%|██████████| 12/12 [00:00<00:00, 366.42it/s]\n",
            "accuracy: 0.9152, loss: 0.2254 ||: 100%|██████████| 281/281 [00:01<00:00, 254.38it/s]\n",
            "accuracy: 0.8612, loss: 0.4398 ||: 100%|██████████| 12/12 [00:00<00:00, 452.81it/s]\n",
            "accuracy: 0.9133, loss: 0.2256 ||: 100%|██████████| 281/281 [00:01<00:00, 261.21it/s]\n",
            "accuracy: 0.8633, loss: 0.4422 ||: 100%|██████████| 12/12 [00:00<00:00, 416.30it/s]\n",
            "accuracy: 0.9165, loss: 0.2210 ||: 100%|██████████| 281/281 [00:01<00:00, 256.17it/s]\n",
            "accuracy: 0.8653, loss: 0.4453 ||: 100%|██████████| 12/12 [00:00<00:00, 386.50it/s]\n",
            "accuracy: 0.9148, loss: 0.2215 ||: 100%|██████████| 281/281 [00:01<00:00, 257.70it/s]\n",
            "accuracy: 0.8673, loss: 0.4416 ||: 100%|██████████| 12/12 [00:00<00:00, 431.69it/s]\n",
            "accuracy: 0.9166, loss: 0.2170 ||: 100%|██████████| 281/281 [00:01<00:00, 257.18it/s]\n",
            "accuracy: 0.8694, loss: 0.4413 ||: 100%|██████████| 12/12 [00:00<00:00, 376.22it/s]\n",
            "accuracy: 0.9184, loss: 0.2157 ||: 100%|██████████| 281/281 [00:01<00:00, 261.98it/s]\n",
            "accuracy: 0.8612, loss: 0.4313 ||: 100%|██████████| 12/12 [00:00<00:00, 433.85it/s]\n",
            "accuracy: 0.9175, loss: 0.2138 ||: 100%|██████████| 281/281 [00:01<00:00, 256.25it/s]\n",
            "accuracy: 0.8673, loss: 0.4270 ||: 100%|██████████| 12/12 [00:00<00:00, 473.10it/s]\n",
            "accuracy: 0.9194, loss: 0.2160 ||: 100%|██████████| 281/281 [00:01<00:00, 260.06it/s]\n",
            "accuracy: 0.8673, loss: 0.4227 ||: 100%|██████████| 12/12 [00:00<00:00, 381.24it/s]\n",
            "accuracy: 0.9193, loss: 0.2090 ||: 100%|██████████| 281/281 [00:01<00:00, 258.93it/s]\n",
            "accuracy: 0.8673, loss: 0.4211 ||: 100%|██████████| 12/12 [00:00<00:00, 506.86it/s]\n",
            "accuracy: 0.9205, loss: 0.2087 ||: 100%|██████████| 281/281 [00:01<00:00, 257.53it/s]\n",
            "accuracy: 0.8714, loss: 0.4137 ||: 100%|██████████| 12/12 [00:00<00:00, 479.80it/s]\n",
            "accuracy: 0.9199, loss: 0.2058 ||: 100%|██████████| 281/281 [00:01<00:00, 260.89it/s]\n",
            "accuracy: 0.8735, loss: 0.4138 ||: 100%|██████████| 12/12 [00:00<00:00, 410.54it/s]\n",
            "accuracy: 0.9213, loss: 0.2060 ||: 100%|██████████| 281/281 [00:01<00:00, 259.34it/s]\n",
            "accuracy: 0.8735, loss: 0.4097 ||: 100%|██████████| 12/12 [00:00<00:00, 442.32it/s]\n",
            "accuracy: 0.9236, loss: 0.2018 ||: 100%|██████████| 281/281 [00:01<00:00, 253.38it/s]\n",
            "accuracy: 0.8714, loss: 0.3977 ||: 100%|██████████| 12/12 [00:00<00:00, 512.05it/s]\n",
            "accuracy: 0.9205, loss: 0.2061 ||: 100%|██████████| 281/281 [00:01<00:00, 258.36it/s]\n",
            "accuracy: 0.8735, loss: 0.4077 ||: 100%|██████████| 12/12 [00:00<00:00, 456.10it/s]\n",
            "accuracy: 0.9220, loss: 0.2027 ||: 100%|██████████| 281/281 [00:01<00:00, 259.93it/s]\n",
            "accuracy: 0.8735, loss: 0.4061 ||: 100%|██████████| 12/12 [00:00<00:00, 481.26it/s]\n",
            "accuracy: 0.9224, loss: 0.1979 ||: 100%|██████████| 281/281 [00:01<00:00, 259.28it/s]\n",
            "accuracy: 0.8714, loss: 0.3967 ||: 100%|██████████| 12/12 [00:00<00:00, 417.34it/s]\n",
            "accuracy: 0.9251, loss: 0.1950 ||: 100%|██████████| 281/281 [00:01<00:00, 261.45it/s]\n",
            "accuracy: 0.8735, loss: 0.4031 ||: 100%|██████████| 12/12 [00:00<00:00, 438.47it/s]\n",
            "accuracy: 0.9249, loss: 0.1931 ||: 100%|██████████| 281/281 [00:01<00:00, 260.54it/s]\n",
            "accuracy: 0.8796, loss: 0.3859 ||: 100%|██████████| 12/12 [00:00<00:00, 470.34it/s]\n",
            "accuracy: 0.9243, loss: 0.1938 ||: 100%|██████████| 281/281 [00:01<00:00, 256.04it/s]\n",
            "accuracy: 0.8755, loss: 0.4105 ||: 100%|██████████| 12/12 [00:00<00:00, 461.87it/s]\n",
            "accuracy: 0.9251, loss: 0.1925 ||: 100%|██████████| 281/281 [00:01<00:00, 256.60it/s]\n",
            "accuracy: 0.8816, loss: 0.3902 ||: 100%|██████████| 12/12 [00:00<00:00, 438.33it/s]\n",
            "accuracy: 0.9268, loss: 0.1890 ||: 100%|██████████| 281/281 [00:01<00:00, 261.42it/s]\n",
            "accuracy: 0.8816, loss: 0.3822 ||: 100%|██████████| 12/12 [00:00<00:00, 505.91it/s]\n",
            "accuracy: 0.9280, loss: 0.1875 ||: 100%|██████████| 281/281 [00:01<00:00, 259.87it/s]\n",
            "accuracy: 0.8837, loss: 0.3793 ||: 100%|██████████| 12/12 [00:00<00:00, 421.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 99,\n",
              " 'best_validation_accuracy': 0.8836734693877552,\n",
              " 'best_validation_loss': 0.3793452437821543,\n",
              " 'epoch': 99,\n",
              " 'peak_cpu_memory_MB': 3221.472,\n",
              " 'peak_gpu_0_memory_MB': 1058,\n",
              " 'training_accuracy': 0.9279737315962292,\n",
              " 'training_cpu_memory_MB': 3221.472,\n",
              " 'training_duration': '0:01:58.824019',\n",
              " 'training_epochs': 99,\n",
              " 'training_gpu_0_memory_MB': 1058,\n",
              " 'training_loss': 0.18746380445160454,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.8836734693877552,\n",
              " 'validation_loss': 0.3793452437821543}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwN6ctqVV0tf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDs_UdIeuFz"
      },
      "source": [
        "The simple code below loops over the validation set, applying the model to each example and collecting out the input token, gold-standard output, and model output. You can see from these methods how to access ground truth and model outputs for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bE4fmLik08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fb299f-b4f7-4a09-8a1a-b3f8c25b6e16"
      },
      "source": [
        "def tag_sentence(s):\n",
        "  tag_ids = np.argmax(model.forward_on_instance(s)['tag_logits'], axis=-1)\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)\n",
        "\n",
        "baseline_output = [tag_sentence(i) for i in validation_dataset]\n",
        "## Show the first example\n",
        "baseline_output[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(With, 'O', 'O'),\n",
              " (a, 'O', 'O'),\n",
              " (wave, 'O', 'O'),\n",
              " (of, 'O', 'O'),\n",
              " (his, 'O', 'O'),\n",
              " (hand, 'O', 'O'),\n",
              " (,, 'O', 'O'),\n",
              " (Peng, 'B-PERSON', 'B-PERSON'),\n",
              " (Dehuai, 'I-PERSON', 'I-PERSON'),\n",
              " (said, 'O', 'O'),\n",
              " (that, 'O', 'O'),\n",
              " (despite, 'O', 'O'),\n",
              " (being, 'O', 'O'),\n",
              " (over, 'O', 'O'),\n",
              " (100, 'B-CARDINAL', 'B-CARDINAL'),\n",
              " (regiments, 'O', 'O'),\n",
              " (,, 'O', 'O'),\n",
              " (let, 'O', 'O'),\n",
              " ('s, 'O', 'O'),\n",
              " (call, 'O', 'O'),\n",
              " (this, 'O', 'O'),\n",
              " (campaign, 'O', 'O'),\n",
              " (the, 'B-EVENT', 'O'),\n",
              " (Hundred, 'I-EVENT', 'I-EVENT'),\n",
              " (Regiments, 'I-EVENT', 'I-EVENT'),\n",
              " (Offensive, 'I-EVENT', 'I-EVENT'),\n",
              " (., 'O', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpYMx7RVfCyT"
      },
      "source": [
        "Now, you can implement two evaluation functions: `violations` and `span_stats`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aibRDyncuLR"
      },
      "source": [
        "### Violations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P042A2Ofg3wa"
      },
      "source": [
        "# count the number of NER label violations,\n",
        "# such as O followed by I-TYPE or B-TYPE followed by\n",
        "# I-OTHER_TYPE\n",
        "# Take tagger output as input\n",
        "def violations(predicted_type_list):\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  # violation 1: I-TYPE following O (or I-TYPE starting sentence)\n",
        "  # mark with 1 whenever we see the violation\n",
        "  invalid_start = [[1 for i in range(len(sentence)) if (sentence[i][0] == 'I' and sentence[i-1][0] == 'O' and i != 0) or \n",
        "                  (i == 0 and sentence[i][0] == 'I')] for sentence in predicted_type_list]\n",
        "  # sum up number of violations\n",
        "  invalid_start = [item for sublist in invalid_start for item in sublist]\n",
        "  count += np.sum(invalid_start)\n",
        "\n",
        "  # violation 2: I-OTHER_TYPE following B-TYPE or I-TYPE\n",
        "  # mark with 1 whenever we see the violation\n",
        "  invalid_type = [[1 for i in range(len(sentence)) if \n",
        "                   (sentence[i][0] == 'I' and sentence[i-1][0] != 'O' and sentence[i-1][1] != sentence[i][1] and i != 0)] \n",
        "                  for sentence in predicted_type_list]\n",
        "  # sum up number of violations\n",
        "  invalid_type = [item for sublist in invalid_type for item in sublist]\n",
        "  count += np.sum(invalid_type)\n",
        "\n",
        "  return count"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNsGhS7Gbz8_"
      },
      "source": [
        "# get list of predicted [B/I/O tag, tag type (if exists)] for each sentence\n",
        "predicted_type_list = [[tag[2].split('-') for tag in sentence] for sentence in baseline_output]\n",
        "# get list of actual [B/I/O tag, tag type (if exists)] for each sentence\n",
        "actual_type_list = [[tag[1].split('-') for tag in sentence] for sentence in baseline_output]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw-NWmU9TsYe",
        "outputId": "cc272b5b-6bdd-4b22-9d81-edbe54b75683"
      },
      "source": [
        "# number of violations\n",
        "violations(predicted_type_list)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oX2KQ7gcy9R"
      },
      "source": [
        "### Span Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkmlXyHAWqAe"
      },
      "source": [
        "# create dataframe with one row per token and columns with predicted tag, predicted type, actual tag, actual type, document number\n",
        "# easier to mark spans in a dataframe than in a list \n",
        "def create_dataframe(predicted_type_list, actual_type_list):\n",
        "\n",
        "  tag_df = pd.DataFrame()\n",
        "\n",
        "  # create dataframes for each record and concat together \n",
        "  for i in range(len(predicted_type_list)):\n",
        "    # dataframe of predicted tags and types\n",
        "    predicted_df = pd.DataFrame.from_records(predicted_type_list[i])\n",
        "    # some records don't have any type because all O tags\n",
        "    if len(predicted_df.columns) == 2: \n",
        "      predicted_df.columns = ['predicted_tag', 'predicted_type']\n",
        "    else:\n",
        "      predicted_df.columns = ['predicted_tag']\n",
        "    predicted_df['num'] = i # mark which document\n",
        "\n",
        "    # dataframe of actual tags and types\n",
        "    actual_df = pd.DataFrame.from_records(actual_type_list[i])\n",
        "    if len(actual_df.columns) == 2:\n",
        "      actual_df.columns = ['actual_tag', 'actual_type']\n",
        "    else:\n",
        "      actual_df.columns = ['actual_tag']\n",
        "\n",
        "    # merge actual and predicted\n",
        "    tmp_df = pd.merge(predicted_df, actual_df, left_index = True, right_index = True)\n",
        "\n",
        "    # concat to overall dataframe for all records\n",
        "    tag_df = pd.concat([tag_df, tmp_df], sort = False)\n",
        "\n",
        "  return tag_df.reset_index(drop = True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyxAxcu2YkPm"
      },
      "source": [
        "# create indicator for each sequence of actual and predicted tags\n",
        "# only consider valid spans \n",
        "def mark_sequences(tag_df, var_type):\n",
        "  tagvar = var_type + '_tag'\n",
        "  typevar = var_type + '_type'\n",
        "  sequencevar = var_type + '_sequence'\n",
        "\n",
        "  # mark start of sequence\n",
        "  tag_df[sequencevar] = np.where(tag_df[tagvar] == 'B', tag_df.index, 0)\n",
        "  # sequence ends if different type var or different record\n",
        "  tag_df[sequencevar] = np.where((tag_df[typevar] != tag_df[typevar].shift()) &\n",
        "                                 (tag_df[sequencevar] == 0),\n",
        "                                 np.nan, tag_df[sequencevar])\n",
        "  tag_df[sequencevar] = np.where((tag_df.num != tag_df.num.shift()) &\n",
        "                                 (tag_df[sequencevar] == 0), np.nan, tag_df[sequencevar])\n",
        "  \n",
        "  # drag forward sequence start indicator over entire sequence\n",
        "  tag_df[sequencevar] = tag_df[sequencevar].replace(to_replace = 0, method = 'ffill')\n",
        "\n",
        "  return tag_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s51eH25yc3Tj"
      },
      "source": [
        "# return the span-level precision, recall, and F1\n",
        "# Only count valid spans that start with a B tag,\n",
        "# followed by zero or more I tags of the same type.\n",
        "# This is harsher than the token-level metric that the\n",
        "# LSTM was trained to optimize, but it is the standard way\n",
        "# of evaluating NER systems.\n",
        "# Take tagger output as input\n",
        "def span_stats(predicted_type_list, actual_type_list):\n",
        "\n",
        "  # mark actual vs predicted sequences\n",
        "  # create pandas dataframe. easier to generate a sense of prolonged sequences and order\n",
        "  tag_df = create_dataframe(predicted_type_list, actual_type_list)\n",
        "  tag_df = mark_sequences(tag_df, 'actual')\n",
        "  tag_df = mark_sequences(tag_df, 'predicted')\n",
        "\n",
        "  # generate lists of actual and predicted sequences\n",
        "  # sequence \"signature\" = index (to mark position) + tag + type\n",
        "    # thus when match actual and predicted sequences up, checking correct words, tags, and types \n",
        "  tag_df['sequence_signature_actual'] = tag_df.index.astype(str) + tag_df.actual_tag + tag_df.actual_type \n",
        "  # for each sequence, get list of signatures. When matching against predicted, all signatures must be present\n",
        "  actual_sequences = tag_df.groupby('actual_sequence').sequence_signature_actual.unique().to_list()\n",
        "  actual_sequences = [list(i) for i in actual_sequences]\n",
        "\n",
        "  tag_df['sequence_signature_predicted'] = tag_df.index.astype(str) + tag_df.predicted_tag + tag_df.predicted_type \n",
        "  predicted_sequences = tag_df.groupby('predicted_sequence').sequence_signature_predicted.unique().to_list()\n",
        "  predicted_sequences = [list(i) for i in predicted_sequences]\n",
        "\n",
        "  # precision: percent of predicted sequences that are correct\n",
        "  correct_sequence = 0\n",
        "  for sequence in predicted_sequences:\n",
        "    if sequence in actual_sequences:\n",
        "      correct_sequence += 1 \n",
        "  precision = correct_sequence / len(predicted_sequences)\n",
        "\n",
        "  # recall: percent of actual sequences that are predicted\n",
        "  correct_sequence = 0\n",
        "  for sequence in actual_sequences:\n",
        "    if sequence in predicted_sequences:\n",
        "      correct_sequence += 1 \n",
        "  recall = correct_sequence / len(actual_sequences)\n",
        "\n",
        "  # f1 score\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  return {'precision': precision,\n",
        "          'recall': recall,\n",
        "          'f1': f1}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ1Y9dWcVBNL",
        "outputId": "da3a98e6-cb01-4ed0-e9e3-1502365ba342"
      },
      "source": [
        "span_stats(predicted_type_list, actual_type_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.3055555555555556,\n",
              " 'precision': 0.3793103448275862,\n",
              " 'recall': 0.2558139534883721}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7-quD2hnzB"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCgW9d9ohsGv"
      },
      "source": [
        "Now you can finally implement the simple Viterbi decoder. The `model` object, when applied to an input sentence, first calculates the scores for each possible output tag for each token. See the expression `model.forward_on_instance(s)['tag_logits']` in the code above.\n",
        "\n",
        "Then, you will construct a transition matrix. You can use the code below to get a list of the tags the model knows about. For a set of K tags, construct a K-by-K matrix with a log(1)=0 when a transition between a given tag pair is valid and a log(0)=-infinity otherwise.\n",
        "\n",
        "Finally, implement a Viterbi decoder that takes the model object and a dataset object and outputs tagged data, just like the `tag_sentence` function above. It should use the Viterbi algorithm with the (max, plus) semiring. You'll be working with sums of log probabilities instead of products of probabilties.\n",
        "\n",
        "Run your `violations` function on the output of this decoder to make sure that there are no invalid tag transitions. Also, compare the span-level metrics on `baseline_output` and your new output using your `span_stats` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McEDq68jqbcK"
      },
      "source": [
        "def create_transition_matrix(vocab):\n",
        "  # initialize matrix of zeros\n",
        "  K = len(vocab.get_index_to_token_vocabulary('labels'))\n",
        "  transition_matrix = np.zeros((K,K))\n",
        "\n",
        "  # get lists of indexes of I tags and B tags\n",
        "  tag_dict = vocab.get_index_to_token_vocabulary('labels')\n",
        "  Ipositions = [k for (k,v) in tag_dict.items() if 'I-' in v]\n",
        "  Bpositions = [k for (k,v) in tag_dict.items() if 'B-' in v]\n",
        "\n",
        "  # mark invalid transitions\n",
        "  # Invalid transition 1: from O to I \n",
        "  for i in Ipositions:\n",
        "    transition_matrix[0][i] = -np.Inf\n",
        "\n",
        "  # Invalid transition 2: from B-TYPE to I-OTHERTYPE \n",
        "  for b in Bpositions:\n",
        "    for i in Ipositions:\n",
        "      # skip if same type\n",
        "      if tag_dict[i].split('-')[1] == tag_dict[b].split('-')[1]:\n",
        "        continue\n",
        "      else:\n",
        "        transition_matrix[b][i] = -np.Inf\n",
        "\n",
        "  # Invalid transition 3: from I-TYPE to I-OTHERTYPE\n",
        "  for i in Ipositions:\n",
        "    for i2 in Ipositions:\n",
        "      # skip if same tag\n",
        "      if i == i2:\n",
        "        continue\n",
        "      else:\n",
        "        transition_matrix[i][i2] = -np.Inf\n",
        "\n",
        "  return transition_matrix, Ipositions"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekmnWcMKCSyL"
      },
      "source": [
        "def decode(s, Ipositions):\n",
        "\n",
        "  # initialize matrix to hold probabilites and best path for each tag-word position combination\n",
        "  # rows = tags, columns = tokens\n",
        "  K = len(vocab.get_index_to_token_vocabulary('labels'))\n",
        "  viterbi = np.zeros((K,len(s['tokens'])))\n",
        "  backpointer = np.zeros((K, len(s['tokens'])))\n",
        "\n",
        "  # initial probabilities of each tag for first token in sentence\n",
        "  viterbi[:,0] = model.forward_on_instance(s)['tag_logits'][0]\n",
        "  # -infinity probability for I tokens (cannot start a sentence)\n",
        "  for i in Ipositions:\n",
        "    viterbi[i,0] = -np.inf\n",
        "  # initial tag on the best path. \n",
        "  backpointer[:,0] = np.argmax(viterbi[:,0])\n",
        "\n",
        "  # loop through tokens in sentence\n",
        "  for token_index in range(1, len(s['tokens'])):\n",
        "    # loop through possible tags\n",
        "    for new_tag_index in range(K):\n",
        "      bestprob = -np.Inf\n",
        "      bestpath = -1\n",
        "      # loop through tags in prior position to find best path so far\n",
        "      for prior_tag_index in range(K):\n",
        "        # prior path probability + transition probability + state observation probability\n",
        "        prob = transition_matrix[prior_tag_index, new_tag_index] + viterbi[prior_tag_index, token_index - 1] + model.forward_on_instance(s)['tag_logits'][token_index, new_tag_index]\n",
        "        # keep track of best seen \n",
        "        if prob > bestprob:\n",
        "          bestprob = prob\n",
        "          bestpath = prior_tag_index\n",
        "      # record best paths and probabilities\n",
        "      viterbi[new_tag_index, token_index] = bestprob \n",
        "      backpointer[new_tag_index, token_index] = bestpath\n",
        "\n",
        "  # generate best path: follow backpointer backwards\n",
        "  # starting at path that ends with highest probability at final token position\n",
        "  best_sequence = []\n",
        "  back_position = int(np.argmax([score[-1] for score in viterbi]))\n",
        "  best_sequence.append(back_position)\n",
        "  for i in range(len(s['tokens'])-1, 0, -1):\n",
        "    back_position = int(backpointer[back_position][i])\n",
        "    best_sequence.append(back_position)\n",
        "  best_sequence = best_sequence[::-1] # reverse sequence \n",
        "\n",
        "  # formatted output\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in best_sequence])\n",
        "\n",
        "  return list(fields)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIGOqdy6uBt"
      },
      "source": [
        "transition_matrix, Ipositions = create_transition_matrix(vocab)\n",
        "output = [decode(i, Ipositions) for i in validation_dataset]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agPj33xyChXA",
        "outputId": "b60f13fc-c881-47d4-a544-55aadbb4edd3"
      },
      "source": [
        "# number of violations\n",
        "# get list of predicted [B/I/O tag, tag type (if exists)] for each sentence\n",
        "predicted_type_list = [[tag[2].split('-') for tag in sentence] for sentence in output]\n",
        "# get list of actual [B/I/O tag, tag type (if exists)] for each sentence\n",
        "actual_type_list = [[tag[1].split('-') for tag in sentence] for sentence in output]\n",
        "violations(predicted_type_list)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LI7-OVvf-Lz"
      },
      "source": [
        "The revised algorithm using a transition matrix to enforce tag ordering constraints significantly improved both the precision and recall. By eliminating paths that are impossible, we have a better chance of finding the correct labeling sequence. For example, the baseline model labels \"The Central Military Comission\" as O, I-EVENT, I-ORG, I-ORG. The improved model cannot generate this path because it has (mutliple) violations. It thus instead finds the next best valid path and ultimately predicts the correct sequence B-ORG, I-ORG, I-ORG, I-ORG.     \n",
        "    \n",
        "More specifically, recall improved more than precision. This makes sense because invalid sequences are not considered in the denominator when calculating precision for the original model (while the denominator of recall is the correct sequences so it is unaffected). Note however that the revised algorithm does still improve precision because it improves the accuracy of even previously valid sequences. For example, the baseline model labels \"600,000 - plus\" as B-CARDINAL, I-ORG, I-PERSON. The B-CARDINAL is a valid but incorrect (incomplete) sequence. Because the following tags are not possible in the improved model, the sequence gets correctly predicted as B-CARDINAL, I-CARDINAL, I-CARDINAL and precision improves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmIbCAoP87Fw",
        "outputId": "18c45611-da45-4746-e809-b12cd4b1b407"
      },
      "source": [
        "span_stats(predicted_type_list, actual_type_list)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1': 0.45977011494252873,\n",
              " 'precision': 0.45454545454545453,\n",
              " 'recall': 0.46511627906976744}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    }
  ]
}